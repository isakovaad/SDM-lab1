,ID,title,abstract,pages,year,link,keyword
0,91b63db746becca15090963a8990dfe2b5103799,"Big data: The next frontier for innovation, competition, and productivity","The amount of data in our world has been exploding, and analyzing large data sets—so-called big data— will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer surplus, according to research by MGI and McKinsey's Business Technology Office. Leaders in every sector will have to grapple with the implications of big data, not just a few data-oriented managers. The increasing volume and detail of information captured by enterprises, the rise of multimedia, social media, and the Internet of Things will fuel exponential growth in data for the foreseeable future.",35-123,2011.0,https://flores.info/posts/categorymain.jsp,Business
1,ccca203382e5dd198c089a0f1d7af7bef0f694e9,TBtools - an integrative toolkit developed for interactive analyses of big biological data.,,50-111,2020.0,http://www.fields-mcintyre.com/exploresearch.html,Medicine
2,f117c6f12d067bd66dad40996b3931c069daa2da,Business Intelligence and Analytics: From Big Data to Big Impact,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",1165-1188,2012.0,https://rosario.com/search/explore/listabout.jsp,Computer Science
3,85328b4a8132bf4299f8cd7f8e79e850d561c8fc,Big Data Analytics: A Survey,"Internet-based programs and communication techniques have become widely used and respected in the IT industry recently. A persistent source of ""big data,"" or data that is enormous in volume, diverse in type, and has a complicated multidimensional structure, is internet applications and communications. Today, several measures are routinely performed with no assurance that any of them will be helpful in understanding the phenomenon of interest in an era of automatic, large-scale data collection. Online transactions that involve buying, selling, or even investing are all examples of e-commerce. As a result, they generate data that has a complex structure and a high dimension. The usual data storage techniques cannot handle those enormous volumes of data. There is a lot of work being done to find ways to minimize the dimensionality of big data in order to provide analytics reports that are even more accurate and data visualizations that are more interesting. As a result, the purpose of this survey study is to give an overview of big data analytics along with related problems and issues that go beyond technology.",36-109,2022.0,https://stark.org/appregister.htm,Technology
4,bf5a42b53d156c0811e88e60d2a49f9fd9367cae,Big data: the management revolution.,"Big data, the authors write, is far more powerful than the analytics of the past. Executives can measure and therefore manage more precisely than ever before. They can make better predictions and smarter decisions. They can target more-effective interventions in areas that so far have been dominated by gut and intuition rather than by data and rigor. The differences between big data and analytics are a matter of volume, velocity, and variety: More data now cross the internet every second than were stored in the entire internet 20 years ago. Nearly real-time information makes it possible for a company to be much more agile than its competitors. And that information can come from social networks, images, sensors, the web, or other unstructured sources. The managerial challenges, however, are very real. Senior decision makers have to learn to ask the right questions and embrace evidence-based decision making. Organizations must hire scientists who can find patterns in very large data sets and translate them into useful business information. IT departments have to work hard to integrate all the relevant internal and external sources of data. The authors offer two success stories to illustrate how companies are using big data: PASSUR Aerospace enables airlines to match their actual and estimated arrival times. Sears Holdings directly analyzes its incoming store data to make promotions much more precise and faster.","
          60-6, 68, 128
        ",2012.0,https://berry-washington.com/mainsearch.html,Computer Science
5,38f5b53b49be555430f33b8363910191a3df1d14,"A Survey on Big Data Analytics: Challenges, Open Research Issues and Tools","Abstract: A huge repository of terabytes of data is generated each day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data challenges, open research issues, and various tools associated with it. As a result, this article provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues.",56-140,2022.0,http://johnson.com/search/explore/postssearch.htm,Computer Science
6,cc017a62c605a0749e35a1264a46d62e78fb68b7,Big Data Analytics,,32-105,2019.0,http://gray.org/categoryauthor.php,Computer Science
7,4b06c7e29280b1c6bc05c9df39023b48fef02c93,Escaping the Big Data Paradigm with Compact Transformers,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",78-102,2021.0,https://www.johnson-collins.com/explore/explore/appregister.php,Computer Science
8,1bc34cb22131554ba18f6ba9e6ede5beb42939f1,"Beyond the hype: Big data concepts, methods, and analytics",,137-144,2015.0,http://miller.com/app/wp-content/postspost.asp,Computer Science
9,41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1,The use of Big Data Analytics in healthcare,,59-134,2022.0,https://lyons-garcia.com/posts/categories/listprivacy.php,Medicine
10,bf69c98fca9a9f6c1cde871beddbcdc668b77771,"Big Data: A Revolution That Will Transform How We Live, Work, and Think","Since Aristotle, we have fought to understand the causes behind everything. But this ideology is fading. The world of big data can crunch However the indirect implication of a raw material in cdc data processing. He says most common search terms think we'll have lost. At the damnation profoundly surprising conclusions, from make it seems more recently I think. Less as a fascinatingand sometimes profoundly surprising ways not knowing why only one to find knowledge.",156,2015.0,https://www.burns.net/bloghomepage.php,Sociology
11,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",865-873,2015.0,https://johnson.com/posts/tag/categorieslogin.html,Computer Science
12,4e6bba65f7636a655c778a3e54cc58e148468963,CRITICAL QUESTIONS FOR BIG DATA,"The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what ‘research’ means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, technological, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.",662 - 679,2012.0,https://www.nelson.org/explore/search/tagabout.html,Sociology
13,3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,,301 - 302,2017.0,https://hanson.com/appregister.php,Computer Science
14,1597449a7f64b6bd24639b4deab96c8a8c184177,"Digital twin-driven product design, manufacturing and service with big data",,3563 - 3576,2017.0,http://www.taylor.org/blog/blog/apppost.php,Engineering
15,cbad0923db89f23febcbd6192ff4149289ff2ad9,A survey on data‐efficient algorithms in big data era,,1-54,2021.0,http://www.fisher.info/category/tagsprivacy.html,Computer Science
16,a0d18dddaa995b126ad373e33767b9b881d16b2f,An Introductory Review of Deep Learning for Prediction Models With Big Data,"Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly—in an almost Lego-like manner—to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.",60-122,2020.0,https://perez.biz/listterms.php,Medicine
17,fe44200fed05f9a7c656f2245deded8fd5f5e1e6,CatBoost for big data: an interdisciplinary review,,56-112,2020.0,http://zuniga-rodriguez.info/categories/exploreterms.html,Computer Science
18,73d4accea441aae2373828a8dc2175aa2759c38f,Big Data in Finance,"
 Big data is revolutionizing the finance industry and has the potential to significantly shape future research in finance. This special issue contains papers following the 2019 NBER-RFS Conference on Big Data. In this introduction to the special issue, we define the “big data” phenomenon as a combination of three features: large size, high dimension, and complex structure. Using the papers in the special issue, we discuss how new research builds on these features to push the frontier on fundamental questions across areas in finance—including corporate finance, market microstructure, and asset pricing. Finally, we offer some thoughts for future research directions.",72-119,2021.0,http://leonard.com/app/wp-content/explorepost.asp,Economics
19,4e13a8e8ba8d33e15ed037bfca7c651047533990,Big data for cyber physical systems in industry 4.0: a survey,"ABSTRACT With the technology development in cyber physical systems and big data, there are huge potential to apply them to achieve personalization and improve resource efficiency in Industry 4.0. As Industry 4.0 is the relatively new concept originated from an advanced manufacturing vision supported by the German government in 2011, there are only several existing surveys on either cyber physical systems or big data in Industry 4.0. In addition, there are much less surveys related to the intersection between cyber physical systems and big data in Industry 4.0. However, cyber physical systems are closely related to big data in nature. For example, cyber physical systems will continuously generate a large amount of data which requires the big data techniques to process and help to improve system scalability, security, and efficiency. Therefore, we conduct this survey to bring more attention to this critical intersection and highlight the future research direction to achieve the fully autonomy in Industry 4.0.",148 - 169,2019.0,https://fowler.net/app/wp-contentabout.php,Computer Science
20,9d6cbac04c498b424dfaa5ce82ac201a180c1502,Big data with cloud computing: Discussions and challenges,"Big Data and cloud computing integration has become a formidable strategy for businesses to unlock the potential of enormous and complicated data sets. With the scalability, flexibility, and cost-effectiveness that this combination provides, businesses are able to handle and analyse massive amounts of data in a distributed, as-needed way. But there are also issues and restrictions that need to be resolved with this integration. This overview of the literature focuses on the issues, difficulties, and potential applications of big data and cloud computing. It offers information on the advantages of this integration, such as improved data processing capabilities, increased scalability, and cost reduction. The difficulties with data migration, security, privacy, data governance, talent needs, vendor lock-in, and compliance are all discussed. Future research areas are also highlighted, such as enhanced analytics methods, edge computing integration, privacy-preserving data analysis, hybrid cloud architectures, data governance, real-time decision-making, and applications specialised to particular industries. Organisations can maximise the potential of Big Data with cloud computing and generate priceless insights to fuel innovation and informed decision-making by comprehending, tackling, and investigating these difficulties.",32-40,2021.0,http://www.fitzgerald.com/explore/explore/appindex.html,Computer Science
21,8894d431a768a35dc7ca4d762ebdba4f407b978c,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",D1145 - D1152,2019.0,https://www.galloway-garcia.com/category/blogcategory.asp,Computer Science
22,456c011594ecacdd24298a161787389ccbe4b88b,Big Data Service Architecture: A Survey,"As one of the main development directions in the information field, big data technology can be applied for data mining, data analysis and data sharing in the massive data, and it created huge economic benefits by using the potential value of data. Meanwhile, it can provide decision-making strategies for social and economic development. Big data service architecture is a new service economic model that takes data as a resource, and it loads and extracts the data collected from different data sources. This service architecture provides various customized data processing methods, data analysis and visualization services for service consumers. This paper first briefly introduces the general big data service architecture and the technical processing framework, which covered data collection and storage. Next, we discuss big data processing and analysis according to different service requirements, which can present valuable data for service consumers. Then, we introduce the detailed cloud computing service system based on big data, which provides high performance solutions for large-scale data storage, processing and analysis. Finally, we summarize some big data application scenarios over various fields.",393-405,2020.0,https://www.haynes.com/search/searchpost.htm,Computer Science
23,17fca92ffd527c78c5dc6c7953e96671743807fa,Big Data Analytics Capabilities and Innovation: The Mediating Role of Dynamic Capabilities and Moderating Effect of the Environment,"With big data analytics growing rapidly in popularity, academics and practitioners have been considering the means through which they can incorporate the shifts these technologies bring into their competitive strategies. Drawing on the resource&#8208;based view, the dynamic capabilities view, and on recent literature on big data analytics, this study examines the indirect relationship between a big data analytics capability (BDAC) and two types of innovation capabilities: incremental and radical. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which in turn positively impact incremental and radical innovation capabilities. To test their proposed research model, the authors used survey data from 175 chief information officers and IT managers working in Greek firms. By means of partial least squares structural equation modelling, the results confirm the authors&#8217; assumptions regarding the indirect effect that BDACs have on innovation capabilities. Specifically, they find that dynamic capabilities fully mediate the effect on both incremental and radical innovation capabilities. In addition, under conditions of high environmental heterogeneity, the impact of BDACs on dynamic capabilities and, in sequence, incremental innovation capability is enhanced, while under conditions of high environmental dynamism the effect of dynamic capabilities on incremental innovation capabilities is amplified.",59-125,2019.0,https://peck.com/postsauthor.htm,Computer Science
24,10d89b13a6309a531c35701d37d3bd76a27a3942,Big Data Storage,,119-141,2021.0,http://jimenez.com/posts/wp-contentprivacy.html,Computer Science
25,78aab73ed574393ab421f25b3a0e3f7343e64748,Big Data and Big Data Analytics,,55-136,2022.0,https://www.james.biz/posts/categoriesprivacy.html,Technology
26,1d174f0e3c391368d0f3384a144a6c7487f2a143,Big Data's Disparate Impact,"Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law — more particularly, through Title VII’s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining’s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission’s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others’ discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data’s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of “discrimination” and “fairness.”",671,2016.0,http://lopez.org/tag/listregister.asp,Sociology
27,b52db9e41e15f76bdcfbe674abe0314af545c430,The Rise of “Big Data” on Cloud Computing,,39-51,2021.0,https://www.hayden.net/wp-content/blogpost.html,Computer Science
28,b34fc78de28be598e21118d7cb9d84d63374addc,Analysis of Dimensionality Reduction Techniques on Big Data,"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.",54776-54788,2020.0,http://haynes.net/appprivacy.html,Computer Science
29,48fc9c42522184c652742255fdf31f7b9ed7ebae,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",57 - 69,2020.0,http://www.holt.info/list/search/appmain.php,Medicine
30,02a88547d6f6022bebc9aba6723a310cdf132f3f,Big Data,,281-287,2014.0,https://www.hall.com/category/main/categoriesauthor.php,Computer Science
31,b6b7fea1846e85ac1e3c7e3adda6e65b127d0368,"IoT, Big Data, and Artificial Intelligence in Agriculture and Food Industry","Internet of Things (IoT) results in a massive amount of streaming data, often referred to as “big data,” which brings new opportunities to monitor agricultural and food processes. Besides sensors, big data from social media is also becoming important for the food industry. In this review, we present an overview of IoT, big data, and artificial intelligence (AI), and their disruptive role in shaping the future of agri-food systems. Following an introduction to the fields of IoT, big data, and AI, we discuss the role of IoT and big data analysis in agriculture (including greenhouse monitoring, intelligent farm machines, and drone-based crop imaging), supply chain modernization, social media (for open innovation and sentiment analysis) in food industry, food quality assessment (using spectral methods and sensor fusion), and finally, food safety (using gene sequencing and blockchain-based digital traceability). A special emphasis is laid on the commercial status of applications and translational research outcomes.",6305-6324,2020.0,https://www.hogan.net/blog/searchregister.asp,Computer Science
32,933baeec555352784848a93284c9dd0e79477759,Big Data in Smart Farming – A review,,69-80,2017.0,http://chandler-skinner.com/blog/maincategory.html,Economics
33,2660dcf5bd16d14862a7bbb241fa4d85ae34327f,"The rise of ""big data"" on cloud computing: Review and open research issues",,98-115,2015.0,https://www.fischer-butler.info/main/exploreregister.jsp,Computer Science
34,bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b,"Big data in healthcare: management, analysis and future prospects",,52-149,2019.0,http://www.perez-allen.com/category/categories/tagsauthor.htm,Computer Science
35,9e3816be8cf4821d74e258de10ee471382936a30,Privacy in the age of medical big data,,37 - 43,2019.0,https://murphy.com/explorelogin.asp,Business
36,dbabab9bf5955558f73a37644f4bb626106a6d73,Big Data Analytics in Intelligent Transportation Systems: A Survey,"Big data is becoming a research focus in intelligent transportation systems (ITS), which can be seen in many projects around the world. Intelligent transportation systems will produce a large amount of data. The produced big data will have profound impacts on the design and application of intelligent transportation systems, which makes ITS safer, more efficient, and profitable. Studying big data analytics in ITS is a flourishing field. This paper first reviews the history and characteristics of big data and intelligent transportation systems. The framework of conducting big data analytics in ITS is discussed next, where the data source and collection methods, data analytics methods and platforms, and big data analytics application categories are summarized. Several case studies of big data analytics applications in intelligent transportation systems, including road traffic accidents analysis, road traffic flow prediction, public transportation service plan, personal travel route plan, rail transportation management and control, and assets maintenance are introduced. Finally, this paper discusses some open challenges of using big data analytics in ITS.",383-398,2019.0,https://www.adams.net/tag/categories/taghomepage.htm,Computer Science
37,5bc511aa30f72720260d792e57537379fb04c395,Sentiment Analysis in Tourism: Capitalizing on Big Data,"Advances in technology have fundamentally changed how information is produced and consumed by all actors involved in tourism. Tourists can now access different sources of information, and they can generate their own content and share their views and experiences. Tourism content shared through social media has become a very influential information source that impacts tourism in terms of both reputation and performance. However, the volume of data on the Internet has reached a level that makes manual processing almost impossible, demanding new analytical approaches. Sentiment analysis is rapidly emerging as an automated process of examining semantic relationships and meaning in reviews. In this article, different sentiment analysis approaches applied in tourism are reviewed and assessed in terms of the datasets used and performances on key evaluation metrics. The article concludes by outlining future research avenues to further advance sentiment analysis in tourism as part of a broader Big Data approach.",175 - 191,2019.0,http://brown-murphy.com/search/blogpost.html,Business
38,247dec05283a1a521f99253a6cca6a5858cac0d2,"Big Data and Predictive Analytics and Manufacturing Performance: Integrating Institutional Theory, Resource‐Based View and Big Data Culture","The importance of big data and predictive analytics has been at the forefront of research for operations and manufacturing management. The literature has reported the influence of big data and predictive analytics for improved supply chain and operational performance, but there has been a paucity of literature regarding the role of external institutional pressures on the resources of the organization to build big data capability. To address this gap, this paper draws on the resource‐based view of the firm, institutional theory and organizational culture to develop and test a model that describes the importance of resources for building capabilities, skills and big data culture and subsequently improving cost and operational performance. We test our research hypotheses using 195 surveys, gathered using a pre‐tested questionnaire. Our contribution lies in providing insights regarding the role of external pressures on the selection of resources under the moderating effect of big data culture and their utilization for capability building, and how this capability affects cost and operational performance.",67-106,2019.0,http://mitchell.com/posts/taghomepage.html,Computer Science
39,391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",97-107,2016.0,http://bailey.info/posts/tagauthor.html,Computer Science
40,b473e91cbe80c8b46451b49153cd5f93030480ab,Critical analysis of Big Data challenges and analytical methods,,263-286,2017.0,https://ho-perez.com/search/search/listprivacy.php,Psychology
41,b080d072cfde697180db3234da08903c092e72c3,Big data analytics capabilities and knowledge management: impact on firm performance,"
Purpose
Big data analytics (BDA) guarantees that data may be analysed and categorised into useful information for businesses and transformed into big data related-knowledge and efficient decision-making processes, thereby improving performance. However, the management of the knowledge generated from the BDA as well as its integration and combination with firm knowledge have scarcely been investigated, despite an emergent need of a structured and integrated approach. The paper aims to discuss these issues.


Design/methodology/approach
Through an empirical analysis based on structural equation modelling with data collected from 88 Italian SMEs, the authors tested if BDA capabilities have a positive impact on firm performances, as well as the mediator effect of knowledge management (KM) on this relationship.


Findings
The findings of this paper show that firms that developed more BDA capabilities than others, both technological and managerial, increased their performances and that KM orientation plays a significant role in amplifying the effect of BDA capabilities.


Originality/value
BDA has the potential to change the way firms compete through better understanding, processing, and exploiting of huge amounts of data coming from different internal and external sources and processes. Some managerial and theoretical implications are proposed and discussed in light of the emergence of this new phenomenon.
",41-147,2019.0,https://www.clayton.com/search/tags/searchhome.htm,Computer Science
42,dac7344737cb824634f757aede2dd46a6eed204b,Big data analytics in healthcare: promise and potential,,79-135,2014.0,http://kelly.com/explorepost.jsp,Medicine
43,9810bcaf5ac1792e6a2738a86f85ce270d448040,Flexible and durable wood-based triboelectric nanogenerators for self-powered sensing in athletic big data analytics,,87-118,2019.0,http://www.frost-castro.biz/tag/categorieshome.html,Medicine
44,aca6d5f3866372a4506cf15773ae298f18c3f453,A comprehensive survey of anomaly detection techniques for high dimensional big data,,60-136,2020.0,https://www.perry.biz/postsabout.asp,Computer Science
45,82870bc488b57cdf5ea62877109a7278af2926b3,Big Data and Artificial Intelligence Modeling for Drug Discovery.,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",22-125,2020.0,http://white.com/list/bloglogin.asp,Computer Science
46,3dfa820702b6181c9964931f0a4d47fd298bf429,Mining Big Data in Education: Affordances and Challenges,"The emergence of big data in educational contexts has led to new data-driven approaches to support informed decision making and efforts to improve educational effectiveness. Digital traces of student behavior promise more scalable and finer-grained understanding and support of learning processes, which were previously too costly to obtain with traditional data sources and methodologies. This synthetic review describes the affordances and applications of microlevel (e.g., clickstream data), mesolevel (e.g., text data), and macrolevel (e.g., institutional data) big data. For instance, clickstream data are often used to operationalize and understand knowledge, cognitive strategies, and behavioral processes in order to personalize and enhance instruction and learning. Corpora of student writing are often analyzed with natural language processing techniques to relate linguistic features to cognitive, social, behavioral, and affective processes. Institutional data are often used to improve student and administrational decision making through course guidance systems and early-warning systems. Furthermore, this chapter outlines current challenges of accessing, analyzing, and using big data. Such challenges include balancing data privacy and protection with data sharing and research, training researchers in educational data science methodologies, and navigating the tensions between explanation and prediction. We argue that addressing these challenges is worthwhile given the potential benefits of mining big data in education.",130 - 160,2020.0,http://www.gillespie.biz/mainindex.php,Psychology
47,718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95,Axes of a revolution: challenges and promises of big data in healthcare,,29 - 38,2020.0,http://olsen.com/postshomepage.php,Business
48,046eb47d56beb8069b0098e3d01608f81ebb6849,"Uncertainty in big data analytics: survey, opportunities, and challenges",,1-16,2019.0,http://www.parker-boyd.net/searchindex.htm,Computer Science
49,3b217403302f9cb9d9685404c7646de7bc0db428,"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data",,314-347,2014.0,http://flynn-wolf.org/tag/wp-contenthomepage.htm,Computer Science
50,91fc647899f801c9d351349ce73779918f90a713,Big data and machine learning algorithms for health-care delivery.,,"
          e262-e273
        ",2019.0,https://krueger.biz/list/tag/categorieslogin.htm,Medicine
51,3789eb72c32ecf5e33442570358dd786dd67c8a2,Text Mining in Big Data Analytics,"Text mining in big data analytics is emerging as a powerful tool for harnessing the power of unstructured textual data by analyzing it to extract new knowledge and to identify significant patterns and correlations hidden in the data. This study seeks to determine the state of text mining research by examining the developments within published literature over past years and provide valuable insights for practitioners and researchers on the predominant trends, methods, and applications of text mining research. In accordance with this, more than 200 academic journal articles on the subject are included and discussed in this review; the state-of-the-art text mining approaches and techniques used for analyzing transcripts and speeches, meeting transcripts, and academic journal articles, as well as websites, emails, blogs, and social media platforms, across a broad range of application areas are also investigated. Additionally, the benefits and challenges related to text mining are also briefly outlined.",1,2020.0,http://www.long-singh.net/categories/wp-content/categoryprivacy.html,Computer Science
52,8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801,"On big data, artificial intelligence and smart cities",,41-113,2019.0,https://goodwin.net/blog/tag/searchhomepage.asp,Business
53,752604994a7ca548ff2954114fc61a501d857b1c,Big data analytics and firm performance: Effects of dynamic capabilities,,356-365,2017.0,http://hooper.org/tagshome.html,Computer Science
54,c24d47ff95cd4bda073c75ec24ececaa3b10c995,A survey of data partitioning and sampling methods to support big data analysis,"Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratified sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and block-level sampling. Record-level sampling is not as efficient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.",85-101,2020.0,https://mcdonald.com/category/categoriesmain.php,Computer Science
55,e8b7a9be9f2d0578a95319ed5841978e10429967,Big data management in the mining industry,,131-139,2020.0,http://oliver.com/wp-content/mainmain.html,Business
56,80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf,Digital Twin and Big Data Towards Smart Manufacturing and Industry 4.0: 360 Degree Comparison,"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.",3585-3593,2018.0,http://www.gonzalez.com/explorefaq.htm,Computer Science
57,75c364909914f17791837ec88090262aa6656d3e,Big data in IBD: big progress for clinical practice,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",1520 - 1532,2020.0,https://vance.com/explorefaq.asp,Computer Science
58,18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",2566-2573,2020.0,http://pratt.biz/postsprivacy.html,Computer Science
59,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,Deep learning applications and challenges in big data analytics,,27-138,2015.0,https://www.adams.info/tags/poststerms.htm,Computer Science
60,c48e0bd0f36c25ab83befbc7b7da369b75fd25f5,Big Data-Survey,"Big data is the term for any gathering of information sets, so expensive and complex, that it gets to be hard to process for utilizing customary information handling applications. The difficulties incorporate investigation, catch, duration, inquiry, sharing, stockpiling, Exchange, perception, and protection infringement. To reduce spot business patterns, anticipate diseases, conflict etc., we require bigger data sets when compared with the smaller data sets. Enormous information is hard to work with utilizing most social database administration frameworks and desktop measurements and perception bundles, needing rather enormously parallel programming running on tens, hundreds, or even a large number of servers. In this paper there was an observation on Hadoop architecture, different tools used for big data and its security issues.",74-80,2016.0,http://johnston.com/categories/categorieshome.html,Computer Science
61,ba9b6f805feb62c978d384211f910790643a023e,Big data monetization throughout Big Data Value Chain: a comprehensive review,,1-22,2020.0,http://www.mcbride.com/taghomepage.jsp,Computer Science
62,a60a4e5f7f872b9825ddff5d379857c2091ca52b,Current landscape and influence of big data on finance,,25-106,2020.0,https://farrell-newton.com/wp-content/wp-contentmain.asp,Computer Science
63,cc1e82125f7f8636b25ccdcdb63e8f812add7f87,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",211-222,2020.0,http://www.ford-lawrence.org/searchcategory.htm,Computer Science
64,0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,Big Data and Its Applications in Smart Real Estate and the Disaster Management Life Cycle: A Systematic Analysis,"Big data is the concept of enormous amounts of data being generated daily in different fields due to the increased use of technology and internet sources. Despite the various advancements and the hopes of better understanding, big data management and analysis remain a challenge, calling for more rigorous and detailed research, as well as the identifications of methods and ways in which big data could be tackled and put to good use. The existing research lacks in discussing and evaluating the pertinent tools and technologies to analyze big data in an efficient manner which calls for a comprehensive and holistic analysis of the published articles to summarize the concept of big data and see field-specific applications. To address this gap and keep a recent focus, research articles published in last decade, belonging to top-tier and high-impact journals, were retrieved using the search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of 139 relevant research articles. Different analyses were conducted on the retrieved papers including bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and affiliated institutes contributing the most to the field of big data. The comparative analyses show that, conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and computing, data management, data refining, data patterns, and machine learning. The results further show that major characteristics of big data can be summarized using the seven Vs, which include variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing methods for big data analysis, their shortcomings, and the possible directions were also explored that could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis, and visualization of the large heterogeneous data, which can be tackled through authentication such as Kerberos and encrypted files, logging of attacks, secure communication through Secure Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models, dividing computations into sub-tasks, checkpoint applications for recursive tasks, and using Solid State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big data management, two frameworks exist including Hadoop and Apache Spark, which must be used simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift, and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e., smart real estate and disaster management, were investigated, and a framework for field-specific applications, as well as a merger of the two areas through big data, was highlighted. The proposed frameworks show that big data can tackle the ever-present issues of customer regrets related to poor quality of information or lack of information in smart real estate to increase the customer satisfaction using an intermediate organization that can process and keep a check on the data being provided to the customers by the sellers and real estate managers. Similarly, for disaster and its risk management, data from social media, drones, multimedia, and search engines can be used to tackle natural disasters such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger framework for smart real estate and disaster risk management show that big data generated from the smart real estate in the form of occupant data, facilities management, and building integration and maintenance can be shared with the disaster risk management and emergency response teams to help prevent, prepare, respond to, or recover from the disasters.",4,2020.0,http://petersen-ho.biz/tag/explore/categoriesabout.htm,Computer Science
65,ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,COVID-19 is spatial: Ensuring that mobile Big Data is used for social good,"The mobility restrictions related to COVID-19 pandemic have resulted in the biggest disruption to individual mobilities in modern times. The crisis is clearly spatial in nature, and examining the geographical aspect is important in understanding the broad implications of the pandemic. The avalanche of mobile Big Data makes it possible to study the spatial effects of the crisis with spatiotemporal detail at the national and global scales. However, the current crisis also highlights serious limitations in the readiness to take the advantage of mobile Big Data for social good, both within and beyond the interests of health sector. We propose two strategical pathways for the future use of mobile Big Data for societal impact assessment, addressing access to both raw mobile Big Data as well as aggregated data products. Both pathways require careful considerations of privacy issues, harmonized and transparent methodologies, and attention to the representativeness, reliability and continuity of data. The goal is to be better prepared to use mobile Big Data in future crises.",22-109,2020.0,http://richard.org/search/tagshomepage.htm,Geography
66,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",26-115,2019.0,http://www.obrien.biz/categories/categoriesauthor.htm,Medicine
67,7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,The value of Big Data in government: The case of ‘smart cities’,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",30-132,2020.0,http://haynes-goodman.com/blogabout.html,Business
68,6aca07154c111f1c8738347d7112cad6b0bf974a,Customer churn prediction in telecom using machine learning in big data platform,,77-107,2019.0,http://www.hernandez-martinez.net/app/categories/explorehomepage.html,Computer Science
69,0a829289a16ae48837cc2905635435db98bacc76,"The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement","The use of big data by the police is part of the very broad discussion of the use of big data in every aspect of our lives. It is the newest member of a small club—whose founding members were death and taxes—of things we cannot avoid. Even before the scandals concerning the alleged meddling of foreign governments in our elections and the revelations about Facebook’s targeted and politicized ads, we all really knew that we were being watched and monitored carefully. The interface between the Internet and GPS pretty much reveals our life story. And people who somehow find a way to disappear from the grid become persons of interest for precisely this reason. It’s why we should think twice about buying an airline ticket with cash. Trying to understand these issues is also a reminder that the day of discrete academic disciplines has gone. Andrew Ferguson’s The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement has to be a multidisciplinary effort to explain something that is part social science, part engineering, and part law. It also resonates for sociologists, particularly sociological theorists, steeped in Michel Foucault’s work. He famously declared that around 1800 a new ‘‘episteme’’ emerged—a new set of institutions, ideas, and technologies that ushered in a new era of discipline and surveillance. Foucault’s appropriation of Bentham’s Panopticon led him to coin the term ‘‘panopticism’’ that anticipated many of the themes that inform Ferguson’s research. For Foucault, controlling criminals through extensive monitoring led all too easily to the control of entire populations of the assumed innocent and even to the people doing the monitoring itself. Ferguson provides us with an up-to-date, high-tech Foucault packaged in an engaging and timely book. As he makes clear in the early chapters, he is very aware of the general issues of big data and mundane surveillance, despite his primary interest in police practices. To this end, Ferguson distinguishes four issues: black, blue, bright, and no data. By ‘‘black data,’’ Ferguson not only highlights the racial inequities and legal concerns that are part of these new policing technologies but also the opacity of the technology itself. That is, the algorithms and computer programs that constitute big data policing are so complicated that accountability is compromised. As Ferguson puts it, ‘‘Policing is not a field that typically requires PhD-level technological expertise’’ (p. 136). Although Ferguson tacitly accepts the technical difficulty of the material, I wondered whether he could have done more to confront the technological opacity of black data. After all, molecular biology is also a tremendously difficult field, but there are ways of making its discoveries accessible. ‘‘Blue data’’ are quite different: they collect information about police practices and provide oversight. Efforts to police the police are the hallmark of blue data, and Ferguson credits the Obama administration with the creation of a collaboration between interested parties that has led to 90 data sets drawn from 50 local jurisdictions (p. 166). Advocates of ‘‘bright data’’ hope that big data can be used to make social programs more efficient by identifying and anticipating risk. Thus, a mother who is identified as someone who is struggling to feed her family could be sent information about food pantries. Ferguson also confronts the twin problems of missing data and expensive data. He estimates that at best big data policing makes use of only 50 percent of crime data and acknowledges that this opens the door to distortions and mistakes (p. 181). Ferguson also recognizes that both the costs and cost-benefits of big data are critical. However, here I was surprised by how little attention the topic actually received—just a page in the middle of the discussion of data concerns in Chapter Ten. Earlier Ferguson had noted very elegantly that ‘‘To find a guilty needle in the haystack, you have to collect a lot of innocent hay’’ (p. 110), and of course this raises the question 170 Reviews",170 - 171,2019.0,http://www.maldonado.org/listhomepage.html,Political Science
70,3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779,"Big data analytics for manufacturing internet of things: opportunities, challenges and enabling technologies","ABSTRACT Data analytics in massive manufacturing data can extract huge business values while can also result in research challenges due to the heterogeneous data types, enormous volume and real-time velocity of manufacturing data. This paper provides an overview on big data analytics in manufacturing Internet of Things (MIoT). This paper first starts with a discussion on necessities and challenges of big data analytics in manufacturing data of MIoT. Then, the enabling technologies of big data analytics of manufacturing data are surveyed and discussed. Moreover, this paper also outlines the future directions in this promising area.",1279 - 1303,2019.0,https://montoya.com/list/category/wp-contenthome.asp,Computer Science
71,ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b,Big data analytics capability in supply chain agility,"
Purpose
The purpose of this paper is to examine when and how organizations build big data analytics capability (BDAC) to improve supply chain agility (SCA) and gain competitive advantage.


Design/methodology/approach
The authors grounded the theoretical framework in two perspectives: the dynamic capabilities view and contingency theory. To test the research hypotheses, the authors gathered 173 usable responses using a pre-tested questionnaire.


Findings
The results suggest that BDAC has a positive and significant effect on SCA and competitive advantage. Further, the results support the hypothesis that organizational flexibility (OF) has a positive and significant moderation effect on the path joining BDAC and SCA. However, contrary to the belief, the authors found no support for the moderation effect of OF on the path joining BDAC and competitive advantage.


Originality/value
The study makes some useful contributions to the literature on BDAC, SCA, OF, and competitive advantage. Moreover, the results may further motivate future scholars to replicate the findings using longitudinal data.
",46-121,2019.0,http://conrad.info/listfaq.html,Economics
72,1f8a23697562b001082b147779b5eaefd3513d0a,Human migration: the big data perspective,,341 - 360,2020.0,http://bennett.com/app/tags/searchfaq.html,Computer Science
73,78e40584f0d149bf6f98beb5561b7b83cb68e1b1,Assessing the impact of big data on firm innovation performance: Big data is not always better data,,91-126,2020.0,https://chavez-vaughan.com/searchregister.jsp,Business
74,d63b884d5ebc739f6e1bdf861fa9276260781404,Deep Learning for IoT Big Data and Streaming Analytics: A Survey,"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.",2923-2960,2017.0,http://henson.com/app/taghome.php,Computer Science
75,75fb60dee6641656fc4c1b663bb95d1aa91c6798,A Delphi study to build consensus on the definition and use of big data in obesity research,,2573 - 2586,2019.0,https://williams.biz/postsauthor.html,Psychology
76,00d681cfff7f0224ace607e515f4ed510e792df9,The Parable of Google Flu: Traps in Big Data Analysis,"Large errors in flu prediction were largely avoidable, which offers lessons for the use of big data. In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?",1203 - 1205,2014.0,https://www.martin.com/tags/tags/mainindex.htm,Medicine
77,ca0e479ba2327f71e842d033b6b48b082962cc6a,Big data and analytics: a data management perspective in public administration,"In recent years, data analytics has enabled the policy makers to improve the accuracy levels of results while framing policies and strategies. This research field still has great potential waiting to be tapped, which would help to mitigate the challenges of public administration system. The present article introduces the concept of big data and provides a comprehensive overview to readers about the 'big data application framework' in public administration via data driven e-governance (DDeG). The conceptual framework here identifies the inherent possibilities of big data from the perspective of individual citizen as well as the administration. The overall finding of the study has broadened the scope of e-governance by exploring the technological aspects like network of internet (IoT), and artificial intelligence (AI). The author has concluded by pointing, the role of big data processes and its corresponding improved characteristics in public administration.",34-126,2020.0,http://www.campbell.net/category/tagsearch.html,Computer Science
78,efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea,Big Data and Machine Learning in Health Care.,"Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT","
          1317-1318
        ",2018.0,https://www.garcia-lopez.com/postsregister.php,Medicine
79,e7c8fcbc24c73a576339e5f34f9f23f5ea732b3b,Creating Strategic Business Value from Big Data Analytics: A Research Framework,"Abstract Despite the publicity regarding big data and analytics (BDA), the success rate of these projects and strategic value created from them are unclear. Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities, but very few studies examine its impact on organizational value. Further, we see limited framing of how BDA can create strategic value for the organization. After all, the ultimate success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. In this study, we describe the value proposition of BDA by delineating its components. We offer a framing of BDA value by extending existing frameworks of information technology value, then illustrate the framework through BDA applications in practice. The framework is then discussed in terms of its ability to study constructs and relationships that focus on BDA value creation and realization. We also present a problem-oriented view of the framework—where problems in BDA components can give rise to targeted research questions and areas for future study. The framing in this study could help develop a significant research agenda for BDA that can better target research and practice based on effective use of data resources.",388 - 423,2018.0,https://www.ramos-moore.biz/search/listpost.jsp,Computer Science
80,6570ee5bd1656fddbdb030410a9d996739586067,Big Data and Big Data Analytics,"Big data is emerging, and the latest developments in technology have spawned enormous amounts of data. The traditional databases lack the capabilities to handle this diverse data and thus has led to the employment of new technologies, methods, and tools. This research discusses big data, the available big data analytical tools, the need to use big data analytics with its benefits and challenges. Through a research drawing on survey questionnaires, observation of the business processes, interviews and secondary research methods, the organizations, and companies in a small island state are identified to survey which of them use analytical tools to handle big data and the benefits it proposes to these businesses. Organizations and companies that do not use these tools were also surveyed and reasons were outlined as to why these organizations hesitate to utilize such tools.",51-133,2021.0,http://www.reyes-mcfarland.info/tags/category/listabout.html,Computer Science
81,6eefbb79f98e902e9d2efa057bfea174843bf3dc,Analysis of healthcare big data,,103-110,2020.0,https://www.dennis.net/listcategory.html,Computer Science
82,4d1fdd81f033cd58f3723bfc61e7d12079647a7a,"Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.","The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.","
          1216-9
        ",2016.0,http://www.vasquez-cook.biz/list/explore/taghome.html,Medicine
83,de6cf3534f39748a223b9bb2b59d2e7ffcb6ae03,Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not Available.,"Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment-the target experiment or target trial-that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.","
          758-64
        ",2016.0,https://reese-thomas.info/category/tag/wp-contentterms.html,Medicine
84,b4c9f49f726f87791768c37e0b29f5e7c8719f6d,"How can Big Data and machine learning benefit environment and water management: a survey of methods, applications, and future directions","Big Data and machine learning (ML) technologies have the potential to impact many facets of environment and water management (EWM). Big Data are information assets characterized by high volume, velocity, variety, and veracity. Fast advances in high-resolution remote sensing techniques, smart information and communication technologies, and social media have contributed to the proliferation of Big Data in many EWM fields, such as weather forecasting, disaster management, smart water and energy management systems, and remote sensing. Big Data brings about new opportunities for data-driven discovery in EWM, but it also requires new forms of information processing, storage, retrieval, as well as analytics. ML, a subdomain of artificial intelligence (AI), refers broadly to computer algorithms that can automatically learn from data. ML may help unlock the power of Big Data if properly integrated with data analytics. Recent breakthroughs in AI and computing infrastructure have led to the fast development of powerful deep learning (DL) algorithms that can extract hierarchical features from data, with better predictive performance and less human intervention. Collectively Big Data and ML techniques have shown great potential for data-driven decision making, scientific discovery, and process optimization. These technological advances may greatly benefit EWM, especially because (1) many EWM applications (e.g. early flood warning) require the capability to extract useful information from a large amount of data in autonomous manner and in real time, (2) EWM researches have become highly multidisciplinary, and handling the ever increasing data volume/types using the traditional workflow is simply not an option, and last but not least, (3) the current theoretical knowledge about many EWM processes is still incomplete, but which may now be complemented through data-driven discovery. A large number of applications on Big Data and ML have already appeared in the EWM literature in recent years. The purposes of this survey are to (1) examine the potential and benefits of data-driven research in EWM, (2) give a synopsis of key concepts and approaches in Big Data and ML, (3) provide a systematic review of current applications, and finally (4) discuss major issues and challenges, and recommend future research directions. EWM includes a broad range of research topics. Instead of attempting to survey each individual area, this review focuses on areas of nexus in EWM, with an emphasis on elucidating the potential benefits of increased data availability and predictive analytics to improving the EWM research.",95-123,2019.0,https://www.cannon.info/categorysearch.php,Computer Science
85,16575f23ff879e6353a55bbfbbcc54e27606bfc5,Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations,,3-13,2018.0,https://www.nelson-johnson.info/category/appprivacy.asp,Computer Science
86,57dcff5c3c15f6c5e5935ef7164c8f467a53848b,Big data in lean six sigma: a review and further research directions,"Manufacturing and service organisations improve their processes on a continuous basis to have better operational performance. They use lean six sigma (LSS) projects for process improvement. Therefore, this study aims to investigate the existing literature in LSS and the application of big data analytics (BDA) to have more confident and predictable decisions in each phase of LSS. Fifty-two articles have been identified after a careful and vigilant screening of closely related themes. Future research directions in the big data and LSS have been highlighted on the basis of organisational theories. Review presents an investigation framework consisting of BDA techniques applicable to each phase of LSS in all the dimensions such as volume, variety, velocity and veracity of big data. Review highlights the concerns of big data in LSS such as system design and integration, system performance, security and reliability of data, sustaining the control and conducting the experiments, distributed material and information flow. The review unveils the application of 8 modern organisational theories to big data in LSS with 21 key aspects of related theories and 19 distinct research gaps as opportunities for future research.",947 - 969,2019.0,https://terry-williams.net/categories/tagindex.html,Computer Science
87,65499384f0947e223ec83f582366203e7aa64f1f,The state of the art and taxonomy of big data analytics: view from new big data framework,,989-1037,2019.0,https://shaffer.com/listpost.html,Computer Science
88,521e5c337be51b8f8fdb858580bb46a0545ab1f9,When Gaussian Process Meets Big Data: A Review of Scalable GPs,"The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.",4405-4423,2018.0,http://young.com/app/main/categoriesregister.html,Medicine
89,c7e10109fed73e523bf2a78b00e4b657004d41fc,Big data and dynamic capabilities: a bibliometric analysis and systematic literature review,"
Purpose
Recently, several manuscripts about the effects of big data on organizations used dynamic capabilities as their main theoretical approach. However, these manuscripts still lack systematization. Consequently, the purpose of this paper is to systematize the literature on big data and dynamic capabilities.


Design/methodology/approach
A bibliometric analysis was performed on 170 manuscripts extracted from the Clarivate Analytics Web of Science Core Collection database. The bibliometric analysis was integrated with a literature review.


Findings
The bibliometric analysis revealed four clusters of papers on big data and dynamic capabilities: big data and supply chain management, knowledge management, decision making, business process management and big data analytics. The systematic literature review helped to clarify each clusters’ content.


Originality/value
To the authors’ best knowledge, minimal attention has been paid to systematizing the literature on big data and dynamic capabilities.
",55-143,2019.0,http://www.moore-ramirez.com/explore/main/categoriesabout.html,Economics
90,243d553f1fc5b7ee8cfb6f49629f3a0a32b2c5c5,Big Data Analytics in Operations Management,"Big data analytics is critical in modern operations management (OM). In this study, we first explore the existing big data‐related analytics techniques, and identify their strengths, weaknesses as well as major functionalities. We then discuss various big data analytics strategies to overcome the respective computational and data challenges. After that, we examine the literature and reveal how different types of big data methods (techniques, strategies, and architectures) can be applied to different OM topical areas, namely forecasting, inventory management, revenue management and marketing, transportation management, supply chain management, and risk analysis. We also investigate via case studies the real‐world applications of big data analytics in top branded enterprises. Finally, we conclude the study with a discussion of future research.",1868 - 1883,2018.0,https://www.mccullough.org/categoriessearch.html,Computer Science
91,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",1-18,2018.0,https://www.mclean.com/tags/categorycategory.php,Economics
92,0a537eaf4444f0bd59f62a550430edab9d0a820d,A survey on addressing high-class imbalance in big data,,1-30,2018.0,http://www.barrett-meyers.com/tag/explore/mainregister.htm,Computer Science
93,1fe201d9ec4d072c7af233d265927388f32d3ab3,Optimization of Big Data Scheduling in Social Networks,"In social network big data scheduling, it is easy for target data to conflict in the same data node. Of the different kinds of entropy measures, this paper focuses on the optimization of target entropy. Therefore, this paper presents an optimized method for the scheduling of big data in social networks and also takes into account each task’s amount of data communication during target data transmission to construct a big data scheduling model. Firstly, the task scheduling model is constructed to solve the problem of conflicting target data in the same data node. Next, the necessary conditions for the scheduling of tasks are analyzed. Then, the a periodic task distribution function is calculated. Finally, tasks are scheduled based on the minimum product of the corresponding resource level and the minimum execution time of each task is calculated. Experimental results show that our optimized scheduling model quickly optimizes the scheduling of social network data and solves the problem of strong data collision.",61-108,2019.0,https://www.baxter.com/blogprivacy.html,Computer Science
94,3527f43e8156d1bf5a0998405047c98036d172f0,Internet of vehicles in big data era,"As the rapid development of automotive telematics, modern vehicles are expected to be connected through heterogeneous radio access technologies and are able to exchange massive information with their surrounding environment. By significantly expanding the network scale and conducting both real time and long term information processing, the traditional Vehicular Ad- Hoc Networks U+0028 VANETs U+0029 are evolving to the Internet of Vehicles U+0028 IoV U+0029, which promises efficient and intelligent prospect for the future transportation system. On the other hand, vehicles are not only consuming but also generating a huge amount and enormous types of data, which are referred to as Big Data. In this article, we first investigate the relationship between IoV and big data in vehicular environment, mainly on how IoV supports the transmission, storage, computing of the big data, and in return how IoV benefits from big data in terms of IoV characterization, performance evaluation and big data assisted communication protocol design. We then investigate the application of IoV big data for autonomous vehicles. Finally the emerging issues of the big data enabled IoV are discussed.",19-35,2018.0,http://www.king.com/categories/explore/exploreabout.asp,Computer Science
95,a5400a6f415d65aac1312edd2ca6b7361c241f85,Challenges and Opportunities with Big Data,"- Data is exploding at a rapid rate. Big data is the term for data sets so large and complicated that it becomes difficult to process using traditional data management tools or processing applications. Heterogeneity, scale, timeliness, complexity, and privacy problems with Big Data impede progress at all phases of the pipeline that can create value from data. The problems start right away during data acquisition, when the data tsunami requires us to make decisions, currently in an ad hoc manner, about what data to keep and what to discard, and how to store what we keep reliably with the right metadata. Much data today is not natively in structured format, tweets and blogs are weakly structured pieces of text, while images and video are structured for storage and display, but not for semantic content and search. Transforming such content into a structured format for later analysis is a major challenge. The value of data explodes when it can be linked with other data, thus data integration is a major creator of value. Since most data is directly generated in digital format today, we have the opportunity and the challenge both to influence the creation to facilitate later linkage and to automatically link previously created data. Data analysis, organization, retrieval, and modeling are other foundational challenges. Data analysis is a clear bottleneck in many applications, both due to lack of scalability of the underlying algorithms and due to the complexity of the data that needs to be analyzed. Finally, presentation of the results and its interpretation by non-technical domain experts is crucial to extracting actionable knowledge.",16-20,2017.0,https://clarke.net/tagsfaq.php,Computer Science
96,e3d74f13374063a86909c7b0a8105be94902b7f4,Role of big data analytics in developing sustainable capabilities,,20-147,2019.0,http://www.anderson.info/explore/categories/exploreprivacy.htm,Business
97,0608fc7e1825c4ac1f61aaa95b67f155d93b0ea4,On the sustainability of smart and smarter cities in the era of big data: an interdisciplinary and transdisciplinary literature review,,1-64,2019.0,https://cisneros-thompson.biz/categoryindex.php,Computer Science
98,adf914b04914a9b856e26c575c789ba19f9f118d,CherryPick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics,"Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to 75% search cost compared to existing solutions.",469-482,2017.0,https://www.marshall.com/mainabout.asp,Computer Science
99,da619a6c524f5ab800b44c8728db3cef3d3b25d9,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",15-150,2014.0,https://www.perez.biz/searchcategory.php,Sociology
100,6eb5744bd933dcd29ff4d79e340cf890272ef099,Big Data for Health,,95-105,2019.0,http://www.rice.com/categories/list/tagauthor.html,Computer Science
101,606b5be1f747387e908674d89df152e85295f0b4,Investigating the adoption of big data analytics in healthcare: the moderating role of resistance to change,,1-20,2019.0,http://garcia-collier.org/exploreregister.asp,Computer Science
102,0c7ae2c9e8c91edd6801f5578176b28e0a0414c6,"Big Data and Business Analytics: Trends, Platforms, Success Factors and Applications","Big data and business analytics are trends that are positively impacting the business world. Past researches show that data generated in the modern world is huge and growing exponentially. These include structured and unstructured data that flood organizations daily. Unstructured data constitute the majority of the world’s digital data and these include text files, web, and social media posts, emails, images, audio, movies, etc. The unstructured data cannot be managed in the traditional relational database management system (RDBMS). Therefore, data proliferation requires a rethinking of techniques for capturing, storing, and processing the data. This is the role big data has come to play. This paper, therefore, is aimed at increasing the attention of organizations and researchers to various applications and benefits of big data technology. The paper reviews and discusses, the recent trends, opportunities and pitfalls of big data and how it has enabled organizations to create successful business strategies and remain competitive, based on available literature. Furthermore, the review presents the various applications of big data and business analytics, data sources generated in these applications and their key characteristics. Finally, the review not only outlines the challenges for successful implementation of big data projects but also highlights the current open research directions of big data analytics that require further consideration. The reviewed areas of big data suggest that good management and manipulation of the large data sets using the techniques and tools of big data can deliver actionable insights that create business values.",32,2019.0,https://www.cole.com/category/wp-content/tagshome.asp,Computer Science
103,4068c3303340ed9796463a2064a74f2dc6ea5795,Big data stream analysis: a systematic literature review,,1-30,2019.0,http://www.stevens.com/appcategory.php,Computer Science
104,e799d31e1c2d80a971c1f956d62b98c0a9f27031,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",101-113,2019.0,https://www.bullock-cantrell.biz/posts/search/categorycategory.php,Computer Science
105,5cbd591298595fb575f1d01e3db910ee1f4573fb,Circular economy and big data analytics: A stakeholder perspective,,75-145,2019.0,http://sanchez-wagner.info/search/postsindex.html,Business
106,8cc8f762a52936f0d6baddb4f1bd5c06f1062605,Social media big data analytics: A survey,,417-428,2019.0,https://brown.biz/list/category/explorefaq.jsp,Computer Science
107,0eb35b498e3e9d5b51e5d7f9435206b99ce22505,An analytical study of information extraction from unstructured and multidimensional big data,,1-38,2019.0,https://elliott-humphrey.com/blog/categoriesfaq.php,Computer Science
108,064eec8458596ee45e9c1787086afdc9352a5ef7,"Big data analytics for healthcare industry: impact, applications, and tools","In recent years, huge amounts of structured, unstructured, and semi-structured data have been generated by various institutions around the world and, collectively, this heterogeneous data is referred to as big data. The health industry sector has been confronted by the need to manage the big data being produced by various sources, which are well known for producing high volumes of heterogeneous data. Various big-data analytics tools and techniques have been developed for handling these massive amounts of data, in the healthcare sector. In this paper, we discuss the impact of big data in healthcare, and various tools available in the Hadoop ecosystem for handling it. We also explore the conceptual architecture of big data analytics for healthcare which involves the data gathering history of different branches, the genome database, electronic health records, text/imagery, and clinical decisions support system.",48-57,2019.0,http://www.taylor.biz/explore/tagsearch.php,Computer Science
109,1d1645c3bbfcd1e45677aabbeffe1f0df2397bf6,A bibliometric analysis of research on Big Data analytics for business and management,"
Purpose
The purpose of this paper is to scrutinize and classify the literature linking Big Data analytics and management phenomena.


Design/methodology/approach
An objective bibliometric analysis is conducted, supported by subjective assessments based on the studies focused on the intertwining of Big Data analytics and management fields. Specifically, deeper descriptive statistics and document co-citation analysis are provided.


Findings
From the document co-citation analysis and its evaluation, four clusters depicting literature linking Big Data analytics and management phenomena are revealed: theoretical development of Big Data analytics; management transition to Big Data analytics; Big Data analytics and firm resources, capabilities and performance; and Big Data analytics for supply chain management.


Originality/value
To the best of the authors’ knowledge, this is one of the first attempts to comprehend the research streams which, over time, have paved the way to the intersection between Big Data analytics and management fields.
",78-117,2019.0,https://lawrence.biz/tag/mainauthor.html,Computer Science
110,2cb4eefa839eb9faea2e0cb5fe5882b6c1a7dd78,A novel big data analytics framework for smart cities,,620-633,2019.0,http://nelson.com/searchprivacy.html,Computer Science
111,ac1eeb98c388fa6be4579817721a75ab17b4de59,The Role of Big Data Analytics in Industrial Internet of Things,,51-117,2019.0,https://carr.net/list/tags/tagsauthor.htm,Computer Science
112,e1ff974613f471f91a15a7746fa0cbaf12eda26d,"Big Data and discrimination: perils, promises and solutions. A systematic review",,16-135,2019.0,http://ross.com/categories/wp-contentregister.html,Computer Science
113,cf6225095886b6b203d61539f917956748c4b8cf,Robust Big Data Analytics for Electricity Price Forecasting in the Smart Grid,"Electricity price forecasting is a significant part of smart grid because it makes smart grid cost efficient. Nevertheless, existing methods for price forecasting may be difficult to handle with huge price data in the grid, since the redundancy from feature selection cannot be averted and an integrated infrastructure is also lacked for coordinating the procedures in electricity price forecasting. To solve such a problem, a novel electricity price forecasting model is developed. Specifically, three modules are integrated in the proposed model. First, by merging of Random Forest (RF) and Relief-F algorithm, we propose a hybrid feature selector based on Grey Correlation Analysis (GCA) to eliminate the feature redundancy. Second, an integration of Kernel function and Principle Component Analysis (KPCA) is used in feature extraction process to realize the dimensionality reduction. Finally, to forecast price classification, we put forward a differential evolution (DE) based Support Vector Machine (SVM) classifier. Our proposed electricity price forecasting model is realized via these three parts. Numerical results show that our proposal has superior performance than other methods.",34-45,2019.0,https://mendoza.info/search/tagsindex.html,Computer Science
114,32a616a105870c594476a9bba35dc5c602aae1d7,Big data in IBD: a look into the future,,312-321,2019.0,http://faulkner-ingram.net/tags/postsregister.html,Medicine
115,3edc44af4020554422f4775d4b7ee6ac9188c84e,Big Data for Cybersecurity: Vulnerability Disclosure Trends and Dependencies,"Complex Big Data systems in modern organisations are progressively becoming attack targets by existing and emerging threat agents. Elaborate and specialised attacks will increasingly be crafted to exploit vulnerabilities and weaknesses. With the ever-increasing trend of cybercrime and incidents due to these vulnerabilities, effective vulnerability management is imperative for modern organisations regardless of their size. However, organisations struggle to manage the sheer volume of vulnerabilities discovered on their networks. Moreover, vulnerability management tends to be more reactive in practice. Rigorous statistical models, simulating anticipated volume and dependence of vulnerability disclosures, will undoubtedly provide important insights to organisations and help them become more proactive in the management of cyber risks. By leveraging the rich yet complex historical vulnerability data, our proposed novel and rigorous framework has enabled this new capability. By utilising this sound framework, we initiated an important study on not only handling persistent volatilities in the data but also further unveiling multivariate dependence structure amongst different vulnerability risks. In sharp contrast to the existing studies on univariate time series, we consider the more general multivariate case striving to capture their intriguing relationships. Through our extensive empirical studies using the real world vulnerability data, we have shown that a composite model can effectively capture and preserve long-term dependency between different vulnerability and exploit disclosures. In addition, the paper paves the way for further study on the stochastic perspective of vulnerability proliferation towards building more accurate measures for better cyber risk management as a whole.",317-329,2019.0,http://www.gamble.com/list/explore/listpost.htm,Computer Science
116,abde3d703d5fa39af43f69cd4eeb1b83f85eb4c5,Big Data,,35-138,2021.0,http://www.pope-avila.com/search/categorieshome.asp,Technology
117,0214a84a2a1e9ac5dc9600c7ee40a6ea48a5f95c,Cybersecurity in Big Data Era: From Securing Big Data to Data-Driven Security,"‘‘Knowledge is power” is an old adage that has been found to be true in today’s information age. Knowledge is derived from having access to information. The ability to gather information from large volumes of data has become an issue of relative importance. Big Data Analytics (BDA) is the term coined by researchers to describe the art of processing, storing and gathering large amounts of data for future examination. Data is being produced at an alarming rate. The rapid growth of the Internet, Internet of Things (IoT) and other technological advances are the main culprits behind this sustained growth. The data generated is a reflection of the environment it is produced out of, thus we can use the data we get out of systems to figure out the inner workings of that system. This has become an important feature in cybersecurity where the goal is to protect assets. Furthermore, the growing value of data has made big data a high value target. In this paper, we explore recent research works in cybersecurity in relation to big data. We highlight how big data is protected and how big data can also be used as a tool for cybersecurity. We summarize recent works in the form of tables and have presented trends, open research challenges and problems. With this paper, readers can have a more thorough understanding of cybersecurity in the big data era, as well as research trends and open challenges in this active research area.",2055-2072,2019.0,https://arellano.org/tags/postspost.htm,Computer Science
118,0ae18b28d8dc00cd4641488084ead5df2a449c89,A survey on data storage and placement methodologies for Cloud-Big Data ecosystem,,1-37,2019.0,https://miller.com/tags/wp-content/wp-contenthome.php,Computer Science
119,d22e8ae8236a669053c4646b4859cdf1fadb64c2,Data Colonialism: Rethinking Big Data’s Relation to the Contemporary Subject,"We are often told that data are the new oil. But unlike oil, data are not a substance found in nature. It must be appropriated. The capture and processing of social data unfolds through a process we call data relations, which ensures the “natural” conversion of daily life into a data stream. The result is nothing less than a new social order, based on continuous tracking, and offering unprecedented new opportunities for social discrimination and behavioral influence. We propose that this process is best understood through the history of colonialism. Thus, data relations enact a new form of data colonialism, normalizing the exploitation of human beings through data, just as historic colonialism appropriated territory and resources and ruled subjects for profit. Data colonialism paves the way for a new stage of capitalism whose outlines we only glimpse: the capitalization of life without limit.",336 - 349,2018.0,https://www.griffith.com/mainhome.html,Political Science
120,a7f8b8e6124901c1e22e940092e87b5b93776ab3,Machine Learning With Big Data: Challenges and Approaches,"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.",7776-7797,2017.0,https://www.horne.org/category/blog/tagsterms.php,Computer Science
121,fa863e822087b6101274cece1e2184fdf943a78c,Big data in tourism research: A literature review,,16-123,2018.0,http://dunlap.com/searchhome.htm,Computer Science
122,3ca7f49ce3554ea9366744c54413d8f5e401f48d,Big data analytics capabilities: a systematic literature review and research agenda,,547 - 578,2017.0,http://mcmahon.com/categories/list/wp-contentmain.htm,Computer Science
123,091929400bd0735c19daa1cc5523e336b354aae9,Challenges and opportunities of digital information at the intersection of Big Data Analytics and supply chain management,"Purpose 
 
 
 
 
Despite the variety of supply chain management (SCM) research, little attention has been given to the use of Big Data Analytics for increased information exploitation in a supply chain. The purpose of this paper is to contribute to theory development in SCM by investigating the potential impacts of Big Data Analytics on information usage in a corporate and supply chain context. As it is imperative for companies in the supply chain to have access to up-to-date, accurate, and meaningful information, the exploratory research will provide insights into the opportunities and challenges emerging from the adoption of Big Data Analytics in SCM. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
Although Big Data Analytics is gaining increasing attention in management, empirical research on the topic is still scarce. Due to the limited availability of comparable material at the intersection of Big Data Analytics and SCM, the authors apply the Delphi research technique. 
 
 
 
 
Findings 
 
 
 
 
Portraying the emerging transition trend from a digital business environment, the presented Delphi study findings contribute to extant knowledge by identifying 43 opportunities and challenges linked to the emergence of Big Data Analytics from a corporate and supply chain perspective. 
 
 
 
 
Research limitations/implications 
 
 
 
 
These constructs equip the research community with a first collection of aspects, which could provide the basis to tailor further research at the nexus of Big Data Analytics and SCM. 
 
 
 
 
Originality/value 
 
 
 
 
The research adds to the existing knowledge base as no empirical research has been presented so far specifically assessing opportunities and challenges on corporate and supply chain level with a special focus on the implications imposed through Big Data Analytics.",10-36,2017.0,https://www.stokes.com/mainmain.html,Economics
124,c7e9d4837edecc50c94a854e80807a27865dfd91,Putting the data before the algorithm in big data addressing personalized healthcare,,77-108,2019.0,https://miller.net/app/categoriessearch.html,Computer Science
125,3a83d8595e6727269c876fcebd23ee9ddd524b76,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",1328-1347,2018.0,https://www.brown.com/blog/main/postscategory.htm,Computer Science
126,e59d5ee879e408cccc7c055e44650dcb6f88c86f,Big Data Surveillance: The Case of Policing,"This article examines the intersection of two structural developments: the growth of surveillance and the rise of “big data.” Drawing on observations and interviews conducted within the Los Angeles Police Department, I offer an empirical account of how the adoption of big data analytics does—and does not—transform police surveillance practices. I argue that the adoption of big data analytics facilitates amplifications of prior surveillance practices and fundamental transformations in surveillance activities. First, discretionary assessments of risk are supplemented and quantified using risk scores. Second, data are used for predictive, rather than reactive or explanatory, purposes. Third, the proliferation of automatic alert systems makes it possible to systematically surveil an unprecedentedly large number of people. Fourth, the threshold for inclusion in law enforcement databases is lower, now including individuals who have not had direct police contact. Fifth, previously separate data systems are merged, facilitating the spread of surveillance into a wide range of institutions. Based on these findings, I develop a theoretical model of big data surveillance that can be applied to institutional domains beyond the criminal justice system. Finally, I highlight the social consequences of big data surveillance for law and social inequality.",1008 - 977,2017.0,https://kennedy.com/tag/postscategory.html,Political Science
127,7e6818390a82838ff3b3df1d7c680a14cd5b2e20,Disease Prediction by Machine Learning Over Big Data From Healthcare Communities,"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013–2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.",8869-8879,2017.0,https://www.gutierrez-curry.org/categorieslogin.asp,Computer Science
128,59755487e71adc7d78c8917f8c401c2d083c21f3,A Longitudinal Big Data Approach for Precision Health,,792 - 804,2019.0,http://schaefer.biz/searchindex.php,Medicine
129,a1352af5cdf57823a772efb81b93307db709c128,Health-CPS: Healthcare Cyber-Physical System Assisted by Cloud and Big Data,"The advances in information technology have witnessed great progress on healthcare technologies in various domains nowadays. However, these new technologies have also made healthcare data not only much bigger but also much more difficult to handle and process. Moreover, because the data are created from a variety of devices within a short time span, the characteristics of these data are that they are stored in different formats and created quickly, which can, to a large extent, be regarded as a big data problem. To provide a more convenient service and environment of healthcare, this paper proposes a cyber-physical system for patient-centric healthcare applications and services, called Health-CPS, built on cloud and big data analytics technologies. This system consists of a data collection layer with a unified standard, a data management layer for distributed storage and parallel computing, and a data-oriented service layer. The results of this study show that the technologies of cloud and big data can be used to enhance the performance of the healthcare system so that humans can then enjoy various smart healthcare applications and services.",88-95,2017.0,http://www.huber-ryan.com/categoriesindex.jsp,Engineering
130,00b0ae9246fb444b4862c4e4a40126d776974f7e,Renewing Felsenstein’s Phylogenetic Bootstrap in the Era of Big Data,,452 - 456,2018.0,http://www.avery.net/mainhomepage.asp,Computer Science
131,1046b1d5ad67940beb01a2ef13fbe6fd6d4ec4ec,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,,89,2018.0,https://www.summers.info/blog/app/postsregister.jsp,Technology
132,116927fbe4c9732fd1e392035a100c33b14e9d59,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",13 - 53,2017.0,https://mahoney.com/list/searchmain.html,Computer Science
133,66372bd949105cc705aec7347269a71f82afadfc,"Big Data, Big Data Analytics Capability, and Sustainable Innovation Performance","Literature suggests that big data is a new competitive advantage and that it enhance organizational performance. Yet, previous empirical research has provided conflicting results. Building on the resource-based view and the organizational inertia theory, we develop a model to investigate how big data and big data analytics capability affect innovation success. We show that there is a trade-off between big data and big data analytics capability and that optimal balance of big data depends upon levels of big data analytics capability. We conduct a four-year empirical research project to secure empirical data on 1109 data-driven innovation projects from the United States and China. This research is the first time reporting the empirical results. The study findings reveal several surprising results that challenge traditional views of the importance of big data in innovation. For U.S. innovation projects, big data has an inverted U-shaped relationship with sales growth. Big data analytics capability exerts a positive moderating effect, that is, the stronger this capability is, the greater the impact of big data on sales growth and gross margin. For Chinese innovation projects, when big data resource is low, promoting big data analytics capability increases sales growth and gross margin up to a certain point; developing big data analytics capability beyond that point may actually inhibit innovation performance. Our findings provide guidance to firms on making strategic decisions regarding resource allocations for big data and big data analytics capability.",43-131,2019.0,http://martinez-hunter.com/category/categories/blogmain.asp,Economics
134,fd0e83749943cc43ab4dd6f4317cfd4299c44cfd,"Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives",,1009 - 1030,2019.0,http://www.gibson.com/searchprivacy.asp,Computer Science
135,208261e342a57fdf9818f474bb01271b706f9c2f,Big Data Analysis and Mining,"Big data analysis and mining aims to discover implicit, previously unknown, and potentially useful information and knowledge from big databases that contain high volumes of valuable veracious data collected or generated at a high velocity from a wide variety of data sources. Among different big data mining tasks, this chapter focuses on big data analysis and mining for frequent patterns. By relying on the MapReduce programming model, researchers only need to specify the “map” and “reduce” functions to discover frequent patterns from (1) big databases of precise data in a breadth-first manner or in a depth-first manner and/or from (2) big databases of uncertain data. Such a big data analysis and mining process can be sped up. The resulting (constrained or unconstrained) frequent patterns mined from big databases provide users with new insights and a sound understanding of users' patterns. Such knowledge is useful is many real-life information science and technology applications.",24-144,2019.0,http://www.perez.com/maincategory.htm,Computer Science
136,afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,IoT-Based Big Data Storage Systems in Cloud Computing: Perspectives and Challenges,"Internet of Things (IoT) related applications have emerged as an important field for both engineers and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations especially in cloud computing. This paper first provides a functional framework that identifies the acquisition, management, processing and mining areas of IoT big data, and several associated technical modules are defined and described in terms of their key characteristics and capabilities. Then current research in IoT application is analyzed, moreover, the challenges and opportunities associated with IoT big data research are identified. We also report a study of critical IoT application publications and research topics based on related academic and industry publications. Finally, some open issues and some typical examples are given under the proposed IoT-related research framework.",75-87,2017.0,http://collins.com/app/category/explorepost.html,Computer Science
137,6e737a0e5ef29303760a565ba5e9d98510ab0976,The real-time city? Big data and smart urbanism,,1 - 14,2013.0,http://pace.com/explore/categories/wp-contentlogin.php,Sociology
138,ac89bccb5806eeafc6122a7e27affea4fe8cb809,Big data analytics for preventive medicine,,4417 - 4451,2019.0,http://stephens.biz/main/tag/postsabout.asp,Computer Science
139,6b9f26445f5620746689b91ab451f997b391724c,Big Data and Climate Change,"Climate science as a data-intensive subject has overwhelmingly affected by the era of big data and relevant technological revolutions. The big successes of big data analytics in diverse areas over the past decade have also prompted the expectation of big data and its efficacy on the big problem—climate change. As an emerging topic, climate change has been at the forefront of the big climate data analytics implementations and exhaustive research have been carried out covering a variety of topics. This paper aims to present an outlook of big data in climate change studies over the recent years by investigating and summarising the current status of big data applications in climate change related studies. It is also expected to serve as a one-stop reference directory for researchers and stakeholders with an overview of this trending subject at a glance, which can be useful in guiding future research and improvements in the exploitation of big climate data.",12,2019.0,https://garcia.com/tagslogin.html,Computer Science
140,b7919fadb4c1bf959b1e410463594afacfda7dc6,A survey on deep learning for big data,,146-157,2018.0,http://www.blanchard.com/main/tagspost.php,Computer Science
141,09e72c1ddaa2f5b26f6c3a04583005bddaa030c7,How ‘Big Data’ Can Make Big Impact: Findings from a Systematic Review and a Longitudinal Case Study,,76-149,2015.0,http://www.olson.org/categoryterms.php,Computer Science
142,b2239452680e97c503a90f62ccdc8137a893b1e9,Big Data and quality data for fake news and misinformation detection,"Fake news has become an important topic of research in a variety of disciplines including linguistics and computer science. In this paper, we explain how the problem is approached from the perspective of natural language processing, with the goal of building a system to automatically detect misinformation in news. The main challenge in this line of research is collecting quality data, i.e., instances of fake and real news articles on a balanced distribution of topics. We review available datasets and introduce the MisInfoText repository as a contribution of our lab to the community. We make available the full text of the news articles, together with veracity labels previously assigned based on manual assessment of the articles’ truth content. We also perform a topic modelling experiment to elaborate on the gaps and sources of imbalance in currently available datasets to guide future efforts. We appeal to the community to collect more data and to make it available for research purposes.",47-118,2019.0,http://www.rice.info/exploreauthor.htm,Computer Science
143,82bbf69ede840a8604d153ff23dcd95b8e5ff317,A Survey on Deep Learning in Big Data,"Big Data means extremely huge large data sets that can be analyzed to find patterns, trends. One technique that can be used for data analysis so that able to help us find abstract patterns in Big Data is Deep Learning. If we apply Deep Learning to Big Data, we can find unknown and useful patterns that were impossible so far. With the help of Deep Learning, AI is getting smart. There is a hypothesis in this regard, the more data, the more abstract knowledge. So a handy survey of Big Data, Deep Learning and its application in Big Data is necessary. In this paper, we provide a comprehensive survey on what is Big Data, comparing methods, its research problems, and trends. Then a survey of Deep Learning, its methods, comparison of frameworks, and algorithms is presented. And at last, application of Deep Learning in Big Data, its challenges, open research problems and future trends are presented.",173-180,2017.0,http://jones-fox.net/tag/posts/listpost.asp,Computer Science
144,a4b2fcd91638be1f98b0b49036ddf4b9d4aa196c,Big Data: Principles and best practices of scalable realtime data systems,"Services like social networks, web analytics, and intelligent e-commerce often need to manage data at a scale too big for a traditional database. As scale and demand increase, so does Complexity. Fortunately, scalability and simplicity are not mutually exclusiverather than using some trendy technology, a different approach is needed. Big data systems use many machines working in parallel to store and process data, which introduces fundamental challenges unfamiliar to most developers. Big Data shows how to build these systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy to understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to use them in practice, and how to deploy and operate them once they're built. Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.",42-127,2015.0,http://santiago-rodriguez.biz/appsearch.asp,Computer Science
145,bc0fd9a19026326a07295525599503a01b02f4a4,Debating big data: A literature review on realizing value from big data,,191-209,2017.0,https://www.jones.com/search/tag/appmain.jsp,Computer Science
146,a607e0c6a0d5d1321eca383939dbcb2b638613a2,A smart home energy management system using IoT and big data analytics approach,"Increasing cost and demand of energy has led many organizations to find smart ways for monitoring, controlling and saving energy. A smart Energy Management System (EMS) can contribute towards cutting the costs while still meeting energy demand. The emerging technologies of Internet of Things (IoT) and Big Data can be utilized to better manage energy consumption in residential, commercial, and industrial sectors. This paper presents an Energy Management System (EMS) for smart homes. In this system, each home device is interfaced with a data acquisition module that is an IoT object with a unique IP address resulting in a large mesh wireless network of devices. The data acquisition System on Chip (SoC) module collects energy consumption data from each device of each smart home and transmits the data to a centralized server for further processing and analysis. This information from all residential areas accumulates in the utility’s server as Big Data. The proposed EMS utilizes off-the-shelf Business Intelligence (BI) and Big Data analytics software packages to better manage energy consumption and to meet consumer demand. Since air conditioning contributes to 60% of electricity consumption in Arab Gulf countries, HVAC (Heating, Ventilation and Air Conditioning) Units have been taken as a case study to validate the proposed system. A prototype was built and tested in the lab to mimic small residential area HVAC systems1.",426-434,2017.0,https://www.chapman.biz/categories/blog/tagsfaq.html,Computer Science
147,0a50bc5230228f7bed622beab0ffc70ecc3098bb,Data-driven innovation: switching the perspective on Big Data,"
Purpose
The pervasive spread of digital technologies brought an incredible boost in data availability. Companies are dealing with massive amount of data that wait to be exploited. At the same time, scholars are providing different strategies and methods to help companies capture the value embedded in their data to foster innovation and improve the efficiency of existing processes. In these research studies, data are the by-product of something else, and they are a silent asset that needs to be exploited. What if data might be considered the final goal? The paper aims to discuss these issues.


Design/methodology/approach
The research is based on an exploratory multiple case study analysis, on the basis of three cases used as an illustration for new ideas. In particular, the gathered data are analyzed according to models previously presented in the literature review, building on and expanding them.


Findings
The research proposes a data-driven approach to innovation, offering a peculiar view of the innovation process. The trigger point is the need of data that let begin the entire development process of a complex system. In this perspective, the application that data are a by-product of the entire innovation process and not the primary output is peculiar since the vast majority of the literature consider data as the by-product of the primary product.


Research limitations/implications
Future research is needed to assess the replicability of the model outside the mobile app industry and to measure its performances. Nevertheless, this paper provides insights both for scholars and managers, enlarging the discussion on digital innovation and digital business models.


Practical implications
The results provide a development process to foster innovation relying on the need of data as the trigger point, guiding entrepreneurs and managers in the building process of the entire digital system.


Originality/value
Previous research studies often considered Big Data (BD) in innovation as a way to enlarge the current product offer or to make the innovation process more effective or efficient; this paper changes the perspective by considering BD as the trigger and the enabler of the entire digital innovation process.
",53-148,2019.0,https://richard.org/explorefaq.asp,Economics
148,335848cc7da51a26126c1fa255bd776a6cff8c03,Big Data and Firm Dynamics,"We study a model where firms accumulate data as a valuable intangible asset. Data accumulation affects firms' dynamics. It increases the skewness of the firm size distribution as large firms generate more data and invest more in active experimentation. On the other hand, small data-savvy firms can overtake more traditional incumbents, provided they can finance their initial money-losing growth. Our model can be used to estimate the market and social value of data.",34-115,2019.0,https://www.whitehead.com/category/search/mainindex.asp,Business
149,04163a7e819b37f7ed1c9031ea128132e1465975,VFDB 2016: hierarchical and refined dataset for big data analysis—10 years on,"The virulence factor database (VFDB, http://www.mgc.ac.cn/VFs/) is dedicated to providing up-to-date knowledge of virulence factors (VFs) of various bacterial pathogens. Since its inception the VFDB has served as a comprehensive repository of bacterial VFs for over a decade. The exponential growth in the amount of biological data is challenging to the current database in regard to big data analysis. We recently improved two aspects of the infrastructural dataset of VFDB: (i) removed the redundancy introduced by previous releases and generated two hierarchical datasets – one core dataset of experimentally verified VFs only and another full dataset including all known and predicted VFs and (ii) refined the gene annotation of the core dataset with controlled vocabularies. Our efforts enhanced the data quality of the VFDB and promoted the usability of the database in the big data era for the bioinformatic mining of the explosively growing data regarding bacterial VFs.",D694 - D697,2015.0,https://www.parker.com/tagmain.php,Computer Science
150,b12a264640c86c4b0fb1abb5ea24ad605e84e705,Big Data: A Survey,,171-209,2014.0,https://www.bean.org/tag/categories/blogsearch.htm,Computer Science
151,2d2fe4a73c98933ae9b8df73c8452b0d8be6475e,An Intelligent Fault Diagnosis Method Using Unsupervised Feature Learning Towards Mechanical Big Data,"Intelligent fault diagnosis is a promising tool to deal with mechanical big data due to its ability in rapidly and efficiently processing collected signals and providing accurate diagnosis results. In traditional intelligent diagnosis methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise. Such processes take advantage of human ingenuity but are time-consuming and labor-intensive. Inspired by the idea of unsupervised feature learning that uses artificial intelligence techniques to learn features from raw data, a two-stage learning method is proposed for intelligent diagnosis of machines. In the first learning stage of the method, sparse filtering, an unsupervised two-layer neural network, is used to directly learn features from mechanical vibration signals. In the second stage, softmax regression is employed to classify the health conditions based on the learned features. The proposed method is validated by a motor bearing dataset and a locomotive bearing dataset, respectively. The results show that the proposed method obtains fairly high diagnosis accuracies and is superior to the existing methods for the motor bearing dataset. Because of learning features adaptively, the proposed method reduces the need of human labor and makes intelligent fault diagnosis handle big data more easily.",3137-3147,2016.0,http://garrett.org/app/wp-contentregister.jsp,Engineering
152,0e80bf0b3fe52b5b2719dd3ff8c07fc4bb145fd9,Learning a No-Reference Quality Assessment Model of Enhanced Images With Big Data,"In this paper, we investigate into the problem of image quality assessment (IQA) and enhancement via machine learning. This issue has long attracted a wide range of attention in computational intelligence and image processing communities, since, for many practical applications, e.g., object detection and recognition, raw images are usually needed to be appropriately enhanced to raise the visual quality (e.g., visibility and contrast). In fact, proper enhancement can noticeably improve the quality of input images, even better than originally captured images, which are generally thought to be of the best quality. In this paper, we present two most important contributions. The first contribution is to develop a new no-reference (NR) IQA model. Given an image, our quality measure first extracts 17 features through analysis of contrast, sharpness, brightness and more, and then yields a measure of visual quality using a regression module, which is learned with big-data training samples that are much bigger than the size of relevant image data sets. The results of experiments on nine data sets validate the superiority and efficiency of our blind metric compared with typical state-of-the-art full-reference, reduced-reference and NA IQA methods. The second contribution is that a robust image enhancement framework is established based on quality optimization. For an input image, by the guidance of the proposed NR-IQA measure, we conduct histogram modification to successively rectify image brightness and contrast to a proper level. Thorough tests demonstrate that our framework can well enhance natural images, low-contrast images, low-light images, and dehazed images. The source code will be released at https://sites.google.com/site/guke198701/publications.",1301-1313,2018.0,https://roman.info/wp-content/explore/postssearch.htm,Computer Science
153,179cf488917709be99fbab81158f520cfead41d3,Big data analytics for personalized medicine.,,"
          161-167
        ",2019.0,https://www.brown.com/wp-content/postscategory.php,Medicine
154,ca68d86366c38eef8451917197191093d18865a6,Big data and predictive analytics for supply chain and organizational performance,,308-317,2017.0,http://www.holt.com/blog/exploreindex.html,Business
155,ae27c78d63e56c2b7610bd94fd704fa1b5dcdd29,Big Data technologies: A survey,,431-448,2017.0,https://cook-berger.biz/exploremain.html,Computer Science
156,2c7fc4bb092929408a6f6954c984b1b1d5dd75d6,Understanding the Factors Affecting the Organizational Adoption of Big Data,"ABSTRACT Big data is rapidly becoming a major driver for firms seeking to gain a competitive advantage. Grounded in the Diffusion of Innovation theory (DOI), the institutional theory, and the Tech-nology–Organization–Environment (TOE) framework, this study applies the results of a content analysis to develop a framework to identify the main factors affecting the organizational adoption of big data. The content analysis is based on the retrieval and review of relevant papers in the business intelligence & analytics (BI&A) literature published during the period 2009–2015. The 26 factors identified by this review are then integrated into a TOE framework. The findings of this research enrich the current big data literature and enhance practitioners’ understanding of the decision-making processes involved in a firm’s adoption of big data.",193 - 203,2018.0,http://alexander.com/searchhomepage.php,Computer Science
157,08f767340bedd7d63523dab0816b0fcc612d1ee2,Big data and business analytics ecosystems: paving the way towards digital transformation and sustainable societies,,479 - 491,2018.0,https://sharp-wood.org/tags/explore/categorycategory.html,Computer Science
158,00d536b61baecedb647ddd10b91cc9eeddd11fa4,Big Data: Deep Learning for financial sentiment analysis,,1-25,2018.0,https://www.stone.com/maincategory.htm,Computer Science
159,d345778be736495187b4fd71205fd8125cd32370,Business intelligence and big data in hospitality and tourism: a systematic literature review,"
Purpose
This paper aims to examine the extent to which Business Intelligence and Big Data feature within academic research in hospitality and tourism published until 2016, by identifying research gaps and future developments and designing an agenda for future research.


Design/methodology/approach
The study consists of a systematic quantitative literature review of academic articles indexed on the Scopus and Web of Science databases. The articles were reviewed based on the following features: research topic; conceptual and theoretical characterization; sources of data; type of data and size; data collection methods; data analysis techniques; and data reporting and visualization.


Findings
Findings indicate an increase in hospitality and tourism management literature applying analytical techniques to large quantities of data. However, this research field is fairly fragmented in scope and limited in methodologies and displays several gaps. A conceptual framework that helps to identify critical business problems and links the domains of business intelligence and big data to tourism and hospitality management and development is missing. Moreover, epistemological dilemmas and consequences for theory development of big data-driven knowledge are still a terra incognita. Last, despite calls for more integration of management and data science, cross-disciplinary collaborations with computer and data scientists are rather episodic and related to specific types of work and research.


Research limitations/implications
This work is based on academic articles published before 2017; hence, scientific outputs published after the moment of writing have not been included. A rich research agenda is designed.


Originality/value
This study contributes to explore in depth and systematically to what extent hospitality and tourism scholars are aware of and working intendedly on business intelligence and big data. To the best of the authors’ knowledge, it is the first systematic literature review within hospitality and tourism research dealing with business intelligence and big data.
",84-147,2018.0,http://www.wagner-davidson.org/categoryfaq.htm,Economics
160,3b718eebc6f4469053dadb5737b67ee18b16722d,Beyond prediction: Using big data for policy problems,"Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making.",483 - 485,2017.0,https://park.org/categoryhome.htm,Computer Science
161,0b842f5d17165280687784ccf6b844242f7bfb7c,A review on the practice of big data analysis in agriculture,,23-37,2017.0,http://www.zamora.org/blog/blog/blogsearch.jsp,Engineering
162,e13af509fabacf0e3d27202c9ef6995ef71d0129,Database Resources of the BIG Data Center in 2019,,D8-D14,2019.0,https://www.parker-mcdonald.org/tagsearch.html,Computer Science
163,c7f686b1047d6519ea1f90c8e3bfd6e7e2d7aadf,Toward the development of a big data analytics capability,,1049-1064,2016.0,https://perez.biz/tags/main/tagsindex.htm,Engineering
164,44454bc0c23fb3d515540eb64c8636de727abe85,"Context-Aware Computing, Learning, and Big Data in Internet of Things: A Survey","Internet of Things (IoT) has been growing rapidly due to recent advancements in communications and sensor technologies. Meanwhile, with this revolutionary transformation, researchers, implementers, deployers, and users are faced with many challenges. IoT is a complicated, crowded, and complex field; there are various types of devices, protocols, communication channels, architectures, middleware, and more. Standardization efforts are plenty, and this chaos will continue for quite some time. What is clear, on the other hand, is that IoT deployments are increasing with accelerating speed, and this trend will not stop in the near future. As the field grows in numbers and heterogeneity, “intelligence” becomes a focal point in IoT. Since data now becomes “big data,” understanding, learning, and reasoning with big data is paramount for the future success of IoT. One of the major problems in the path to intelligent IoT is understanding “context,” or making sense of the environment, situation, or status using data from sensors, and then acting accordingly in autonomous ways. This is called “context-aware computing,” and it now requires both sensing and, increasingly, learning, as IoT systems get more data and better learning from this big data. In this survey, we review the field, first, from a historical perspective, covering ubiquitous and pervasive computing, ambient intelligence, and wireless sensor networks, and then, move to context-aware computing studies. Finally, we review learning and big data studies related to IoT. We also identify the open issues and provide an insight for future study areas for IoT researchers.",1-27,2018.0,https://norman.info/search/main/postsabout.html,Computer Science
165,6f0af225c3aa849c2083d7257b4ff9ed4b4eb042,Big Data consumer analytics and the transformation of marketing,,897-904,2016.0,http://www.cantu-hunt.info/explore/posts/categoryauthor.html,Computer Science
166,079b2cfe950a96d5a43a3febc983d151fb533b53,Machine learning on big data: Opportunities and challenges,,350-361,2017.0,http://www.walker-king.biz/posts/wp-contentlogin.htm,Computer Science
167,ac469d36498a54ecaa129c6ef4dc09745bfa3b03,The role of big data analytics in Internet of Things,,459-471,2017.0,http://www.house-thomas.biz/postsauthor.asp,Computer Science
168,d2bed4fe58f38dfdda55ac530521ed442bfc6ae7,Factors influencing big data decision-making quality,,338-345,2017.0,https://www.miranda.net/categoryindex.asp,Business
169,5766f245105247b4ce3f580ddf2c8213c80f7969,The role of Big Data in explaining disaster resilience in supply chains for sustainability,,1108-1118,2017.0,https://walton.com/search/tags/appabout.php,Sociology
170,f673051978825fc796b2b8812808ca6cd8de83f2,"A Survey on Big Data Market: Pricing, Trading and Protection","Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.",15132-15154,2018.0,http://watts.net/blog/wp-content/appcategory.html,Computer Science
171,8b58f608261132a950a5e83ec00aa3b3836ccab7,"Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election","Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualiﬁcations for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1% survey with 60% response rate or a self-reported administrative dataset covering 80% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size n , probabilistic or not, the difference between the sample average X n and the population average X N is the product of three terms: (1) a data quality measure, ρ R,X , the correlation between X j and the response/recording indicator R j ; (2) a data quantity measure, √ (N − n)/n , where N is the population size; and (3) a problem difﬁculty measure, σ X , the standard deviation of X . This decompo-sition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρ R,X at the level of N − 1 / 2 ; (II) When we lose this control, the impact of N is no longer canceled by ρ R,X , leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1 / √ n , increases with √ N ; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f = n/N , not the absolute size n ; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρ R,X ≈ − 0 . 005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly mi-nuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1% of the US eligible voters, that is, n ≈ 2,300,000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n ≈ 400, a 99 . 98% reduction of sample size (and hence our conﬁdence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95% conﬁdence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox : the more the data, the surer we fool ourselves.",31-129,2018.0,https://bernard.biz/searchsearch.html,Economics
172,ace8d8f019cdd1c9f6a775263d0552bb5c51f8a2,Situating methods in the magic of Big Data and AI,"ABSTRACT “Big Data” and “artificial intelligence” have captured the public imagination and are profoundly shaping social, economic, and political spheres. Through an interrogation of the histories, perceptions, and practices that shape these technologies, we problematize the myths that animate the supposed “magic” of these systems. In the face of an increasingly widespread blind faith in data-driven technologies, we argue for grounding machine learning-based practices and untethering them from hype and fear cycles. One path forward is to develop a rich methodological framework for addressing the strengths and weaknesses of doing data analysis. Through provocatively reimagining machine learning as computational ethnography, we invite practitioners to prioritize methodological reflection and recognize that all knowledge work is situated practice.",57 - 80,2018.0,http://www.barnes.biz/main/tagsprivacy.asp,Sociology
173,2e8e061b3548dc181d783bacacbb6b0bd54851a5,Big data analytics in smart grids: a review,,28-126,2018.0,https://newton.info/categories/categoryhome.html,Computer Science
174,6a028cc26d7eceada86e873a37e57a3e113e24af,"Big Data in Public Health: Terminology, Machine Learning, and Privacy.","The digital world is generating data at a staggering and still increasing rate. While these ""big data"" have unlocked novel opportunities to understand public health, they hold still greater potential for research and practice. This review explores several key issues that have arisen around big data. First, we propose a taxonomy of sources of big data to clarify terminology and identify threads common across some subtypes of big data. Next, we consider common public health research and practice uses for big data, including surveillance, hypothesis-generating research, and causal inference, while exploring the role that machine learning may play in each use. We then consider the ethical implications of the big data revolution with particular emphasis on maintaining appropriate care for privacy in a world in which technology is rapidly changing social norms regarding the need for (and even the meaning of) privacy. Finally, we make suggestions regarding structuring teams and training to succeed in working with big data in research and practice.","
          95-112
        ",2018.0,https://byrd.com/wp-content/categories/categoryprivacy.php,Medicine
175,5ebf42bc9c25a1baf09f18bfade506644a930fea,Big Data Driven Vehicular Networks,"VANETs enable information exchange among vehicles, other end devices and public networks, which plays a key role in road safety/infotainment, intelligent transportation systems, and self-driving systems. As vehicular connectivity soars, and new on-road mobile applications and technologies emerge, VANETs are generating an ever-increasing amount of data, requiring fast and reliable transmissions through VANETs. On the other hand, a variety of VANETs related data can be analyzed and utilized to improve the performance of VANETs. In this article, we first review VANETs technologies to efficiently and reliably transmit big data. Then, the methods employing big data for studying VANETs characteristics and improving VANETs performance are discussed. Furthermore, we present a case study where machine learning schemes are applied to analyze VANETs measurement data for efficiently",160-167,2018.0,http://www.hill.com/list/explorecategory.htm,Engineering
176,5a765bef6d657595066d66c173ff228bd01a3627,Big Data in Natural Disaster Management: A Review,"Undoubtedly, the age of big data has opened new options for natural disaster management, primarily because of the varied possibilities it provides in visualizing, analyzing, and predicting natural disasters. From this perspective, big data has radically changed the ways through which human societies adopt natural disaster management strategies to reduce human suffering and economic losses. In a world that is now heavily dependent on information technology, the prime objective of computer experts and policy makers is to make the best of big data by sourcing information from varied formats and storing it in ways that it can be effectively used during different stages of natural disaster management. This paper aimed at making a systematic review of the literature in analyzing the role of big data in natural disaster management and highlighting the present status of the technology in providing meaningful and effective solutions in natural disaster management. The paper has presented the findings of several researchers on varied scientific and technological perspectives that have a bearing on the efficacy of big data in facilitating natural disaster management. In this context, this paper reviews the major big data sources, the associated achievements in different disaster management phases, and emerging technological topics associated with leveraging this new ecosystem of Big Data to monitor and detect natural hazards, mitigate their effects, assist in relief efforts, and contribute to the recovery and reconstruction processes.",165,2018.0,http://robbins-wheeler.com/list/explore/wp-contentregister.html,Business
177,efa5558bddd68abe4adc81adbbef6f739e648392,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",35-119,2015.0,https://www.meyer.org/main/categoriessearch.jsp,Medicine
178,fff51615943e08d05080682009c9c656321ef0b2,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",676-682,2018.0,https://www.robinson.com/category/list/appregister.html,Materials Science
179,b58a48f0aba6ccf4ea4ee5c5155ddad75a9617f9,Opportunities and Challenges for Big Data in Agricultural and Environmental Analysis,"Agriculture stands on the cusp of a digital revolution, and the same technologies that created the Internet and are transforming medicine are now being applied in our farms and on our fields. Overall, this digital agricultural revolution is being driven by the low cost of collecting data on everything from soil conditions to animal health and crop development along with weather station data and data collected by drones and satellites. The promise of these technologies is more food, produced on less land, with fewer inputs and a smaller environmental footprint. At present, however, barriers to realizing this potential include a lack of ability to aggregate and interpret data in such a way that it results in useful decision support tools for farmers and the need to train farmers in how to use new tools. This article reviews the state of the literature on the promise and barriers to realizing the potential for Big Data to revolutionize agriculture.",100-111,2018.0,https://www.king.com/wp-content/taghomepage.html,Business
180,fb5d6279f1967fc73e83b7466679b2fbb9ec71c1,"Fog Based Intelligent Transportation Big Data Analytics in The Internet of Vehicles Environment: Motivations, Architecture, Challenges, and Critical Issues","The intelligent transportation system (ITS) concept was introduced to increase road safety, manage traffic efficiently, and preserve our green environment. Nowadays, ITS applications are becoming more data-intensive and their data are described using the “5Vs of Big Data”. Thus, to fully utilize such data, big data analytics need to be applied. The Internet of vehicles (IoV) connects the ITS devices to cloud computing centres, where data processing is performed. However, transferring huge amount of data from geographically distributed devices creates network overhead and bottlenecks, and it consumes the network resources. In addition, following the centralized approach to process the ITS big data results in high latency which cannot be tolerated by the delay-sensitive ITS applications. Fog computing is considered a promising technology for real-time big data analytics. Basically, the fog technology complements the role of cloud computing and distributes the data processing at the edge of the network, which provides faster responses to ITS application queries and saves the network resources. However, implementing fog computing and the lambda architecture for real-time big data processing is challenging in the IoV dynamic environment. In this regard, a novel architecture for real-time ITS big data analytics in the IoV environment is proposed in this paper. The proposed architecture merges three dimensions including intelligent computing (i.e. cloud and fog computing) dimension, real-time big data analytics dimension, and IoV dimension. Moreover, this paper gives a comprehensive description of the IoV environment, the ITS big data characteristics, the lambda architecture for real-time big data analytics, several intelligent computing technologies. More importantly, this paper discusses the opportunities and challenges that face the implementation of fog computing and real-time big data analytics in the IoV environment. Finally, the critical issues and future research directions section discusses some issues that should be considered in order to efficiently implement the proposed architecture.",15679-15701,2018.0,https://novak-gould.com/app/posts/categoriesfaq.htm,Computer Science
181,a7797aed6d23d7b599e71ad129211c2834925c0d,EMG Pattern Recognition in the Era of Big Data and Deep Learning,"The increasing amount of data in electromyographic (EMG) signal research has greatly increased the importance of developing advanced data analysis and machine learning techniques which are better able to handle “big data”. Consequently, more advanced applications of EMG pattern recognition have been developed. This paper begins with a brief introduction to the main factors that expand EMG data resources into the era of big data, followed by the recent progress of existing shared EMG data sets. Next, we provide a review of recent research and development in EMG pattern recognition methods that can be applied to big data analytics. These modern EMG signal analysis methods can be divided into two main categories: (1) methods based on feature engineering involving a promising big data exploration tool called topological data analysis; and (2) methods based on feature learning with a special emphasis on “deep learning”. Finally, directions for future research in EMG pattern recognition are outlined and discussed.",21,2018.0,https://ayers.com/exploreterms.html,Computer Science
182,b016670e4e0986a85cc70a625e8627a80217a0a7,DATA MINING FOR BIG DATA,"Big data"" is pervasive, and yet still the notion engenders confusion. Big data has been used to convey all sorts of concepts, including: huge quantities of data, social media analytics, next generation data management capabilities, real-time data, and much more. Whatever the label, organizations are starting to understand and explore how to process and analyze a vast array of information in new ways. Big data is the term for a collection of data sets which are large and complex, it contain structured and unstructured both type of data. Data comes from everywhere, sensors used to gather climate information, posts to social media sites, digital pictures and videos etc This data is known as big data. Useful data can be extracted from this big data with the help of data mining. Data mining is a technique for discovering interesting patterns as well as descriptive, understandable models from large scale data. In this paper we overviewed types of big data and challenges in big data for future.",517-520,2015.0,http://www.webster.com/tagsfaq.html,Computer Science
183,111e7daa3c579208ba80c23ac33ab7e7b5b1f099,Big Data Analytics in Medicine and Healthcare,"Abstract This paper surveys big data with highlighting the big data analytics in medicine and healthcare. Big data characteristics: value, volume, velocity, variety, veracity and variability are described. Big data analytics in medicine and healthcare covers integration and analysis of large amount of complex heterogeneous data such as various – omics data (genomics, epigenomics, transcriptomics, proteomics, metabolomics, interactomics, pharmacogenomics, diseasomics), biomedical data and electronic health records data. We underline the challenging issues about big data privacy and security. Regarding big data characteristics, some directions of using suitable and promising open-source distributed data processing software platform are given.",33-144,2018.0,https://james-lee.com/category/search/categoryprivacy.htm,Computer Science
184,e9e5549b839e16384300229e2b93c46a9635040f,"Big Data for Open Innovation in SMEs and Large Corporations: Trends, Opportunities, and Challenges","The notion of ‘Big Data’ has recently been attracting an increasing degree of attention from scholars and practitioners in an attempt to identify how it may be leveraged to create innovative solutions and business opportunities. Specifically, Big Data may come from a variety of sources, especially sources outside the usual boundaries of organizations, and it represents an interesting and emerging opportunity for sustaining and enhancing the effectiveness of the so-called open innovation paradigm. However, to the best of our knowledge, no prior works have provided a broad overview of the use of Big Data for open innovation strategies. We aim to fill this gap. In particular, we have focused our investigation on two types of companies: small and medium-sized enterprises (SMEs) and big corporations, reviewing the major academic works published so far and analysing the main industrial applications on this topic. As a result, we provide a relevant list of the main trends, opportunities, and challenges faced by SMEs and large corporations when dealing with Big Data for open innovation strategies.",66-134,2018.0,https://rivera-sellers.com/blog/listcategory.htm,Business
185,5b2812a083ae67de9da889b32ddd6aa464df845c,Big Data Emerging Technology: Insights into Innovative Environment for Online Learning Resources,"Digital devices like tablets, smart phones, and laptop have become increasingly raised and utilised in higher education. As a result, current trends on ICT (information and communication technology) used in education begin widely with focusing on teaching and learning. The new concept of big data in recent ICT domain extends the promising research direction on online learning and big data integration through promising content that can be tailored for each student based on the context and Internet behaviour of users in online learning. This paper aims to explore innovative design for innovative online learning in Higher education using Big Data approach. Critical review from referred journals and books was conducted using thematic analysis. This paper proposes model reference which can be implemented with the technology in teaching and learning to improve student learning environment and outcomes and to enhance students’ development, performance and achievement in learning process in higher education.",23-36,2018.0,https://www.shelton.biz/blog/wp-contenthome.html,Computer Science
186,a51034d6cda97671b987990c43bfe5b263b2205d,Big Data in Agriculture: A Challenge for the Future,"This article examines the challenge and opportunities of Big Data, and concludes that these technologies will lead to relevant analysis at every stage of the agricultural value chain. Big Data is defined by several characteristics beyond size, particularly, the volume, velocity, variety, and veracity of the data. We discuss a set of analytical techniques that are increasingly relevant to our profession as one addresses these issues. Ultimately, we resolve that agricultural and applied economists are uniquely positioned to contribute to the research and outreach agenda on Big Data. We believe there are relevant policy, farm management, supply chain, consumer demand, and sustainability issues where our profession can make major contributions. The authors are thankful to the anonymous reviewers and editor Craig Gundersen for helpful comments. Support was provided by the Mississippi Agricultural and Forestry Experiment Station Special Research Initiative.",79–96,2018.0,https://perez.com/exploreindex.html,Business
187,92b8a8124872dfa576fbe9ea44d2a8ab723fe477,"The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences","Despite their importance, little conceptual attention has traditionally been paid to data, at least in comparison with the sophistication and depth of the debate on the nature of knowledge and of information. In contrast, efforts have largely been directed towards the development of sound methods for sampling and analysing in order to ensure the validity of the conclusions made on the back of data. However, these are not—despite their name—a neutral given, but a wholly social construct; therefore, their unpacking is necessary in order to understand how data come into existence, how they do work in the world, and with what consequences. The book does this by introducing the notion of the data assemblage, and using it to examine three key components of the data revolution: data infrastructures, open data and Big Data.",272,2015.0,https://www.smith-porter.org/postslogin.asp,Computer Science
188,9b0dd87208a03e78105491e3727213b9b8ac0419,Big Data: New Tricks for Econometrics,"Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.",3-28,2014.0,https://potter.info/search/categoryauthor.html,Computer Science
189,433a6460a07ad6c2e24e8ef9c8d197b07303416d,Medical big data: promise and challenges,"The concept of big data, commonly characterized by volume, variety, velocity, and veracity, goes far beyond the data type and includes the aspects of data analysis, such as hypothesis-generating, rather than hypothesis-testing. Big data focuses on temporal stability of the association, rather than on causal relationship and underlying probability distribution assumptions are frequently not required. Medical big data as material to be analyzed has various features that are not only distinct from big data of other disciplines, but also distinct from traditional clinical epidemiology. Big data technology has many areas of application in healthcare, such as predictive modeling and clinical decision support, disease or safety surveillance, public health, and research. Big data analytics frequently exploits analytic methods developed in data mining, including classification, clustering, and regression. Medical big data analyses are complicated by many technical issues, such as missing values, curse of dimensionality, and bias control, and share the inherent limitations of observation study, namely the inability to test causality resulting from residual confounding and reverse causation. Recently, propensity score analysis and instrumental variable analysis have been introduced to overcome these limitations, and they have accomplished a great deal. Many challenges, such as the absence of evidence of practical benefits of big data, methodological issues including legal and ethical issues, and clinical integration and utility issues, must be overcome to realize the promise of medical big data as the fuel of a continuous learning healthcare system that will improve patient outcome and reduce waste in areas including nephrology.",3 - 11,2017.0,https://www.davidson.com/listlogin.html,Medicine
190,404c8bd8d1497a341d25889d714ce1d41507c2b0,Banking with blockchain-ed big data,"Blockchain is disrupting the banking industry and contributing to the increased big data in banking. However, there exists a gap in research and development into blockchain-ed big data in banking f...",83-132,2018.0,https://www.jackson.com/appfaq.asp,Business
191,6bf9d589f80823735084956f056728ae1a7bcfa8,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",27-106,2018.0,https://jackson.com/tag/mainindex.php,Computer Science
192,8a7ccb4ffb953fafd44b01018eb53771d93d8c70,Deep Convolutional Computation Model for Feature Learning on Big Data in Internet of Things,"Currently, a large number of industrial data, usually referred to big data, are collected from Internet of Things (IoT). Big data are typically heterogeneous, i.e., each object in big datasets is multimodal, posing a challenging issue on the convolutional neural network (CNN) that is one of the most representative deep learning models. In this paper, a deep convolutional computation model (DCCM) is proposed to learn hierarchical features of big data by using the tensor representation model to extend the CNN from the vector space to the tensor space. To make full use of the local features and topologies contained in the big data, a tensor convolution operation is defined to prevent overfitting and improve the training efficiency. Furthermore, a high-order backpropagation algorithm is proposed to train the parameters of the deep convolutional computational model in the high-order space. Finally, experiments on three datasets, i.e., CUAVE, SNAE2, and STL-10 are carried out to verify the performance of the DCCM. Experimental results show that the deep convolutional computation model can give higher classification accuracy than the deep computation model or the multimodal model for big data in IoT.",790-798,2018.0,http://garcia.org/listhomepage.jsp,Computer Science
193,cff7f7f5aa2393a343444f469b2419c8770fd65d,Artificial Intelligence and Big Data in Public Health,"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.",76-105,2018.0,http://www.johnson.net/blog/wp-content/mainterms.html,Medicine
194,c113b3f17d8f08bdaaa18ae88c24afd4d41cb543,Big Data Research in Information Systems: Toward an Inclusive Research Agenda,": Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data’s characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data’s disruptive effects.",3,2016.0,https://williams-spencer.com/categories/explore/categorycategory.htm,Computer Science
195,7403e177957cbb51f17018210da02d2ceab88f8a,Diversity in Big Data: A Review,"Big data technology offers unprecedented opportunities to society as a whole and also to its individual members. At the same time, this technology poses significant risks to those it overlooks. In this article, we give an overview of recent technical work on diversity, particularly in selection tasks, discuss connections between diversity and fairness, and identify promising directions for future work that will position diversity as an important component of a data-responsible society. We argue that diversity should come to the forefront of our discourse, for reasons that are both ethical-to mitigate the risks of exclusion-and utilitarian, to enable more powerful, accurate, and engaging data analysis and use.","
          73-84
        ",2017.0,http://www.ray.net/wp-content/wp-content/searchregister.php,Computer Science
196,812ac6e60df272dab8ff7e835c8813cc0f46697c,Detecting Anomaly in Big Data System Logs Using Convolutional Neural Network,"Nowadays, big data systems are being widely adopted by many domains for offering effective data solutions, such as manufacturing, healthcare, education, and media. Big data systems produce tons of unstructured logs that contain buried valuable information. However, it is a daunting task to manually unearth the information and detect system anomalies. A few automatic methods have been developed, where the cutting-edge machine learning technique is one of the most promising ways. In this paper, we propose a novel approach for anomaly detection from big data system logs by leveraging Convolutional Neural Networks (CNN). Different from other existing statistical methods or traditional rule-based machine learning approaches, our CNN-based model can automatically learn event relationships in system logs and detect anomaly with high accuracy. Our deep neural network consists of logkey2vec embeddings, three 1D convolutional layers, dropout layer, and max-pooling. According to our experiment, our CNN-based approach has better accuracy(reaches to 99%) compared to other approaches using Long Short term memory (LSTM) and Multilayer Perceptron (MLP) on detecting anomaly in Hadoop Distributed File System (HDFS) logs.",151-158,2018.0,http://gordon-moore.biz/list/blogsearch.html,Computer Science
197,7df5a0412ea3888df35c779f72cdc53dd0c3efb3,Big Data and Service Operations,"This study discusses how the tremendous volume of available data collected by firms has been transforming the service industry. The focus is primarily on services in the following sectors: finance/banking, transportation and hospitality, and online platforms (e.g., subscription services, online advertising, and online dating). We report anecdotal evidence borrowed from various collaborations and discussions with executives and data analysts who work in management consulting or finance, or for technology/startup companies. Our main goals are (i) to present an overview of how big data is shaping the service industry, (ii) to describe several mechanisms used in the service industry that leverage the potential information hidden in big data, and (iii) to point out some of the pitfalls and risks incurred. On one hand, collecting and storing large amounts of data on customers and on past transactions can help firms improve the quality of their services. For example, firms can now customize their services to unprecedented levels of granularity, which enables the firms to offer targeted personalized offers (sometimes, even in real‐time). On the other hand, collecting this data may allow some firms to utilize the data against their customers by charging them higher prices. Furthermore, data‐driven algorithms may often be biased toward illicit discrimination. The availability of data on sensitive personal information may also attract hackers and gives rise to important cybersecurity concerns (e.g., information leakage, fraud, and identity theft).",1709 - 1723,2018.0,https://reed.org/tagsmain.php,Business
198,021332da6de9f907f77a39a0a6aefb110140d2cd,"Synergy of Big Data and 5G Wireless Networks: Opportunities, Approaches, and Challenges","This article presents the synergistic and complementary features of big data and 5G wireless networks. An overview of their interplay is provided first, including big-data-driven networking and big data assisted networking. The former exploits heterogeneous resources such as communication, caching, and computing in 5G wireless networks to support big data applications and services, by catering for big data's features such as volume, velocity, and variety. The latter leverages big data techniques to collect wireless big data and extract in-depth knowledge regarding the networks and users to improve network planning and operation. To further illustrate the mutual benefits, two case studies on network aided data acquisition and big data assisted edge content caching are provided. Finally, some interesting open research issues are discussed.",12-18,2018.0,https://www.andrews.com/explore/categoriesregister.php,Computer Science
199,14911a345e4b223d1bd756e317a7410983c28c68,Big Data and Predictive Analytics: Recalibrating Expectations,"With the routine use of electronic health records (EHRs) in hospitals, health systems, and physician practices, there has been rapid growth in the availability of health care data over the last decade. In addition to the structured data in EHRs, new methods such as natural language processing can derive meaning from unstructured data, permitting the capture of substantial clinical information embedded in clinical notes. Furthermore, the growth in the availability of registries and claims data and the linkages between all these data sources have created a big data platform in health care, vast in both size and scope. Concurrently, new computational machine learning approaches promise ever-more-accurate prediction. The marvel of Google and of Watson, the inexorability of Moore’s law (ie, computing power doubles every 2 years for the same cost), suggest a future in which medicine will be transformed into an information science, and each clinical decision may be optimized based on a forecasting of outcomes under alternative treatment options, beyond the knowledge and understanding of the individual physician. Yet despite these innovations and those to come, quantitative risk prediction in medicine has been available for several decades, based on more classical",27–28,2018.0,https://brooks-james.com/explore/wp-contentlogin.jsp,Medicine
200,6d97dca6deb258c30381956e9af93b12bd5c6d03,Big Data Analytics for Physical Internet-based intelligent manufacturing shop floors,"Physical Internet (PI, π) has been widely used for transforming and upgrading the logistics and supply chain management worldwide. This study extends the PI concept into manufacturing shop floors where typical logistics resources are converted into smart manufacturing objects (SMOs) using Internet of Things (IoT) and wireless technologies to create a RFID-enabled intelligent shop floor environment. In such PI-based environment, enormous RFID data could be captured and collected. This study introduces a Big Data Analytics for RFID logistics data by defining different behaviours of SMOs. Several findings are significant. It is observed that task weight is primarily considered in the logistics decision-making in this case. Additionally, the highest residence time occurs in a buffer with the value of 12.17 (unit of time) which is 40.57% of the total delivery time. That implies the high work-in-progress inventory level in this buffer. Key findings and observations are generated into managerial implications, which are useful for various users to make logistics decisions under PI-enabled intelligent shop floors.",2610 - 2621,2017.0,https://www.jones.com/wp-content/categoriespost.html,Engineering
201,fdd1ccdea15d3000bcf66a45d64d7a243006b021,A formal definition of Big Data based on its essential features,"Purpose – The purpose of this paper is to identify and describe the most prominent research areas connected with “Big Data” and propose a thorough definition of the term. Design/methodology/approach – The authors have analysed a conspicuous corpus of industry and academia articles linked with Big Data to find commonalities among the topics they treated. The authors have also compiled a survey of existing definitions with a view of generating a more solid one that encompasses most of the work happening in the field. Findings – The main themes of Big Data are: information, technology, methods and impact. The authors propose a new definition for the term that reads as follows: “Big Data is the Information asset characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value.” Practical implications – The formal definition that is proposed can enable a more coherent development of the concept of Big Data, as it solely relies on ...",122-135,2016.0,https://adams-gray.com/categories/wp-content/tagsmain.jsp,Computer Science
202,8566f36d7917d4bd5705e1f7dff39197e2d22817,A Manufacturing Big Data Solution for Active Preventive Maintenance,"Industry 4.0 has become more popular due to recent developments in cyber-physical systems, big data, cloud computing, and industrial wireless networks. Intelligent manufacturing has produced a revolutionary change, and evolving applications, such as product lifecycle management, are becoming a reality. In this paper, we propose and implement a manufacturing big data solution for active preventive maintenance in manufacturing environments. First, we provide the system architecture that is used for active preventive maintenance. Then, we analyze the method used for collection of manufacturing big data according to the data characteristics. Subsequently, we perform data processing in the cloud, including the cloud layer architecture, the real-time active maintenance mechanism, and the offline prediction and analysis method. Finally, we analyze a prototype platform and implement experiments to compare the traditionally used method with the proposed active preventive maintenance method. The manufacturing big data method used for active preventive maintenance has the potential to accelerate implementation of Industry 4.0.",2039-2047,2017.0,http://www.clark.com/main/maincategory.html,Computer Science
203,753dec0e435c76a3f60e5a3db50e78670e63d33e,A Parallel Random Forest Algorithm for Big Data in a Spark Cloud Computing Environment,"With the emergence of the big data age, the issue of how to obtain valuable knowledge from a dataset efficiently and accurately has attracted increasingly attention from both academia and industry. This paper presents a Parallel Random Forest (PRF) algorithm for big data on the Apache Spark platform. The PRF algorithm is optimized based on a hybrid approach combining data-parallel and task-parallel optimization. From the perspective of data-parallel optimization, a vertical data-partitioning method is performed to reduce the data communication cost effectively, and a data-multiplexing method is performed is performed to allow the training dataset to be reused and diminish the volume of data. From the perspective of task-parallel optimization, a dual parallel approach is carried out in the training process of RF, and a task Directed Acyclic Graph (DAG) is created according to the parallel training process of PRF and the dependence of the Resilient Distributed Datasets (RDD) objects. Then, different task schedulers are invoked for the tasks in the DAG. Moreover, to improve the algorithm's accuracy for large, high-dimensional, and noisy data, we perform a dimension-reduction approach in the training process and a weighted voting approach in the prediction process prior to parallelization. Extensive experimental results indicate the superiority and notable advantages of the PRF algorithm over the relevant algorithms implemented by Spark MLlib and other studies in terms of the classification accuracy, performance, and scalability. With the expansion of the scale of the random forest model and the Spark cluster, the advantage of the PRF algorithm is more obvious.",919-933,2017.0,http://leblanc.com/category/tagabout.htm,Computer Science
204,a7e7c77b07d88dfc4937b189a308a236ac120d0e,The role of big data in smart city,,748-758,2016.0,https://galvan.com/main/wp-contentcategory.htm,Computer Science
205,9cacbc50b5a94e28b9de139c6c3abd1f00bd632f,Medical Internet of Things and Big Data in Healthcare,"Objectives A number of technologies can reduce overall costs for the prevention or management of chronic illnesses. These include devices that constantly monitor health indicators, devices that auto-administer therapies, or devices that track real-time health data when a patient self-administers a therapy. Because they have increased access to high-speed Internet and smartphones, many patients have started to use mobile applications (apps) to manage various health needs. These devices and mobile apps are now increasingly used and integrated with telemedicine and telehealth via the medical Internet of Things (mIoT). This paper reviews mIoT and big data in healthcare fields. Methods mIoT is a critical piece of the digital transformation of healthcare, as it allows new business models to emerge and enables changes in work processes, productivity improvements, cost containment and enhanced customer experiences. Results Wearables and mobile apps today support fitness, health education, symptom tracking, and collaborative disease management and care coordination. All those platform analytics can raise the relevancy of data interpretations, reducing the amount of time that end users spend piecing together data outputs. Insights gained from big data analysis will drive the digital disruption of the healthcare world, business processes and real-time decision-making. Conclusions A new category of ""personalised preventative health coaches"" (Digital Health Advisors) will emerge. These workers will possess the skills and the ability to interpret and understand health and well-being data. They will help their clients avoid chronic and diet-related illness, improve cognitive function, achieve improved mental health and achieve improved lifestyles overall. As the global population ages, such roles will become increasingly important.",156 - 163,2016.0,http://miller-young.biz/mainabout.html,Medicine
206,dcb401a9ff0ff14300e181c6fcd35479b719725d,Internet of Things and Big Data Analytics for Smart and Connected Communities,"This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.",766-773,2016.0,http://ortega.net/postsfaq.asp,Computer Science
207,c38bab2d1fcb823300b74b11c513e6a8aa7017de,Concurrence of big data analytics and healthcare: A systematic review,,"
          57-65
        ",2018.0,http://murillo-cannon.biz/categorysearch.html,Computer Science
208,54e8764d47bc08bbb29cfe89547374fec21093eb,Big Data’s Role in Precision Public Health,"Precision public health is an emerging practice to more granularly predict and understand public health risks and customize treatments for more specific and homogeneous subpopulations, often using new data, technologies, and methods. Big data is one element that has consistently helped to achieve these goals, through its ability to deliver to practitioners a volume and variety of structured or unstructured data not previously possible. Big data has enabled more widespread and specific research and trials of stratifying and segmenting populations at risk for a variety of health problems. Examples of success using big data are surveyed in surveillance and signal detection, predicting future risk, targeted interventions, and understanding disease. Using novel big data or big data approaches has risks that remain to be resolved. The continued growth in volume and variety of available data, decreased costs of data capture, and emerging computational methods mean big data success will likely be a required pillar of precision public health into the future. This review article aims to identify the precision public health use cases where big data has added value, identify classes of value that big data may bring, and outline the risks inherent in using big data in precision public health efforts.",63-150,2018.0,http://gutierrez-rodriguez.com/wp-contentpost.htm,Computer Science
209,3eb25255a46bb99c8fb2dd10fa547f1bc19810bb,Big data analytics in E-commerce: a systematic review and agenda for future research,,173-194,2016.0,https://taylor-campbell.net/tags/explore/appmain.php,Engineering
210,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,A survey of machine learning for big data processing,,30-103,2016.0,http://harrison-shaffer.com/app/category/categoriespost.html,Computer Science
211,b997492c79f7980999b0f734cca76ef2761b4274,"Modelling quality dynamics, business value and firm performance in a big data analytics environment","Big data analytics have become an increasingly important component for firms across advanced economies. This paper examines the quality dynamics in big data environment that are linked with enhancing business value and firm performance (FPER). The study identifies that system quality (i.e. system reliability, accessibility, adaptability, integration, response time and privacy) and information quality (i.e. completeness, accuracy, format and currency) are key to enhance business value and FPER in a big data environment. The study also proposes that the relationship between quality and FPER is mediated by business value of big data. Drawing on the resource-based theory and the information systems success literature, this study extends knowledge in this domain by linking system quality, information quality, business value and FPER.",5011 - 5026,2017.0,https://www.hardy-lin.com/blog/appauthor.php,Computer Science
212,fdf5033947321dd1a89975eb67ba1ed6ed0eefc8,A survey towards an integration of big data analytics to big insights for value-creation,,758-790,2018.0,http://heath.com/tag/posts/tagregister.php,Computer Science
213,37d9283061bb8057adff53ff4033dd11ccdf2a0c,Blockchain solutions for big data challenges: A literature review,"The popularity of Blockchain technology and the huge extent of its application, results with much ongoing research in different practical and scientific areas. Although still new and in experimenting phase, the Blockchain is being seen as a revolutionary solution, addressing modern technology concerns like decentralization, trust, identity, data ownership and data-driven decisions. At the same time, the world is facing an expansion in quantity and diversity of digital data that are generated by both users and machines. While actively searching for the best way to store, organize and process Big Data, the Blockchain technology comes in providing significant input. Its proposed solutions about decentralized management of private data, digital property resolution, IoT communication and public institutions' reforms are having significant impact on how Big Data may evolve. This paper presents the novel solutions associated with some of the Big Data areas that can be empowered by the Blockchain technology.",763-768,2017.0,http://flores.com/wp-content/list/searchpost.htm,Computer Science
214,6a23f1aa4fa8201b603e6fa6eb6bdccaac7df0af,Database Resources of the BIG Data Center in 2019,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of multi-omics data generated at unprecedented scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big data integration and value-added curation. Resources with significant updates in the past year include BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Science Wikis (a catalog of biological knowledge wikis for community annotations) and IC4R (Information Commons for Rice). Newly released resources include EWAS Atlas (a knowledgebase of epigenome-wide association studies), iDog (an integrated omics data resource for dog) and RNA editing resources (for editome-disease associations and plant RNA editosome, respectively). To promote biodiversity and health big data sharing around the world, the Open Biodiversity and Health Big Data (BHBD) initiative is introduced. All of these resources are publicly accessible at http://bigd.big.ac.cn.",D8 - D14,2018.0,https://small.info/appprivacy.jsp,Biology
215,f1559567fbcc1dad4d99574eff3d6dcc216c51f6,"Next-Generation Big Data Analytics: State of the Art, Challenges, and Future Research Topics","The term big data occurs more frequently now than ever before. A large number of fields and subjects, ranging from everyday life to traditional research fields (i.e., geography and transportation, biology and chemistry, medicine and rehabilitation), involve big data problems. The popularizing of various types of network has diversified types, issues, and solutions for big data more than ever before. In this paper, we review recent research in data types, storage models, privacy, data security, analysis methods, and applications related to network big data. Finally, we summarize the challenges and development of big data to predict current and future trends.",1891-1899,2017.0,http://white.com/categoriesauthor.jsp,Computer Science
216,07ef61804c200c4071ad6fe7c1a21cd67ebfd77a,Big Data for Development: A Review of Promises and Challenges,"type=""main"" xml:id=""dpr12142-abs-0001""> The article uses a conceptual framework to review empirical evidence and some 180 articles related to the opportunities and threats of Big Data Analytics for international development. The advent of Big Data delivers a cost-effective prospect for improved decision-making in critical development areas such as healthcare, economic productivity and security. At the same time, the well-known caveats of the Big Data debate, such as privacy concerns and human resource scarcity, are aggravated in developing countries by long-standing structural shortages in the areas of infrastructure, economic resources and institutions. The result is a new kind of digital divide: a divide in the use of data-based knowledge to inform intelligent decision-making. The article systematically reviews several available policy options in terms of fostering opportunities and minimising risks.",88-112,2016.0,https://noble.info/wp-content/wp-contenthomepage.html,Economics
217,9e6aee8a492c009c91bbc52a7bc0e4d481dc2d4a,Smart manufacturing must embrace big data,,23-25,2017.0,http://www.frost.info/main/search/taghome.htm,Engineering
218,fc5747192aa13a30ba70cdba54fd10b65d1abe1b,Big Data in Healthcare Management: A Review of Literature,"A systematic literature review of papers on big data in healthcare published between 2010 and 2015 was conducted. This paper reviews the definition, process, and use of big data in healthcare management. Unstructured data are growing very faster than semi-structured and structured data. 90 percentages of the big data are in a form of unstructured data, major steps of big data management in healthcare industry are data acquisition, storage of data, managing the data, analysis on data and data visualization. Recent researches targets on big data visualization tools. In this paper the authors analysed the effective tools used for visualization of big data and suggesting new visualization tools to manage the big data in healthcare industry. This article will be helpful to understand the processes and use of big data in healthcare management.",57,2018.0,https://www.ruiz.org/category/posts/listterms.asp,Computer Science
219,00a4bdc5158945a0b9463a29da4810838e474875,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",053208,2016.0,http://norris-booker.com/listfaq.jsp,Materials Science
220,0ea7937f7e87eb8967e724d9261ee925ed254a27,Intrusion detection model using machine learning algorithm on Big Data environment,,1-12,2018.0,https://davis-cain.net/explore/wp-contenthomepage.html,Computer Science
221,90a9f18396ffcccf295eb849a866b8934b9b21c5,The big data analysis,"This article tells about the Big Data files, their features. The readers will know about methods and programms, helping to deal with Big Data. Perspectives of development and possible problems.",285-286,2018.0,https://www.steele-knox.info/explore/wp-content/categoriesregister.asp,Computer Science
222,bd1b111a2f840e88cf6e646cdfab82fe3910f896,A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives,"Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.",325-340,2018.0,http://stafford.com/explore/mainhome.asp,Computer Science
223,579c2aad3834e525a90740913fb58a8c8e9ef218,Big Data Methods,"Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.",525 - 547,2018.0,https://www.miranda-thomas.com/main/search/bloghome.html,Computer Science
224,89bd09cef9a6378685bfd67347c515d1b46e76ef,Big data and medical research in China,Luxia Zhang and colleagues discuss the development of big data in Chinese healthcare and the opportunities for its use in medical research,80-150,2018.0,https://www.russell-bush.net/app/wp-content/wp-contentregister.html,Political Science
225,7165864608ed8c0fc7343e8c2e90726936e3d980,Statistical Challenges in “Big Data” Human Neuroimaging,,263-268,2018.0,http://www.martin-moreno.info/wp-content/tagslogin.php,Medicine
226,46da3f3d592ea58211ce6bf8bec05ef0b5e792c2,Transforming big data into smart data: An insight on the use of the k‐nearest neighbors algorithm to obtain quality data,"The k‐nearest neighbors algorithm is characterized as a simple yet effective data mining technique. The main drawback of this technique appears when massive amounts of data—likely to contain noise and imperfections—are involved, turning this algorithm into an imprecise and especially inefficient technique. These disadvantages have been subject of research for many years, and among others approaches, data preprocessing techniques such as instance reduction or missing values imputation have targeted these weaknesses. As a result, these issues have turned out as strengths and the k‐nearest neighbors rule has become a core algorithm to identify and correct imperfect data, removing noisy and redundant samples, or imputing missing values, transforming Big Data into Smart Data—which is data of sufficient quality to expect a good outcome from any data mining algorithm. The role of this smart data gleaning algorithm in a supervised learning context are investigated. This includes a brief overview of Smart Data, current and future trends for the k‐nearest neighbor algorithm in the Big Data context, and the existing data preprocessing techniques based on this algorithm. We present the emerging big data‐ready versions of these algorithms and develop some new methods to cope with Big Data. We carry out a thorough experimental analysis in a series of big datasets that provide guidelines as to how to use the k‐nearest neighbor algorithm to obtain Smart/Quality Data for a high‐quality data mining process. Moreover, multiple Spark Packages have been developed including all the Smart Data algorithms analyzed.",18-141,2018.0,http://gomez-fisher.net/tags/tag/searchfaq.html,Computer Science
227,f257e3ac714cd8fcd3b22d7d27ac6fab2db34097,Multimedia Big Data Analytics,"With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.",1 - 34,2018.0,https://www.yu.net/postsauthor.jsp,Computer Science
228,6d1c5eb45a50d62c031088bc4b82828d582068a4,"Datafication, dataism and dataveillance: Big Data between scientific paradigm and ideology","Metadata and data have become a regular currency for citizens to pay for their communication services and security—a trade-off that has nestled into the comfort zone of most people. This article deconstructs the ideological grounds of datafication. Datafication is rooted in problematic ontological and epistemological claims. As part of a larger social media logic, it shows characteristics of a widespread secular belief. Dataism, as this conviction is called, is so successful because masses of people — naively or unwittingly — trust their personal information to corporate platforms. The notion of trust becomes more problematic because people’s faith is extended to other public institutions (e.g. academic research and law enforcement) that handle their (meta)data. The interlocking of government, business, and academia in the adaptation of this ideology makes us want to look more critically at the entire ecosystem of connective media.",197-208,2014.0,https://www.hickman.biz/tagsauthor.htm,Sociology
229,0d5047af65ddbce7fa21726cc5512844e32cbc0a,Big Data fraud detection using multiple medicare data sources,,1-21,2018.0,http://www.burton.com/list/posts/categoryabout.php,Computer Science
230,6a229b2c1fb7a117d8fa24598f66c51955c6d6ca,Big Data Challenges and Data Aggregation Strategies in Wireless Sensor Networks,"The emergence of new data handling technologies and analytics enabled the organization of big data in processes as an innovative aspect in wireless sensor networks (WSNs). Big data paradigm, combined with WSN technology, involves new challenges that are necessary to resolve in parallel. Data aggregation is a rapidly emerging research area. It represents one of the processing challenges of big sensor networks. This paper introduces the big data paradigm, its main dimensions that represent one of the most challenging concepts, and its principle analytic tools which are more and more introduced in the WSNs technology. The paper also presents the big data challenges that must be overcome to efficiently manipulate the voluminous data, and proposes a new classification of these challenges based on the necessities and the challenges of WSNs. As the big data aggregation challenge represents the center of our interest, this paper surveys its proposed strategies in WSNs.",20558-20571,2018.0,http://beard-martin.com/postsfaq.html,Computer Science
231,83aa94353bb6b870e9f57a2567358b29fcb83507,Big Data Analytics Services for Enhancing Business Intelligence,"ABSTRACT This article examines how to use big data analytics services to enhance business intelligence (BI). More specifically, this article proposes an ontology of big data analytics and presents a big data analytics service-oriented architecture (BASOA), and then applies BASOA to BI, where our surveyed data analysis shows that the proposed BASOA is viable for enhancing BI and enterprise information systems. This article also explores temporality, expectability, and relativity as the characteristics of intelligence in BI. These characteristics are what customers and decision makers expect from BI in terms of systems, products, and services of organizations. The proposed approach in this article might facilitate the research and development of business analytics, big data analytics, and BI as well as big data science and big data computing.",162 - 169,2018.0,https://gonzalez.com/tagsabout.html,Computer Science
232,442d88cde81ee9560d660d2dd66db7647bf15154,Ten simple rules for responsible big data research,"The use of big data research methods has grown tremendously over the past five years in both academia and industry. As the size and complexity of available datasets has grown, so too have the ethical questions raised by big data research. These questions become increasingly urgent as data and research agendas move well beyond those typical of the computational and natural sciences, to more directly address sensitive aspects of human behavior, interaction, and health. The tools of big data research are increasingly woven into our daily lives, including mining digital medical records for scientific and economic insights, mapping relationships via social media, capturing individuals’ speech and action via sensors, tracking movement across space, shaping police and security policy via “predictive policing,” and much more.",69-101,2017.0,http://www.jackson.com/blog/listcategory.htm,Computer Science
233,2e76fa107c50485985487b03a601eb0c11894193,"Big Data: A Revolution That Will Transform How We Live, Work, and Think","Amazon Exclusive: Q&A with Kenneth Cukier and Viktor Mayer-Schonberger Q. What did it take to write Big Data? A. Kenn has written about technology and business from Europe, Asia, and the US for The Economist, and is well-connected to the data community. Viktor had researched the information economy as a professor at Harvard and now at Oxford, and his book Delete had been well received. So we thought we had a good basis to make a contribution in the area. As we wrote the book, we had to dig deep to find unheard stories about big data pioneers and interview them. We wanted Big Data to be about a big idea, but also to be full of examples and success stories -- and be engrossing to read. Q. Are you big datas cheerleaders? A. Absolutely not. We are the messengers of big data, not its evangelists. The big data age is happening, and in the book we take a look at the drivers, and big datas likely trajectory: how it will change how we work and live. We emphasize that the fundamental shift is not in the machines that calculate data, but in the data itself and how we use it. Q. In discovering big data applications, what was your biggest surprise? A. It is tempting to say that it was predicting exploding manholes, tracking inflation in real time, or how big data saves the lives of premature babies. But the biggest surprise for us perhaps was the very diversity of the uses of big data, and how it already is changing peoples everyday world. Many people see big data through the lens of the Internet economy, since Google and Facebook have so much data. But that misses the point: big data is everywhere. Q. Is Big Data then primarily a story about economic efficiency? A. Big data improves economic efficiency, but thats only a very small part of the story. We realized when talking to dozens and dozens of big data pioneers that it improves health care, advances better education, and helps predict societal changefrom urban sprawl to the spread of the flu. Big data is roaring through all sectors of the economy and all areas of life. Q. So big data offers only upside? A. Not at all. We are very concerned about what we call in our book the dark side of big data. However the real challenge is that the problem is not necessarily where we initially tend to think it is, such as surveillance and privacy. After looking into the potential misuses of big data, we became much more troubled by propensity -- that is, big data predictions being used to police and punish. And by the fetishization of data that may occur, whereby organizations may blindly defer to what the data says without understanding its limitations. Q. What can we do about this dark side? A. Knowing about it is the first step. We thought hard to suggest concrete steps that can be taken to minimize and mitigate big datas risk, and came up with a few ways to ensure transparency, guarantee human free will, and strike a better balance on privacy and the use of personal information. These are deeply serious issues. If we do not take action soon, it might be too late.",31-145,2013.0,http://nichols.com/app/exploreprivacy.php,Engineering
234,cbe6b1066a9a9489b9b52d7f59af97141828fe58,The Challenges of Data Quality and Data Quality Assessment in the Big Data Era,"High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.",2,2015.0,http://www.barker.com/bloglogin.php,Computer Science
235,8065484c20a0b972f2c73531513361550ee9c194,Big Data as a Governance Mechanism,"This study empirically investigates two effects of alternative data availability: stock price informativeness and its disciplining effect on managers’ actions. Recent computing advancements have enabled technology companies to collect real-time, granular indicators of fundamentals to sell to investment professionals. These data include consumer transactions and satellite images. The introduction of these data increases price informativeness through decreased information acquisition costs, particularly in firms in which sophisticated investors have higher incentives to uncover information. I document two effects on managers. First, managers reduce their opportunistic trading. Second, investment efficiency increases, consistent with price informativeness improving managers’ incentives to invest and divest efficiently.Received June 1, 2017; editorial decision June 1, 2018 by Editor Wei Jiang. The Author has furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",100-115,2018.0,http://www.oneal-lee.info/categories/postslogin.html,Business
236,ae3c5a309638b59f0a86f36a33d360581d65c4d1,Big data and disaster management: a systematic review and agenda for future research,,939 - 959,2017.0,http://www.parker.com/tags/appmain.html,Political Science
237,474de4adfce86e14a59d02ccce59cf158fdc4c36,Big Data for Internet of Things: A Survey,,601-614,2018.0,https://king.com/wp-contentauthor.php,Computer Science
238,60119658af638693f6de23d8466968e60c428ac7,Agricultural remote sensing big data: Management and applications,,89-145,2018.0,http://www.smith.com/search/mainmain.html,Computer Science
239,db1daa777f253a5949883c3a60364a23d6a65726,Privacy Issues and Data Protection in Big Data: A Case Study Analysis under GDPR,"Big data has become a great asset for many organizations, promising improved operations and new business opportunities. However, big data has increased access to sensitive information that when processed can directly jeopardize the privacy of individuals and violate data protection laws. As a consequence, data controllers and data processors may be imposed tough penalties for non-compliance that can result even to bankruptcy. In this paper, we discuss the current state of the legal regulations and analyse different data protection and privacy-preserving techniques in the context of big data analysis. In addition, we present and analyse two real-life research projects as case studies dealing with sensitive data and actions for complying with the data regulation laws. We show which types of information might become a privacy risk, the employed privacy-preserving techniques in accordance with the legal requirements, and the influence of these techniques on the data processing phase and the research results.",5027-5033,2018.0,http://www.smith.com/category/taglogin.html,Computer Science
240,d5b65fac5a6f511c0261a14c93d702b6697ef98e,Digitalisation and Big Data Mining in Banking,"Banking as a data intensive subject has been progressing continuously under the promoting influences of the era of big data. Exploring the advanced big data analytic tools like Data Mining (DM) techniques is key for the banking sector, which aims to reveal valuable information from the overwhelming volume of data and achieve better strategic management and customer satisfaction. In order to provide sound direction for the future research and development, a comprehensive and most up to date review of the current research status of DM in banking will be extremely beneficial. Since existing reviews only cover the applications until 2013, this paper aims to fill this research gap and presents the significant progressions and most recent DM implementations in banking post 2013. By collecting and analyzing the trends of research focus, data resources, technological aids, and data analytical tools, this paper contributes to bringing valuable insights with regard to the future developments of both DM and the banking sector along with a comprehensive one stop reference table. Moreover, we identify the key obstacles and present a summary for all interested parties that are facing the challenges of big data.",18,2018.0,http://www.sullivan-anderson.info/list/wp-contentterms.html,Computer Science
241,19071a8dab567758045e996ef8f9211ddff237b3,Big Data Analysis-Based Security Situational Awareness for Smart Grid,"Advanced communications and data processing technologies bring great benefits to the smart grid. However, cyber-security threats also extend from the information system to the smart grid. The existing security works for smart grid focus on traditional protection and detection methods. However, a lot of threats occur in a very short time and overlooked by exiting security components. These threats usually have huge impacts on smart gird and disturb its normal operation. Moreover, it is too late to take action to defend against the threats once they are detected, and damages could be difficult to repair. To address this issue, this paper proposes a security situational awareness mechanism based on the analysis of big data in the smart grid. Fuzzy cluster based analytical method, game theory and reinforcement learning are integrated seamlessly to perform the security situational analysis for the smart grid. The simulation and experimental results show the advantages of our scheme in terms of high efficiency and low error rate for security situational awareness.",408-417,2018.0,http://www.smith-summers.net/tagshomepage.php,Computer Science
242,db0175df5df3deb48753545625ca003fe3dd2640,BIG data – BIG gains? Understanding the link between big data analytics and innovation,"ABSTRACT This paper analyzes the relationship between firms' use of big data analytics and their innovative performance in terms of product innovations. Since big data technologies provide new data information practices, they create novel decision-making possibilities, which are widely believed to support firms' innovation process. Applying German firm-level data within a knowledge production function framework we find suggestive evidence that big data analytics is a relevant determinant for the likelihood of a firm becoming a product innovator as well as for the market success of product innovations. These results hold for the manufacturing as well as for the service sector but are contingent on firms' investment in IT-specific skills. Overall, the results support the view that big data analytics have the potential to enable innovation.",296 - 316,2018.0,https://www.hogan-juarez.com/category/app/applogin.htm,Engineering
243,8b924846eef928a9f30c88cdd9733662668335c0,Big data: From beginning to future,,1231-1247,2016.0,http://moreno.com/categories/search/categoryauthor.htm,Computer Science
244,7d969b2a0a5ec818006f1fd7afb9e898eb913357,Cloud Infrastructure Resource Allocation for Big Data Applications,"Increasing popular big data applications bring about invaluable information, but along with challenges to industrial community and academia. Cloud computing with unlimited resources seems to be the way out. However, this panacea cannot play its role if we do not arrange fine allocation for cloud infrastructure resources. In this paper, we present a multi-objective optimization algorithm to trade off the performance, availability, and cost of Big Data application running on Cloud. After analyzing and modeling the interlaced relations among these objectives, we design and implement our approach on experimental environment. Finally, three sets of experiments show that our approach can run about 20 percent faster than traditional optimization approaches, and can achieve about 15 percent higher performance than other heuristic algorithms, while saving 4 to 20 percent cost.",313-324,2018.0,https://matthews.info/main/list/listhome.html,Computer Science
245,6ef71db293160a0e0a1a66d787f9242d99555370,Big Data and Analytics in the Modern Audit Engagement: Research Needs,"SUMMARY: Modern audit engagements often involve examination of clients that are using Big Data and analytics to remain competitive and relevant in today's business environment. Client systems now are integrated with the cloud, the Internet of Things, and external data sources such as social media. Furthermore, many engagement clients are now integrating this Big Data with new and complex business analytical approaches to generate intelligence for decision making. This scenario provides almost limitless opportunities and the urgency for the external auditor to utilize advanced analytics. This paper first positions the need for the external audit profession to move toward Big Data and audit analytics. It then reviews the regulations regarding audit evidence and analytical procedures, in contrast to the emerging environment of Big Data and advanced analytics. In a Big Data environment, the audit profession has the potential to undertake more advanced predictive and prescriptive-oriented analytics. The next s...",1-27,2017.0,https://www.sexton.com/searchfaq.html,Business
246,beb7348536ac3fcc77a770ec22e58ba97113dda1,–Omic and Electronic Health Record Big Data Analytics for Precision Medicine,"<italic>Objective:</italic> Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. <italic>Methods:</italic> In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. <italic>Results:</italic> To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. <italic>Conclusion: </italic> Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. <italic>Significance:</italic> Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.",263-273,2017.0,https://www.reed.com/tags/listhomepage.asp,Computer Science
247,acd2e65a122059f3dd0fed0aa1c4bf4dd8ce58d1,How AUDI AG Established Big Data Analytics in Its Digital Transformation,"Digital transformation, which often includes establishing big data analytics capabilities, poses considerable challenges for traditional manufacturing organizations, such as car companies. Successfully introducing big data analytics requires substantial organizational transformation and new organizational structures and business processes. Based on the three-stage evolution of big data analytics capabilities at AUDI, we provide recommendations for how traditional manufacturing organizations can successfully introduce big data analytics and master the related organizational transformations.",3,2017.0,https://davis-steele.info/wp-contentlogin.asp,Computer Science
248,02e0982171b3eed379f4dea393e1aa61d4f0a9da,Big Data Techniques in Auditing Research and Practice: Current Trends and Future Opportunities,"Abstract This paper analyses the use of big data techniques in auditing, and finds that the practice is not as widespread as it is in other related fields. We first introduce contemporary big data techniques to promote understanding of their potential application. Next, we review existing research on big data in accounting and finance. In addition to auditing, our analysis shows that existing research extends across three other genealogies: financial distress modelling, financial fraud modelling, and stock market prediction and quantitative modelling. Auditing is lagging behind the other research streams in the use of valuable big data techniques. A possible explanation is that auditors are reluctant to use techniques that are far ahead of those adopted by their clients, but we refute this argument. We call for more research and a greater alignment to practice. We also outline future opportunities for auditing in the context of real-time information and in collaborative platforms and peer-to-peer marketplaces.",89-146,2017.0,http://www.burns-wilson.org/blogregister.html,Business
249,c0f28236a0e351281ba6ec92cb33d8e5feef569e,How Sustainable Is Big Data?,"The rapid growth of “big data” provides tremendous opportunities for making better decisions, where “better” can be defined using any combination of economic, environmental, or social metrics. This essay provides a few examples of how the use of big data can precipitate more sustainable decision‐making. However, as with any technology, the use of big data on a large scale will have some undesirable consequences. Some of these are foreseeable, while others are entirely unpredictable. This essay highlights some of the sustainability‐related challenges posed by the use of big data. It does not intend to suggest that the advent of big data is an undesirable development. However, it is not too early to start asking what the unwanted repercussions of the big data revolution might be.",1685 - 1695,2018.0,https://www.weaver.info/categoryhomepage.html,Business
250,577564ac25a12b37972d77a35b589f6b2270a45f,Big Data and Data Science in Critical Care.,,"
          1239-1248
        ",2018.0,http://jenkins.info/search/tags/tagprivacy.htm,Medicine
251,352cf2e54105798d228cbd0962880bc19059a1cd,Applications of big data to smart cities,,1-15,2015.0,http://ford-rocha.com/blog/search/categoryterms.html,Computer Science
252,ca72f601cc7c3f40f2c6300e9b115610dabf4f5b,Tensor Completion Algorithms in Big Data Analytics,"Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.",1 - 48,2017.0,https://williamson.com/categoriessearch.html,Mathematics
253,363cf3b41ea1f98de4935994e45a166695f41f34,"The 10 Vs, Issues and Challenges of Big Data","In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.",59-128,2018.0,https://www.harrison.com/mainprivacy.htm,Computer Science
254,a7412ecf07ff540f9ede6eb6831803e115a16206,Recent Development in Big Data Analytics for Business Operations and Risk Management,"“Big data” is an emerging topic and has attracted the attention of many researchers and practitioners in industrial systems engineering and cybernetics. Big data analytics would definitely lead to valuable knowledge for many organizations. Business operations and risk management can be a beneficiary as there are many data collection channels in the related industrial systems (e.g., wireless sensor networks, Internet-based systems, etc.). Big data research, however, is still in its infancy. Its focus is rather unclear and related studies are not well amalgamated. This paper aims to present the challenges and opportunities of big data analytics in this unique application domain. Technological development and advances for industrial-based business systems, reliability and security of industrial systems, and their operational risk management are examined. Important areas for future research are also discussed and revealed.",81-92,2017.0,http://www.wyatt.com/blog/categories/postsprivacy.html,Medicine
255,625a9e9822603b79f754c4ce044760f7363b5eb6,Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations,"—Data quality issues trace back their origin to the early days of computing. A wide range of domain-speciﬁc techniques to assess and improve the quality of data exist in the literature. These solutions primarily target data which resides in relational databases and data warehouses. The recent emergence of big data analytics and renaissance in machine learning necessitates evaluating the suitability relational database-centric approaches to data quality. In this paper, we describe the nature of the data quality issues in the context of big data and machine learning. We discuss facets of data quality, present a data governance-driven framework for data quality lifecycle for this new scenario, and describe an approach to its implementation. A sampling of the tools available for data quality management are indicated and future trends are discussed.",57-132,2017.0,https://www.sanders.com/exploreindex.jsp,Technology
256,f957ec0ae0a4bc19a3958a2bcff9223f97f58567,"Big data: Dimensions, evolution, impacts, and challenges",,293-303,2017.0,https://kim.info/categoriesregister.php,Computer Science
257,08a02c91597115e4dad97950adf01804862ec049,"Big-Crypto: Big Data, Blockchain and Cryptocurrency","Cryptocurrency has been a trending topic over the past decade, pooling tremendous technological power and attracting investments valued over trillions of dollars on a global scale. The cryptocurrency technology and its network have been endowed with many superior features due to its unique architecture, which also determined its worldwide efficiency, applicability and data intensive characteristics. This paper introduces and summarises the interactions between two significant concepts in the digitalized world, i.e., cryptocurrency and Big Data. Both subjects are at the forefront of technological research, and this paper focuses on their convergence and comprehensively reviews the very recent applications and developments after 2016. Accordingly, we aim to present a systematic review of the interactions between Big Data and cryptocurrency and serve as the one stop reference directory for researchers with regard to identifying research gaps and directing future explorations.",34,2018.0,https://cox.info/posts/categoriessearch.html,Computer Science
258,dc967b697caec331ec7f0e95d23b1e98873b4c1e,Big Data in Health Care: Applications and Challenges,"Abstract The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.",175 - 197,2018.0,http://hernandez.org/categories/list/listabout.htm,Computer Science
259,e282659f372b67219567938a770648903713cb0a,An insight into imbalanced Big Data classification: outcomes and challenges,,105-120,2017.0,https://www.jones-briggs.net/tags/categoryabout.html,Computer Science
260,8113771bcdb76cf7363c385dbc668dcf6cef2f2f,Big Data Analytics in Healthcare,"The rapidly expanding field of big data analytics has started to play a pivotal role in the evolution of healthcare practices and research. It has provided tools to accumulate, manage, analyze, and assimilate large volumes of disparate, structured, and unstructured data produced by current healthcare systems. Big data analytics has been recently applied towards aiding the process of care delivery and disease exploration. However, the adoption rate and research development in this space is still hindered by some fundamental problems inherent within the big data paradigm. In this paper, we discuss some of these major challenges with a focus on three upcoming and promising areas of medical research: image, signal, and genomics based analytics. Recent research which targets utilization of large volumes of medical data while combining multimodal data from disparate sources is discussed. Potential areas of research within this field which have the ability to provide meaningful impact on healthcare delivery are also examined.",17-119,2015.0,https://www.everett.net/tagsterms.htm,Computer Science
261,58525d47d85d38bc48dbfbcb17020c57a7242ebf,"Big Data Facilitation, Utilization, and Monetization: Exploring the 3Vs in a New Product Development Process","Big data is transforming the new product development (NPD) process. Organizations are investing heavily in big data capabilities to capitalize on the ongoing analytics movement. Yet there is a lack of understanding of how firms can leverage big data as a capability to generate innovation success in dynamic marketplaces. To address this need for improved insights, the authors operationalize and analyze the 3Vs of big data usage—volume, variety, and velocity—in an NPD model. Drawing on the results of a survey of 261 managers reporting on their business unit's NPD processes and big data usage, this study identifies the antecedents of the multidimensional usage of big data. Empirically assessing the effects of firm orientations, the authors show that an exploration orientation has a positive effect on all three dimensions of a firm's big data usage while an exploitation orientation has no effect. Moving downstream, the results also reveal that the environmental factor of customer turbulence interacts differentially with the big data usage dimensions' impact on new product revenue (NPR). Specifically, customer turbulence accentuates the relationship between big data velocity and NPR but attenuates the relationship between big data volume and NPR.",640-658,2017.0,http://parrish.com/mainsearch.htm,Business
262,d92897c169ebb44e6d690e1dd69266894ad5c8b8,Mystiko—Blockchain Meets Big Data,"Blockchain is a peer-to-peer distributed storage that stores chronological series of transactions in a tamper-resistant manner. Blockchain became popular in various industries due to its decentralized trust ecosystem. When integrating blockchain with big data, one encounters many challenges. Current public blockchain does not support high transaction throughput; it does not scale in terms of big data storage and management; it does not provide keyword-based search and retrieval; and so on. As a result, it is hard to incorporate existing blockchain systems for big data applications. In this research, we propose a new blockchain storage ""Mystiko"" that is built over the Apache Cassandra distributed database to incorporate big data. Mystiko supports high transaction throughput, high scalability, high availability and full text search features. With Mystiko, we make big data more secure, structured and meaningful, and allows further data analytics on big data to be more easily performed.",3024-3032,2018.0,http://gomez.org/tags/main/wp-contentlogin.html,Computer Science
263,f158cad7a51f46be0d8831a00a2869ee97524048,Big Data Semantics,,65 - 85,2018.0,https://navarro.com/tagsregister.htm,Computer Science
264,d17fe932f9ebd1b94488d9e94d7bd3743dd496db,"Big data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect","The bullwhip effect is causing inefficiencies in today’s supply chains. This study deals with the potential of big data on the improvement of the various supply chain processes. The aim of this paper is to elaborate which characteristic of big data (lever) has the greatest potential to mitigate the bullwhip effect. From previous research, starting points for big data applications are derived. By using an existing system dynamics model, the big data levers ‘velocity’, ‘volume’ and ‘variety’ are transferred into a simulation. Overall, positive impacts of all the big data levers are elaborated. Findings suggest that the data property ‘velocity’ relatively bears the greatest potential to enhance performance. The results of this research will help in justifying the application of big data in supply chain management. The paper contributes to the literature by operationalising big data in the control engineering analyses.",5108 - 5126,2017.0,http://morales.com/app/postsregister.jsp,Engineering
265,056d86a6599429db18a31ec04d037589295e9028,The 'big data' revolution in healthcare: Accelerating value and innovation,"""An era of open information in healthcare is now under way. We have already experienced a decade of progress in digitizing medical records, as pharmaceutical companies and other organizations aggregate years of research and development data in electronic databases. The federal government and other public stakeholders have also accelerated the move toward transparency by making decades of stored data usable, searchable, and actionable by the healthcare sector as a whole. Together, these increases in data liquidity have brought the industry to the tipping point...""",75-127,2016.0,http://ramos.com/wp-content/explorefaq.html,Engineering
266,4272bb248e6221a4a7eee45bcdd9babb07c30f24,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,"A former Wall Street quant sounds an alarm on the mathematical models that pervade modern life and threaten to rip apart our social fabricWe live in the age of the algorithm. Increasingly, the decisions that affect our liveswhere we go to school, whether we get a car loan, how much we pay for health insuranceare being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated. But as Cathy ONeil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when theyre wrong. Most troubling, they reinforce discrimination: If a poor student cant get a loan because a lending model deems him too risky (by virtue of his zip code), hes then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a toxic cocktail for democracy. Welcome to the dark side of Big Data. Tracing the arc of a persons life, ONeil exposes the black box models that shape our future, both as individuals and as a society. These weapons of math destruction score teachers and students, sort rsums, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. ONeil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, its up to us to become more savvy about the models that govern our lives. This important book empowers us to ask the tough questions, uncover the truth, and demand change.",97 - 98,2016.0,https://turner.com/tags/maincategory.php,Engineering
267,b1b2c709b29623d1a8215315e9096f7b93f5c5b6,Data ex Machina: Introduction to Big Data,"Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a few trends that are not yet common in research using big data but will play an increasing role in it. Expected final online publication date for the Annual Review of Sociology Volume 43 is July 30, 2017. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",19-39,2017.0,http://taylor-jackson.com/categoriespost.html,Sociology
268,b9cc31a2c718b54d5798b8dab3902acc33284d93,Big data challenges and opportunities in the hype of Industry 4.0,"The world of industrial automation technology is at the outset of a new era of innovation with the hype of Industry 4.0. Global modern industrial system converges the power of machines, computing, analytics, connectivity, cyber-physical systems, Internet of things, automation, cloud system and data exchange. Industry 4.0 is a revolution towards the digital world of digital factories and smart products. Big data is an integration of multi-disciplinary technologies and facilitates customer by bringing incredible services to a click. Internet of things connected the world of machines by adding communication capability in every device to connect to other devices or access the Internet. Big Data inflict a new horizon of opportunities in these systems. In this paper, challenges and opportunities of industrial big data are revealed in the context of Industry 4.0 with a different perspective. The current study helps the researchers to threshold these modern systems of Industry 4.0 in designing big data algorithms and techniques.",1-6,2017.0,http://snow.com/search/mainmain.php,Computer Science
269,71ce7c52edbad555e7d3559f76c6c86971f76298,Macroeconomic Nowcasting and Forecasting with Big Data,"Data, data, data ... Economists know their importance well, especially when it comes to monitoring macroeconomic conditions -- the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before ""big data"" became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.",49-134,2017.0,https://anderson.info/categories/categorieshomepage.html,Computer Science
270,2b9d65579c65c2e29310632079accbe7be2fd5ab,Big Data in the Public Sector: Lessons for Practitioners and Scholars,"In this essay, we consider the role of Big Data in the public sector. Motivating our work is the recognition that Big Data is still in its infancy and many important questions regarding the true value of Big Data remain unanswered. The question we consider is as follows: What are the limits, or potential, of Big Data in the public sector? By reviewing the literature and summarizing insights from a series of interviews from public sector Chief Information Officers (CIOs), we offer a scholarly foundation for both practitioners and researchers interested in understanding Big Data in the public sector.",1043 - 1064,2017.0,https://yang.com/wp-contentprivacy.php,Political Science
271,6830f672c1765867a914155d5eb7df140fe7de16,CREDIT SCORING IN THE ERA OF BIG DATA,"TABLE OF CONTENTS TABLE OF CONTENTS I. INTRODUCTION II. TRADITIONAL CREDIT-ASSESSMENT TOOLS III. ALGORITHMS, MACHINE LEARNING, AND THE ALTERNATIVE CREDIT-SCORING MARKET A. Introduction to basic terminology and concepts B. How traditional credit-modeling tools compare to alternative, ""big-data"" tools C. Using machine learning to build a big-data credit-scoring model--how it works and potential problems IV. THE INADEQUACIES IN THE EXISTING LEGAL FRAMEWORK FOR CREDIT SCORING A. The Fair Credit Reporting Act (FCRA) B. The Equal Credit Opportunity Act (ECOA) V. THE CHALLENGES OF ALTERNATIVE CREDIT-SCORING AND A LEGISLATIVE FRAMEWORK FOR CHANGE A. Existing transparency rules are inadequate B. The burden of ensuring accuracy should not fall to the consumer C. Better tools are needed to detect and prevent discrimination by proxy D. Credit-assessment tools should not be used to target vulnerable consumers VI. CONCLUSION VII. ANNEXES I. INTRODUCTION One day in late 2008, Atlanta businessman Kevin Johnson returned home from his vacation to find an unpleasant surprise waiting in his mailbox. It was a letter from his credit card company, American Express, informing him that his credit limit had been lowered from $10,800 to a mere $3,800. (1) While Kevin was shocked that American Express would make such a drastic change to his limit, he was even more surprised by the company's reasoning. By any measure, Kevin had been an ideal customer. Kevin, who is black, was running a successful Atlanta public relations firm, was a homeowner, and had always paid his bills on time, rarely carrying a balance on his card. (2) Kevin's father, who had worked in the credit industry, had taught him the importance of responsible spending and, ""because of his father's lessons, [Kevin had] scrupulously maintained his credit since college."" (3) Yet his stellar track record and efforts to maintain ""scrupulous"" credit seemed to matter little, if at all, to American Express. The company had deemed him a risk simply because, as the letter put it, ""[o]ther customers who ha[d] used their card at establishments where [Kevin] recently shopped have a poor repayment history with American Express."" (4) When Kevin sought an explanation, the company was unwilling to share any information on which of businesses--many of them major retailers--contributed to American Express's decision to slash Kevin's limit by more than 65 percent. (5) Kevin Johnson was an early victim of a new form of credit assessment that some experts have labeled ""behavioral analysis"" or ""behavioral scoring,"" (6) but which might also be described as ""creditworthiness by association."" Rather than being judged on their individual merits and actions, consumers may find that access to credit depends on a lender's opaque predictions about a consumer's friends, neighbors, and people with similar interests, income levels, and backgrounds. This data-centric approach to credit is reminiscent of the racially discriminatory and now illegal practice of ""redlining,"" by which lenders classified applicants on the basis their zip codes, and not their individual capacities to borrow responsibly. (7) Since 2008, lenders have only intensified their use of big-data profiling techniques. With increased use of smartphones, social media, and electronic means of payment, every consumer leaves behind a digital trail of data that companies--including lenders and credit scorers--are eagerly scooping up and analyzing as a means to better predict consumer behavior. (8) The credit-scoring industry has experienced a recent explosion of start-ups that take an ""all data is credit data"" approach that combines conventional credit information with thousands of data points mined from consumers' offline and online activities. (9) Many companies also use complex algorithms to detect patterns and signals within a vast sea of information about consumers' daily lives. …",5,2017.0,https://www.harding-holmes.info/main/tagssearch.htm,Sociology
272,ac7252f1ba8fc446c95934a5287843905a80fcf9,Big data in operations and supply chain management: current trends and future perspectives,"Abstract Operations and supply chain management encompasses a vast domain and hence provides a myriad of opportunities for huge voluminous data generated from various sources in real time. Such huge data having the requisite properties of big data can be utilised to gain critical and fundamental insights towards optimising the operations and supply chain and thus making effective and efficient decisions. In the recent years, research interest in big data has increased substantially and therefore researchers and practitioners have also tried to tap the capabilities of big data to optimise operations and supply chain management. In this paper, the literature relating to the integration of big data with operations and supply chain management is reviewed. In particular, reviewing past work is primarily focused on three key areas of the operations and supply chain management, namely manufacturing, procurement and logistics where big data has been applied. In addition to reviewing past literature, paper also proposes application of big data in operations and supply chain management.",877 - 890,2017.0,http://sullivan.com/applogin.htm,Engineering
273,0d2a99f498221d284adb9782332e8950192c3b84,Analysis of agriculture data using data mining techniques: application of big data,,1-15,2017.0,http://stephens.com/posts/search/blogcategory.htm,Computer Science
274,93ac3365c6c878c5daaf32eb2c8db2a7089a4c73,Big data with cognitive computing: A review for the future,,78-89,2018.0,https://www.martinez-herrera.com/posts/tag/categoriesprivacy.html,Computer Science
275,e87f503e838ce7c2ae8a90bfa3cda7bbac8c919a,Privacy-Preserving Record Linkage for Big Data: Current Approaches and Research Challenges,,851-895,2017.0,http://www.howe.net/search/categoryregister.html,Computer Science
276,dcbd4a20c1b636376b0818b89943e9087f2914f3,SECURITY ISSUES ASSOCIATED WITH BIG DATA IN CLOUD COMPUTING,"In this paper, we discuss security issues for cloud computing, Big data, Map Reduce and Hadoop environment. The main focus is on security issues in cloud computing that are associated with big data. Big data applications are a great benefit to organizations, business, companies and many large scale and small scale industries. We also discuss various possible solutions for the issues in cloud computing security and Hadoop. Cloud computing security is developing at a rapid pace which includes computer security, network security, information security, and data privacy. Cloud computing plays a very vital role in protecting data, applications and the related infrastructure with the help of policies, technologies, controls, and big data tools. Moreover, cloud computing, big data and its applications, advantages are likely to represent the most promising new frontiers in science.",39-45,2017.0,http://www.silva-white.net/search/bloglogin.html,Computer Science
277,14f4429d868f9c4a06b0ffb1810e20bc064c95f3,Big data for policymaking: fad or fasttrack?,,367 - 382,2017.0,http://gutierrez-luna.com/searchmain.php,Economics
278,116b2bc7b7f9c78d0c5589e8c1f9f723253e6232,Internet of Things and Big Data Technologies for Next Generation Healthcare,,45-103,2017.0,http://www.lopez-rogers.com/list/main/apphomepage.jsp,Engineering
279,e41b32583e19885e7692c1168a3e241cef6e898a,Big Data Analytics for Smart Manufacturing: Case Studies in Semiconductor Manufacturing,"Smart manufacturing (SM) is a term generally applied to the improvement in manufacturing operations through integration of systems, linking of physical and cyber capabilities, and taking advantage of information including leveraging the big data evolution. SM adoption has been occurring unevenly across industries, thus there is an opportunity to look to other industries to determine solution and roadmap paths for industries such as biochemistry or biology. The big data evolution affords an opportunity for managing significantly larger amounts of information and acting on it with analytics for improved diagnostics and prognostics. The analytics approaches can be defined in terms of dimensions to understand their requirements and capabilities, and to determine technology gaps. The semiconductor manufacturing industry has been taking advantage of the big data and analytics evolution by improving existing capabilities such as fault detection, and supporting new capabilities such as predictive maintenance. For most of these capabilities: (1) data quality is the most important big data factor in delivering high quality solutions; and (2) incorporating subject matter expertise in analytics is often required for realizing effective on-line manufacturing solutions. In the future, an improved big data environment incorporating smart manufacturing concepts such as digital twin will further enable analytics; however, it is anticipated that the need for incorporating subject matter expertise in solution design will remain.",39,2017.0,http://www.davis.net/searchmain.htm,Engineering
280,99b389e6928d5d102386b3440dd7c407e7b7a131,Applicability of Big Data Techniques to Smart Cities Deployments,"This paper presents the main foundations of big data applied to smart cities. A general Internet of Things based architecture is proposed to be applied to different smart cities applications. We describe two scenarios of big data analysis. One of them illustrates some services implemented in the smart campus of the University of Murcia. The second one is focused on a tram service scenario, where thousands of transit-card transactions should be processed. Results obtained from both scenarios show the potential of the applicability of this kind of techniques to provide profitable services of smart cities, such as the management of the energy consumption and comfort in smart buildings, and the detection of travel profiles in smart transport.",800-809,2017.0,http://www.copeland-wiley.com/tags/category/taglogin.asp,Computer Science
281,33a0d4f2d280a7f1080aa1da11e8f05f06263cc6,Big Data Security Intelligence for Healthcare Industry 4.0,,103-126,2017.0,http://www.young.com/main/postscategory.htm,Business
282,1d9e78fa5bd16b0a87375712f9150c074c9a651d,Forecasting Destination Weekly Hotel Occupancy with Big Data,"Hospitality constituencies need accurate forecasting of future performance of hotels in specific destinations to benchmark their properties and better optimize operations. As competition increases, hotel managers have urgent need for accurate short-term forecasts. In this study, time-series models incorporating several tourism big data sources, including search engine queries, website traffic, and weekly weather information, are tested in order to construct an accurate forecasting model of weekly hotel occupancy for a destination. The results show the superiority of ARMAX models with both search engine queries and website traffic data in accurate forecasting. Also, the results suggest that weekly dummies are superior to Fourier terms in capturing the hotel seasonality. The limitations of the inclusion of multiple big data sources are noted since the reduction in forecasting error is minimal.",957 - 970,2017.0,http://www.davis.com/category/main/searchregister.asp,Computer Science
283,fc51107f341fff07a17139ad31ee16587ad1cd45,Exploring the path to big data analytics success in healthcare,,287-299,2017.0,http://www.lewis.org/postsprivacy.html,Computer Science
284,f08ad50f72abbfdaaf3da16d1f36f98caa499c1d,Networking for Big Data: A Survey,"Complementary to the fancy big data applications, networking for big data is an indispensable supporting platform for these applications in practice. This emerging research branch has gained extensive attention from both academia and industry in recent years. In this new territory, researchers are facing many unprecedented theoretical and practical challenges. We are therefore motivated to solicit the latest works in this area, aiming to pave a comprehensive and solid starting ground for interested readers. We first clarify the definition of networking for big data based on the cross disciplinary nature and integrated needs of the domain. Second, we present the current understanding of big data from different levels, including its formation, networking features, mathematical representations, and the networking technologies. Third, we discuss the challenges and opportunities from various perspectives in this hopeful field. We further summarize the lessons we learned based on the survey. We humbly hope this paper will shed light for forthcoming researchers to further explore the uncharted part of this promising land.",531-549,2017.0,https://rios-harris.com/exploresearch.jsp,Computer Science
285,18682ba1feb27958f96bea076bdd7decca7c4d96,"Big Data management in smart grid: concepts, requirements and implementation",,1-19,2017.0,https://www.cummings.com/searchcategory.html,Computer Science
286,5b5130060aee7c96efae18101bc8084f52380306,Incompatible: The GDPR in the Age of Big Data,"After years of drafting and negotiations, the EU finally passed the General Data Protection Regulation (GDPR). The GDPR’s impact will, most likely, be profound. Among the challenges data protection law faces in the digital age, the emergence of Big Data is perhaps the greatest. Indeed, Big Data analysis carries both hope and potential harm to the individuals whose data is analyzed, as well as other individuals indirectly affected by such analyses. These novel developments call for both conceptual and practical changes in the current legal setting. 
Unfortunately, the GDPR fails to properly address the surge in Big Data practices. The GDPR’s provisions are — to borrow a key term used throughout EU data protection regulation — incompatible with the data environment that the availability of Big Data generates. Such incompatibility is destined to render many of the GDPR’s provisions quickly irrelevant. Alternatively, the GDPR’s enactment could substantially alter the way Big Data analysis is conducted, transferring it to one that is suboptimal and inefficient. It will do so while stalling innovation in Europe and limiting utility to European citizens, while not necessarily providing such citizens with greater privacy protection. 
After a brief introduction (Part I), Part II quickly defines Big Data and its relevance to EU data protection law. Part III addresses four central concepts of EU data protection law as manifested in the GDPR: Purpose Specification, Data Minimization, Automated Decisions and Special Categories. It thereafter proceeds to demonstrate that the treatment of every one of these concepts in the GDPR is lacking and in fact incompatible with the prospects of Big Data analysis. Part IV concludes by discussing the aggregated effect of such incompatibilities on regulated entities, the EU, and society in general.",73-129,2017.0,http://richard.com/mainlogin.htm,Business
287,bdbec0c80b73919a656125bbd32f71f602980706,Big data driven smart energy management: From big data to big insights,,215-225,2016.0,https://kaiser-wade.com/main/searchhome.html,Engineering
288,b184eb1a4aaa47993c7d7fb7fb994f6abd2b274a,The Role of Big Data and Predictive Analytics in Retailing,,79-95,2017.0,https://smith.com/tagauthor.html,Computer Science
289,b14c291d99f32580fe987c20e52a248c62838e68,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",65-138,2014.0,https://bell-barnes.info/categoriesterms.php,Sociology
290,ec4bf3c2a63d2f212045d6774ff2d35a221837c1,A Secure Mechanism for Big Data Collection in Large Scale Internet of Vehicle,"As an extension for Internet of Things (IoT), Internet of Vehicles (IoV) achieves unified management in smart transportation area. With the development of IoV, an increasing number of vehicles are connected to the network. Large scale IoV collects data from different places and various attributes, which conform with heterogeneous nature of big data in size, volume, and dimensionality. Big data collection between vehicle and application platform becomes more and more frequent through various communication technologies, which causes evolving security attack. However, the existing protocols in IoT cannot be directly applied in big data collection in large scale IoV. The dynamic network structure and growing amount of vehicle nodes increases the complexity and necessary of the secure mechanism. In this paper, a secure mechanism for big data collection in large scale IoV is proposed for improved security performance and efficiency. To begin with, vehicles need to register in the big data center to connect into the network. Afterward, vehicles associate with big data center via mutual authentication and single sign-on algorithm. Two different secure protocols are proposed for business data and confidential data collection. The collected big data is stored securely using distributed storage. The discussion and performance evaluation result shows the security and efficiency of the proposed secure mechanism.",601-610,2017.0,https://www.hughes-harris.net/search/search/appindex.html,Computer Science
291,56e2e7643ac3a9832d14ade7d868892ed544fa90,Big Data Analytics for Genomic Medicine,"Genomic medicine attempts to build individualized strategies for diagnostic or therapeutic decision-making by utilizing patients’ genomic information. Big Data analytics uncovers hidden patterns, unknown correlations, and other insights through examining large-scale various data sets. While integration and manipulation of diverse genomic data and comprehensive electronic health records (EHRs) on a Big Data infrastructure exhibit challenges, they also provide a feasible opportunity to develop an efficient and effective approach to identify clinically actionable genetic variants for individualized diagnosis and therapy. In this paper, we review the challenges of manipulating large-scale next-generation sequencing (NGS) data and diverse clinical data derived from the EHRs for genomic medicine. We introduce possible solutions for different challenges in manipulating, managing, and analyzing genomic and clinical data to implement genomic medicine. Additionally, we also present a practical Big Data toolset for identifying clinically actionable genetic variants using high-throughput NGS data and EHRs.",71-150,2017.0,https://dunn-smith.com/exploresearch.html,Medicine
292,d6119ea48453b12b4928f3ab36a4b04f31aac929,Big Data with Ten Big Characteristics,"This paper reveals ten big characteristics (10 Bigs) of big data and explores their non-linear interrelationships through presenting a unified framework of big data. The framework has three levels: fundamental level, technological level, and socio-economic level. The fundamental level has four big fundamental characteristics of big data. The technological level consists of three big technological characteristics of big data. The socioeconomic level has three big socioeconomic characteristics of big data. The paper looks at each level of the proposed framework from a service-oriented perspective. The proposed approach in this paper might facilitate the research and development of big data, big data analytics, business intelligence, and business analytics.",33-117,2018.0,http://www.wright.biz/categorylogin.html,Computer Science
293,becb893edf5629482f8d8acc566a12054477aa3b,Big Data Analytics: A Review on Theoretical Contributions and Tools Used in Literature,,203 - 229,2017.0,http://ramsey.com/categoryabout.html,Computer Science
294,259dbbd8fe040863d6f7cf7bbdc03cc1c3f193be,A Big Data Analytics Method for Tourist Behaviour Analysis,,771-785,2017.0,https://www.russell.com/explore/category/bloglogin.htm,Computer Science
295,932c3b5ddef5248a984ce83f7a6dfdd5528c72b2,Will Democracy Survive Big Data and Artificial Intelligence?,,33-127,2017.0,http://www.robinson.net/posts/appabout.htm,Sociology
296,3b6c41af9410d9971325b82d521763cd607dbca7,Transport modelling in the age of big data,"ABSTRACT New Big Data sources such as mobile phone call data records, smart card data and geo-coded social media records allow to observe and understand mobility behaviour on an unprecedented level of detail. Despite the availability of such new Big Data sources, transport demand models used in planning practice still, almost exclusively, are based on conventional data such as travel diary surveys and population census. This literature review brings together recent advances in harnessing Big Data sources to understand travel behaviour and inform travel demand models that allow transport planners to compute what-if scenarios. From trip identification to activity inference, we review and analyse the existing data-mining methods that enable these opportunistically collected mobility traces inform transport demand models. We identify that future research should tap on the potential of probabilistic models and machine learning techniques as commonly used in data science. Those data-mining approaches are designed to handle the uncertainty of sparse and noisy data as it is the case for mobility traces derived from mobile phone data. In addition, they are suitable to integrate different related data sets in a data fusion scheme so as to enrich Big Data with information from travel diaries. In any case, we also acknowledge that sophisticated modelling knowledge has developed in the domain of transport planning and therefore we strongly advise that still, domain expert knowledge should build the fundament when applying data-driven approaches in transport planning. These new challenges call for a multidisciplinary collaboration between transport modellers and data scientists.",19 - 42,2017.0,https://williams.com/list/listcategory.jsp,Engineering
297,d6d0ff535fd7b86ba4d19de11587591e7359a6a0,"Big data, big decisions: The impact of big data on board level decision-making",,53-106,2018.0,http://jackson.net/categoriesmain.php,Psychology
298,e8048433239796b1eb6d65c5121512b0f811185e,A resource-efficient encryption algorithm for multimedia big data,,22703-22724,2017.0,http://jimenez-lewis.com/main/categories/tagsindex.php,Computer Science
299,5ddbe916e203ed85d99828677133dbc6e29dd678,"Big Data in the construction industry: A review of present status, opportunities, and future trends",,500-521,2016.0,http://evans.com/explorehomepage.jsp,Computer Science
300,be07d32a3eb02807f910291cc865a83a9afdc708,Sports analytics and the big-data era,,213-222,2018.0,https://www.gomez.com/blogcategory.asp,Computer Science
301,594180789a8d77b9122ed010cd43744828f2561e,Big data text analytics: an enabler of knowledge management,"Purpose 
 
 
 
 
The purpose of this paper is to examine the role of big data text analytics as an enabler of knowledge management (KM). The paper argues that big data text analytics represents an important means to visualise and analyse data, especially unstructured data, which have the potential to improve KM within organisations. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The study uses text analytics to review 196 articles published in two of the leading KM journals – Journal of Knowledge Management and Journal of Knowledge Management Research & Practice – in 2013 and 2014. The text analytics approach is used to process, extract and analyse the 196 papers to identify trends in terms of keywords, topics and keyword/topic clusters to show the utility of big data text analytics. 
 
 
 
 
Findings 
 
 
 
 
The findings show how big data text analytics can have a key enabler role in KM. Drawing on the 196 articles analysed, the paper shows the power of big data-oriented text analytics tools in supporting KM through the visualisation of data. In this way, the authors highlight the nature and quality of the knowledge generated through this method for efficient KM in developing a competitive advantage. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The research has important implications concerning the role of big data text analytics in KM, and specifically the nature and quality of knowledge produced using text analytics. The authors use text analytics to exemplify the value of big data in the context of KM and highlight how future studies could develop and extend these findings in different contexts. 
 
 
 
 
Practical implications 
 
 
 
 
Results contribute to understanding the role of big data text analytics as a means to enhance the effectiveness of KM. The paper provides important insights that can be applied to different business functions, from supply chain management to marketing management to support KM, through the use of big data text analytics. 
 
 
 
 
Originality/value 
 
 
 
 
The study demonstrates the practical application of the big data tools for data visualisation, and, with it, improving KM.",18-34,2017.0,https://hall.info/blog/mainabout.jsp,Computer Science
302,76925b455073c8021c05157d64332a616f0c8aab,Wireless Big Data Computing in Smart Grid,"The development of smart grid brings great improvement in the efficiency, reliability, and economics to power grid. However, at the same time, the volume and complexity of data in the grid explode. To address this challenge, big data technology is a strong candidate for the analysis and processing of smart grid data. In this article, we propose a big data computing architecture for smart grid analytics, which involves data resources, transmission, storage, and analysis. In order to enable big data computing in smart grid, a communication architecture is then described consisting of four main domains. Key technologies to enable big-data-aware wireless communication for smart grid are investigated. As a case study of the proposed architecture, we introduce a big-data- enabled storage planning scheme based on wireless big data computing. A hybrid approach is adopted for the optimization including GA for storage planning and a game theoretic inner optimization for daily energy scheduling. Simulation results indicate that the proposed storage planning scheme greatly reduce",58-64,2017.0,http://alvarez.org/tags/tagsprivacy.html,Computer Science
303,569fa8f129e33c09529bc28fa8d3fbf2862a7257,"Big Data, Big Insights? Advancing Service Innovation and Design With Machine Learning","Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline.",17 - 39,2018.0,http://www.blake.com/app/category/explorelogin.asp,Computer Science
304,859edc821f821b74fc9c818e45bcecb850603d07,Toward Scalable Systems for Big Data Analytics: A Technology Tutorial,"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.",652-687,2014.0,http://vincent-lyons.biz/search/postsindex.html,Computer Science
305,b890447611d11dcd8b835183a35afef16c096eb9,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.","
          63-85
        ",2017.0,https://ingram.info/wp-content/tagregister.asp,Engineering
306,26a0611b59bc73cf29a99fdd5d130d5b5e5658a9,A Survey on Emerging Computing Paradigms for Big Data,"The explosive growth of data volume and the ever-increasing demands of data value extraction have driven us into the era of big data. The “5V” (Variety, Velocity, Volume, Value, and Veracity) characteristics of big data pose great challenges to traditional computing paradigms and motivate the emergence of new solutions. Cloud computing is one of the representative technologies that can perform massive-scale and complex data computing by taking advantages of virtualized resources, parallel processing and data service integration with scalable data storage. However, as we are also experiencing the revolution of Internet-of-things (IoT), the limitations of cloud computing on supporting lightweight end devices significantly impede the flourish of cloud computing at the intersection of big data and IoT era. It also promotes the urgency of proposing new computing paradigms. We provide an overview on the topic of big data, and a comprehensive survey on how cloud computing as well as its related technologies can address the challenges arisen by big data. Then, we analyze the disadvantages of cloud computing when big data encounters IoT, and introduce two promising computing paradigms, including fog computing and transparent computing, to support the big data services of IoT. Finally, some open challenges and future directions are summarized to foster continued research efforts into this evolving field of study.",1-12,2017.0,http://www.reyes.net/postsabout.asp,Computer Science
307,2a9a2f3622ee6e0f77274986a3316e50cdfd4dd2,Big Data Analytics,,55-108,2017.0,http://www.robinson.org/categoriesauthor.html,Engineering
308,1a8d4b2054782b931840d1a5c9e885da0d3edd0b,Intellectual capital in the age of Big Data: establishing a research agenda,"The purpose of this paper is to contribute to the literature on intellectual capital (IC) in light of the emerging paradigm of Big Data. Through a literature review, this paper provides momentum for researchers and scholars to explore the emerging trends and implications of the Big Data movement in the field of IC.,A literature review highlights novel and emerging issues in IC and Big Data research, focussing on: IC for organisational value, the staged evolution of IC research, and Big Data research from the technological to the managerial paradigm. It is expected that identifying these contributions will help establish future research directions.,A conceptual multi-level framework demonstrates how Big Data validates the need to shift the focus of IC research from organisations to ecosystems. The framework is organised into four sections: “why” – the managerial reasons for incorporating Big Data into IC; “what” – the Big Data typologies that enhance IC practice; “who” – the stakeholders involved in and impacted by Big Data IC value creation; and “how” – the Big Data processes suitable for IC management.,The paper provides many avenues for future research in this emerging area of investigation. The key research questions posed aim to advance the contribution of Big Data to research on IC approaches.,The paper outlines the socio-economic value of Big Data generated by and about organisational ecosystems. It identifies opportunities for existing companies to renew their value propositions through Big Data, and discusses new tools for managing Big Data to support disclosing IC value drivers and creating new intangible assets.,This paper investigates the effects and implications Big Data offers for IC management, in support of the fourth stage of IC research. Additionally, it provides an original interpretation of IC research through the lens of Big Data.",242-261,2017.0,http://www.valencia.com/category/categories/appfaq.html,Business
309,73c57c28f60e59a0bccdae836aa34f90d0dbab15,Big Data and corporate reporting: impacts and paradoxes,"Purpose - The purpose of this paper is to investigate the phenomenon of Big Data and corporate reporting, and to determine the impact of Big Data and the current Big Data state of mind with regard to corporate reporting, what accountant and non-accountant participants’ perceptions are of the phenomenon, what the accountants’ role is and will be in this regard, and what opportunities and risks are associated with Big Data and corporate reporting. Furthermore, this study seeks to identify the inherent technological paradoxes of Big Data and corporate reporting. Design/methodology/approach - The current study is qualitative in nature and assumes an interpretive stance, investigating participants’ perceptions of the phenomenon of Big Data and corporate reporting. To this end, interview data from 25 participants, video and text material, were analysed to enhance and triangulate findings. A four-fold sampling strategy was employed to ensure that any collected data would contribute to the findings. Data were analysed on the basis of open and selective coding stages. Data collection and analysis took place in two stages, in 2014 and in 2016. Findings - Three topics, or categories, emerged from the data analysis, which have sufficient explanatory power to illustrate the phenomenon of Big Data and corporate reporting, namely the Big Data state of mind and corporate reporting, accountants’ role and future related to Big Data, and perceived opportunities and risks of Big Data. Features of a new approach to corporate reporting were identified and discussed. Furthermore, four paradoxes emerged to express inherent opposing positions of Big Data and corporate reporting, namely empowerment vs enslavement, fulfilling vs creating needs, reliability vs timeliness and simplicity vs complexity. Originality/value - The original contribution of the study lies in the empirical investigation of the phenomenon of Big Data and corporate reporting as one of the most recent and praised developments in the accounting context. The dual communication flows of corporate reporting with Big Data is an important element of the findings, which can enhance the prospective financial statements significantly. Finally, technological paradoxes of Big Data and corporate reporting are discussed for the first time, two of which are based on the literature and the remaining two are inherent in the phenomenon of Big Data and corporate reporting.",850-873,2017.0,https://www.hall.info/wp-content/categoriescategory.php,Sociology
310,f4e66bd035e195f539f1b65a5aaec0e873cdee29,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,1066 - 1078,2017.0,http://gilbert.com/tags/list/mainprivacy.html,Computer Science
311,4959bc93879786e7e3c5c80db6430bb1acbeca4f,Big Data Knowledge System in Healthcare,,133-157,2017.0,http://www.thomas.net/searchhomepage.htm,Engineering
312,1ff13ea4f9b9c9096062d120c5eb7a26e660ad96,Big Health Application System based on Health Internet of Things and Big Data,"The world is facing problems, such as uneven distribution of medical resources, the growing chronic diseases, and the increasing medical expenses. Blending the latest information technology into the healthcare system will greatly mitigate the problems. This paper presents the big health application system based on the health Internet of Things and big data. The system architecture, key technologies, and typical applications of big health system are introduced in detail.",7885-7897,2017.0,https://www.miller.com/categories/tagabout.php,Computer Science
313,1d984051486471b0ec819113e26fc4440ebff21b,The core enabling technologies of big data analytics and context-aware computing for smart sustainable cities: a review and synthesis,,1-50,2017.0,https://www.patel-pacheco.com/explore/app/wp-contenthomepage.htm,Computer Science
314,fd323c20df356d0b7f082806e22ca92eaa0bcf03,Significance and Challenges of Big Data Research,,59-64,2015.0,http://reese-jenkins.biz/tags/wp-content/searchterms.html,Computer Science
315,1cbe7243f3e91f95069020bdaf5fa753fb663439,‘Hypernudge’: Big Data as a mode of regulation by design,"ABSTRACT This paper draws on regulatory governance scholarship to argue that the analytic phenomenon currently known as ‘Big Data’ can be understood as a mode of ‘design-based’ regulation. Although Big Data decision-making technologies can take the form of automated decision-making systems, this paper focuses on algorithmic decision-guidance techniques. By highlighting correlations between data items that would not otherwise be observable, these techniques are being used to shape the informational choice context in which individual decision-making occurs, with the aim of channelling attention and decision-making in directions preferred by the ‘choice architect’. By relying upon the use of ‘nudge’ – a particular form of choice architecture that alters people’s behaviour in a predictable way without forbidding any options or significantly changing their economic incentives, these techniques constitute a ‘soft’ form of design-based control. But, unlike the static Nudges popularised by Thaler and Sunstein [(2008). Nudge. London: Penguin Books] such as placing the salad in front of the lasagne to encourage healthy eating, Big Data analytic nudges are extremely powerful and potent due to their networked, continuously updated, dynamic and pervasive nature (hence ‘hypernudge’). I adopt a liberal, rights-based critique of these techniques, contrasting liberal theoretical accounts with selective insights from science and technology studies (STS) and surveillance studies on the other. I argue that concerns about the legitimacy of these techniques are not satisfactorily resolved through reliance on individual notice and consent, touching upon the troubling implications for democracy and human flourishing if Big Data analytic techniques driven by commercial self-interest continue their onward march unchecked by effective and legitimate constraints.",118 - 136,2016.0,http://walker.biz/mainhome.jsp,Computer Science
316,0a4569cfcb193a548de7445106135445bae5950f,"Big Data: Challenges, Opportunities and Realities","With the advent of Internet of Things (IoT) and Web 2.0 technologies, there has been a tremendous growth in the amount of data generated. This chapter emphasizes on the need for big data, technological advancements, tools and techniques being used to process big data are discussed. Technological improvements and limitations of existing storage techniques are also presented. Since, the traditional technologies like Relational Database Management System (RDBMS) have their own limitations to handle big data, new technologies have been developed to handle them and to derive useful insights. This chapter presents an overview of big data analytics, its application, advantages, and limitations. Few research issues and future directions are presented in this chapter.",44-140,2017.0,http://morgan-sanchez.com/tags/blog/categoriesfaq.html,Computer Science
317,a197b5e02d4effb1df5a249d41b975a8aa70d9b5,A bibliometric approach to tracking big data research trends,,1-18,2017.0,https://www.thomas.biz/explore/search/tagssearch.html,Computer Science
318,8af6996d540a3bd1a9ceb0fc24fc50d7a5caa0c7,Does big data mean big knowledge? KM perspectives on big data and analytics,"Purpose 
 
 
 
 
This viewpoint study aims to make the case that the field of knowledge management (KM) must respond to the significant changes that big data/analytics is bringing to operationalizing the production of organizational data and information. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
This study expresses the opinions of the guest editors of “Does Big Data Mean Big Knowledge? Knowledge Management Perspectives on Big Data and Analytics”. 
 
 
 
 
Findings 
 
 
 
 
A Big Data/Analytics-Knowledge Management (BDA-KM) model is proposed that illustrates the centrality of knowledge as the guiding principle in the use of big data/analytics in organizations. 
 
 
 
 
Research limitations/implications 
 
 
 
 
This is an opinion piece, and the proposed model still needs to be empirically verified. 
 
 
 
 
Practical implications 
 
 
 
 
It is suggested that academics and practitioners in KM must be capable of controlling the application of big data/analytics, and calls for further research investigating how KM can conceptually and operationally use and integrate big data/analytics to foster organizational knowledge for better decision-making and organizational value creation. 
 
 
 
 
Originality/value 
 
 
 
 
The BDA-KM model is one of the early models placing knowledge as the primary consideration in the successful organizational use of big data/analytics.",1-6,2017.0,http://www.shaw-miller.biz/tagsearch.asp,Computer Science
319,fe944ce6918c27e20e8c410d05950e84bd89987f,Heterogeneous Data and Big Data Analytics,"Heterogeneity is one of major features of big data and heterogeneous data result in problems in data integration and Big Data analytics. This paper introduces data processing methods for heterogeneous data and Big Data analytics, Big Data tools, some traditional data mining (DM) and machine learning (ML) methods. Deep learning and its potential in Big Data analytics are analysed. The benefits of the confluences among Big Data analytics, deep learning, high performance computing (HPC), and heterogeneous computing are presented. Challenges of dealing with heterogeneous data and Big Data analytics are also discussed.",8-15,2017.0,http://myers-barker.com/tags/appmain.html,Computer Science
320,311e4108de154ac0d60c4b2e9b66ed543ac69e2a,Addressing barriers to big data,,285-292,2017.0,https://www.preston.org/tagsabout.php,Business
321,236c874bad9845d2750e9579d0c23a97c71f4cfb,Big Data Visualization Tools,,60-135,2018.0,http://www.johnson-cardenas.net/categoriesprivacy.html,Computer Science
322,e968e60740164dcd75731892679939904d3551ff,Big Data sources and methods for social and economic analyses,,99-113,2017.0,http://www.glass.com/tags/taglogin.php,Economics
323,32bebd75012f43840ada24c6ab3c917d6dff99bf,Social big data: Recent achievements and new challenges,,45 - 59,2015.0,http://russell.net/app/blog/listpost.php,Computer Science
324,213976a51735aaabc96eb53444b02f2c7ca04f4f,Big Data and Management,The authors reflect on management of big data by organizations. They comment on service level agreements (SLA) which define the nature and quality of information technology services and mention big data-sharing agreements tend to be poorly structured and informal. They reflect on the methodologies of analyzing big data and state it is easy to get false correlations when using typical statistical tools in analyzing big data. They talk about the use of big data in management and behavior research.,321-326,2014.0,http://www.jenkins-brennan.net/appsearch.jsp,Computer Science
325,f34769765f75f94dc81b21faa2e50594ef1fcbfb,A Survey of Clustering Algorithms for Big Data: Taxonomy and Empirical Analysis,"Clustering algorithms have emerged as an alternative powerful meta-learning tool to accurately analyze the massive volume of data generated by modern applications. In particular, their main goal is to categorize data into clusters such that objects are grouped in the same cluster when they are similar according to specific metrics. There is a vast body of knowledge in the area of clustering and there has been attempts to analyze and categorize them for a larger number of applications. However, one of the major issues in using clustering algorithms for big data that causes confusion amongst practitioners is the lack of consensus in the definition of their properties as well as a lack of formal categorization. With the intention of alleviating these problems, this paper introduces concepts and algorithms related to clustering, a concise survey of existing (clustering) algorithms as well as providing a comparison, both from a theoretical and an empirical perspective. From a theoretical perspective, we developed a categorizing framework based on the main properties pointed out in previous studies. Empirically, we conducted extensive experiments where we compared the most representative algorithm from each of the categories using a large number of real (big) data sets. The effectiveness of the candidate clustering algorithms is measured through a number of internal and external validity metrics, stability, runtime, and scalability tests. In addition, we highlighted the set of clustering algorithms that are the best performing for big data.",267-279,2014.0,http://www.mcdaniel.biz/listregister.php,Computer Science
326,9883e7c5b130a8cd9656bce0cfe92de27e63b67e,Predicting Tourist Demand Using Big Data,,13-29,2017.0,http://welch.com/tags/categoryindex.asp,Business
327,2302c56eb40f3339e910fa8197d7429e6ba8b032,The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts,,303 - 341,2015.0,http://www.waller.net/list/search/searchmain.php,Sociology
328,d5bf31824ec3560262525868fcd89211b2f50475,Big Data Model of Security Sharing Based on Blockchain,"The rise of big data age in the Internet has led to the explosive growth of data size. However, trust issue has become the biggest problem of big data, leading to the difficulty in data safe circulation and industry development. The blockchain technology provides a new solution to this problem by combining non-tampering, traceable features with smart contracts that automatically execute default instructions. In this paper, we present a credible big data sharing model based on blockchain technology and smart contract to ensure the safe circulation of data resources.",117-121,2017.0,http://conner.com/exploreindex.jsp,Computer Science
329,8a076dc0034c9c92e444c60dbd3f5f31ac28ab8b,Big data analytics: does organizational factor matters impact technology acceptance?,,59-106,2017.0,https://mitchell-hale.com/explore/app/searchcategory.htm,Computer Science
330,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",144-160,2017.0,http://www.holmes.biz/list/list/searchterms.html,Computer Science
331,dd4015e51085e24a2a213c7e2efe35c4b10ae781,Speaking Sociologically with Big Data: Symphonic Social Science and the Future for Big Data Research,"Recent years have seen persistent tension between proponents of big data analytics, using new forms of digital data to make computational and statistical claims about ‘the social’, and many sociologists sceptical about the value of big data, its associated methods and claims to knowledge. We seek to move beyond this, taking inspiration from a mode of argumentation pursued by Piketty, Putnam and Wilkinson and Pickett that we label ‘symphonic social science’. This bears both striking similarities and significant differences to the big data paradigm and – as such – offers the potential to do big data analytics differently. This offers value to those already working with big data – for whom the difficulties of making useful and sustainable claims about the social are increasingly apparent – and to sociologists, offering a mode of practice that might shape big data analytics for the future.",1132 - 1148,2017.0,https://www.brock.com/blog/tagsterms.html,Sociology
332,8e641ea510c06485f33a61ea65555cf791efad2d,Big data and its technical challenges,Exploring the inherent technical challenges in realizing the potential of Big Data.,86-94,2014.0,https://mathis-baker.com/app/blog/exploreabout.htm,Computer Science
333,22837fbb237260d74ff3a5754a1ddb55577e1250,Leveraging Frontline Employees’ Small Data and Firm-Level Big Data in Frontline Management,"The advent of new forms of data, modern technology, and advanced data analytics offer service providers both opportunities and risks. This article builds on the phenomenon of big data and offers an integrative conceptual framework that captures not only the benefits but also the costs of big data for managing the frontline employee (FLE)-customer interaction. Along the positive path, the framework explains how the “3Vs” of big data (volume, velocity, and variety) have the potential to improve service quality and reduce service costs by influencing big data value and organizational change at the firm and FLE levels. However, the 3Vs of big data also increase big data veracity, which casts doubt about the value of big data. The authors further propose that because of heterogeneity in big data absorptive capacities at the firm level, the costs of adopting big data in FLE management may outweigh the benefits. Finally, while FLEs can benefit from big data, extracting knowledge from such data does not discount knowledge derived from FLEs’ small data. Rather, combining and integrating the firm’s big data with FLEs’ small data are crucial to absorbing and applying big data knowledge. An agenda for future research concludes.",12 - 28,2017.0,https://ali.biz/wp-contentlogin.jsp,Business
334,dcdd7b809849be54f82876137ecca7404e7f9d27,Big data issues in smart grid – A review,,1099-1107,2017.0,https://www.taylor-anderson.com/tags/mainmain.html,Engineering
335,7f663e08b32c0d1f0758d6ea02cb261cc3770fbc,Disparate Impact in Big Data Policing,"Police departments large and small have begun to use data mining techniques to predict the where, when, and who of crime before it occurs. But data mining systems can have a disproportionately adverse impact on vulnerable communities, and predictive policing is no different. Reviewing the technical process of predictive policing, the Article begins by illustrating how use of predictive policing technology will often result in disparate impact on communities of color. After evaluating the possibilities for Fourth Amendment regulation and finding them wanting, the Article turns toward a new regulatory proposal.The Article proposes the use of a rulemaking procedure centered on “discrimination impact assessments.” Predictive policing, like a great deal of data mining solutions, is sold in part as a “neutral” method to counteract unconscious biases. At the moment, however, police departments adopting the technology are not evaluating its potential for a discriminatory impact, which might reproduce or exacerbate the unconscious biases its proponents claim it will cure. Modeled on the environmental impact statements of the National Environmental Policy Act, discrimination impact assessments would require police departments to evaluate the potential discriminatory effects of competing alternative algorithms and to publicly consider mitigation procedures. This regulation balances the need for police expertise in the adoption of new crime control technologies with transparency and public input regarding the potential for harm. Such a public process will also serve to increase trust between police departments and the communities they serve.",3373,2017.0,https://garza.com/tagpost.html,Business
336,025917fd73695c87b2b35d8059b2961f433ae048,Big data machine learning using apache spark MLlib,"Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.",3492-3498,2017.0,https://www.tucker.net/main/explorepost.jsp,Computer Science
337,031cbbc628c6d83e977d24f05097130ee5ad945b,HR and analytics: why HR is set to fail the big data challenge,"The HR world is abuzz with talk of big data and the transformative potential of HR analytics. This article takes issue with optimistic accounts which hail HR analytics as a ‘must have’ capability that will ensure HR’s future as a strategic management function while transforming organisational performance for the better. It argues that unless the HR profession wises up to both the potential and drawbacks of this emerging field, and engages operationally and strategically to develop better methods and approaches, it is unlikely that existing practices of HR analytics will deliver transformational change. Indeed, it is possible that current trends will seal the exclusion of HR from strategic, board level influence while doing little to benefit organisations and actively damaging the interests of employees.",1-11,2016.0,https://curtis.org/categorypost.asp,Business
338,644ba94b6fe1d19708feee478cdf31bd457ff6ba,"What makes Big Data, Big Data? Exploring the ontological characteristics of 26 datasets","Big Data has been variously defined in the literature. In the main, definitions suggest that Big Data possess a suite of key traits: volume, velocity and variety (the 3Vs), but also exhaustivity, resolution, indexicality, relationality, extensionality and scalability. However, these definitions lack ontological clarity, with the term acting as an amorphous, catch-all label for a wide selection of data. In this paper, we consider the question ‘what makes Big Data, Big Data?’, applying Kitchin’s taxonomy of seven Big Data traits to 26 datasets drawn from seven domains, each of which is considered in the literature to constitute Big Data. The results demonstrate that only a handful of datasets possess all seven traits, and some do not possess either volume and/or variety. Instead, there are multiple forms of Big Data. Our analysis reveals that the key definitional boundary markers are the traits of velocity and exhaustivity. We contend that Big Data as an analytical category needs to be unpacked, with the genus of Big Data further delineated and its various species identified. It is only through such ontological work that we will gain conceptual clarity about what constitutes Big Data, formulate how best to make sense of it, and identify how it might be best used to make sense of the world.",35-137,2016.0,https://www.stevens.biz/listpost.html,Computer Science
339,fd4879e9c17a5d6551dd9f3a0cb9df0fe1d04637,Big Data Application in Biomedical Research and Health Care: A Literature Review,"Big data technologies are increasingly used for biomedical and health-care informatics research. Large amounts of biological and clinical data have been generated and collected at an unprecedented speed and scale. For example, the new generation of sequencing technologies enables the processing of billions of DNA sequence data per day, and the application of electronic health records (EHRs) is documenting large amounts of patient data. The cost of acquiring and analyzing biomedical data is expected to decrease dramatically with the help of technology upgrades, such as the emergence of new sequencing machines, the development of novel hardware and software for parallel computing, and the extensive expansion of EHRs. Big data applications present new opportunities to discover new knowledge and create novel methods to improve the quality of health care. The application of big data in health care is a fast-growing field, with many new discoveries and methodologies published in the last five years. In this paper, we review and discuss big data application in four major biomedical subdisciplines: (1) bioinformatics, (2) clinical informatics, (3) imaging informatics, and (4) public health informatics. Specifically, in bioinformatics, high-throughput experiments facilitate the research of new genome-wide association studies of diseases, and with clinical informatics, the clinical field benefits from the vast amount of collected patient data for making intelligent decisions. Imaging informatics is now more rapidly integrated with cloud platforms to share medical image data and workflows, and public health informatics leverages big data techniques for predicting and monitoring infectious disease outbreaks, such as Ebola. In this paper, we review the recent progress and breakthroughs of big data applications in these health-care domains and summarize the challenges, gaps, and opportunities to improve and advance big data applications in health care.",1 - 10,2016.0,https://cook-johnson.info/wp-content/posts/postscategory.htm,Medicine
340,312c3d413fabb11478b8374bf307c24a90be213f,Handbook of Big Data Technologies,,15-126,2017.0,https://www.barber.net/blogmain.php,Computer Science
341,d517b13f2b152c913b81ce534a149493517dbdad,Big Data Deep Learning: Challenges and Perspectives,"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.",514-525,2014.0,http://ford-henderson.org/postspost.jsp,Computer Science
342,6d5867465c55b2d8fda24591c3cb40af416de525,Big Data for Remote Sensing: Challenges and Opportunities,"Every day a large number of Earth observation (EO) spaceborne and airborne sensors from many different countries provide a massive amount of remotely sensed data. Those data are used for different applications, such as natural hazard monitoring, global climate change, urban planning, etc. The applications are data driven and mostly interdisciplinary. Based on this it can truly be stated that we are now living in the age of big remote sensing data. Furthermore, these data are becoming an economic asset and a new important resource in many applications. In this paper, we specifically analyze the challenges and opportunities that big data bring in the context of remote sensing applications. Our focus is to analyze what exactly does big data mean in remote sensing applications and how can big data provide added value in this context. Furthermore, this paper describes the most challenging issues in managing, processing, and efficient exploitation of big data for remote sensing problems. In order to illustrate the aforementioned aspects, two case studies discussing the use of big data in remote sensing are demonstrated. In the first test case, big data are used to automatically detect marine oil spills using a large archive of remote sensing data. In the second test case, content-based information retrieval is performed using high-performance computing (HPC) to extract information from a large database of remote sensing images, collected after the terrorist attack to the World Trade Center in New York City. Both cases are used to illustrate the significant challenges and opportunities brought by the use of big data in remote sensing applications.",2207-2219,2016.0,http://www.edwards.org/app/listsearch.asp,Computer Science
343,d4dafb2f797630d1ff45d7833e385a848380e449,Big data technologies and Management: What conceptual modeling can do,,50-67,2017.0,https://skinner.com/categories/categorieshomepage.html,Computer Science
344,99b7c4f33ae58fcbc118571e6635f80d025576eb,Conceptualising the right to data protection in an era of Big Data,"In 2009, with the enactment of the Lisbon Treaty, the Charter of Fundamental Rights of the European Union entered into force. Under Article 8 of the Charter, for the first time, a stand-alone fundamental right to data protection was declared. The creation of this right, standing as a distinct right to the right to privacy, is undoubtedly significant, and it is unique to the European legal order, being absent from other international human rights instruments. This commentary examines the parameters of this new right to data protection, asking what are the principles underpinning the right. It argues that the right reflects some key values inherent in the European legal order, namely: privacy, transparency, autonomy and nondiscrimination. It also analyses some of the challenges in implementing this right in an era of ubiquitous veillance practices and Big Data.",97-117,2017.0,http://www.wright.org/explore/tags/exploreindex.jsp,Computer Science
345,fa4fa3418559e97418af35c8361628a7f81b7932,Big data preprocessing: methods and prospects,,1-22,2016.0,http://www.houston.com/exploresearch.jsp,Computer Science
346,53834f0ee8df731cf0e629cd594dce0afaaa3d97,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer","
          1351-2
        ",2013.0,https://garcia.com/wp-content/tags/searchsearch.php,Medicine
347,0738f0d7dec95724fd2b58b51ef04b6bff3cf531,Big Data Meets Quantum Chemistry Approximations: The Δ-Machine Learning Approach.,"Chemically accurate and comprehensive studies of the virtual space of all possible molecules are severely limited by the computational cost of quantum chemistry. We introduce a composite strategy that adds machine learning corrections to computationally inexpensive approximate legacy quantum methods. After training, highly accurate predictions of enthalpies, free energies, entropies, and electron correlation energies are possible, for significantly larger molecular sets than used for training. For thermochemical properties of up to 16k isomers of C7H10O2 we present numerical evidence that chemical accuracy can be reached. We also predict electron correlation energy in post Hartree-Fock methods, at the computational cost of Hartree-Fock, and we establish a qualitative relationship between molecular entropy and electron correlation. The transferability of our approach is demonstrated, using semiempirical quantum chemistry and machine learning models trained on 1 and 10% of 134k organic molecules, to reproduce enthalpies of all remaining molecules at density functional theory level of accuracy.","
          2087-96
        ",2015.0,https://bauer.org/wp-contentabout.asp,Physics
348,8f2a211fe0386b539ab6de383516fe17a0c47345,Smart Clothing: Connecting Human with Clouds and Big Data for Sustainable Health Monitoring,,825 - 845,2016.0,http://williams.com/postsprivacy.jsp,Computer Science
349,11d6ab00e757b75497414e589d7c03951c433217,Big data analytics for security and criminal investigations,"Applications of various data analytics technologies to security and criminal investigation during the past three decades have demonstrated the inception, growth, and maturation of criminal analytics. We first identify five cutting‐edge data mining technologies such as link analysis, intelligent agents, text mining, neural networks, and machine learning. Then, we explore their recent applications to the criminal analytics domain, and discuss the challenges arising from these innovative applications. We also extend our study to big data analytics which provides some state‐of‐the‐art technologies to reshape criminal investigations. In this paper, we review the recent literature, and examine the potentials of big data analytics for security intelligence under a criminal analytics framework. We examine some common data sources, analytics methods, and applications related to two important aspects of social network analysis namely, structural analysis and positional analysis that lay the foundation of criminal analytics. Another contribution of this paper is that we also advocate a novel criminal analytics methodology that is underpinned by big data analytics. We discuss the merits and challenges of applying big data analytics to the criminal analytics domain. Finally, we highlight the future research directions of big data analytics enhanced criminal investigations. WIREs Data Mining Knowl Discov 2017, 7:e1208. doi: 10.1002/widm.1208",44-102,2017.0,http://www.smith.com/tags/wp-contentterms.html,Computer Science
350,05425a0b1e8cd37c10250d520ae7cda14b611cf8,Big data analytics to improve cardiovascular care: promise and challenges,,350-359,2016.0,http://www.norman-butler.com/tagregister.html,Medicine
351,d6d793cf8cfbee473125c322b046e6df26361efd,Big data caching for networking: moving from cloud to edge,"In order to cope with the relentless data tsunami in 5G wireless networks, current approaches such as acquiring new spectrum, deploying more BSs, and increasing nodes in mobile packet core networks are becoming ineffective in terms of scalability, cost and flexibility. In this regard, context- aware 5G networks with edge/cloud computing and exploitation of big data analytics can yield significant gains for mobile operators. In this article, proactive content caching in 5G wireless networks is investigated in which a big-data-enabled architecture is proposed. In this practical architecture, a vast amount of data is harnessed for content popularity estimation, and strategic contents are cached at BSs to achieve higher user satisfaction and backhaul offloading. To validate the proposed solution, we consider a real-world case study where several hours worth of mobile data traffic is collected from a major telecom operator in Turkey, and big-data-enabled analysis is carried out, leveraging tools from machine learning. Based on the available information and storage capacity, numerical studies show that several gains are achieved in terms of both user satisfaction and backhaul offloading. For example, in the case of 16 BSs with 30 percent of content ratings and 13 GB storage size (78 percent of total library size), proactive caching yields 100 percent user satisfaction and offloads 98 percent of the backhaul.",36-42,2016.0,http://harvey-french.info/categories/categoriesregister.html,Computer Science
352,5ea3ff5390ec5c36db5ecc8d7a04fa2d88559005,Persisting big-data: The NoSQL landscape,,1-23,2017.0,http://www.gibbs.com/tags/tags/categoryterms.htm,Computer Science
353,aebe2eda5774f46d6413f45c11df58d6339a1003,Challenges and Opportunities of Big Data in Health Care: A Systematic Review,"Background Big data analytics offers promise in many business sectors, and health care is looking at big data to provide answers to many age-related issues, particularly dementia and chronic disease management. Objective The purpose of this review was to summarize the challenges faced by big data analytics and the opportunities that big data opens in health care. Methods A total of 3 searches were performed for publications between January 1, 2010 and January 1, 2016 (PubMed/MEDLINE, CINAHL, and Google Scholar), and an assessment was made on content germane to big data in health care. From the results of the searches in research databases and Google Scholar (N=28), the authors summarized content and identified 9 and 14 themes under the categories Challenges and Opportunities, respectively. We rank-ordered and analyzed the themes based on the frequency of occurrence. Results The top challenges were issues of data structure, security, data standardization, storage and transfers, and managerial skills such as data governance. The top opportunities revealed were quality improvement, population management and health, early detection of disease, data quality, structure, and accessibility, improved decision making, and cost reduction. Conclusions Big data analytics has the potential for positive impact and global implications; however, it must overcome some legitimate obstacles.",92-122,2016.0,http://www.leon.info/category/categoryprivacy.php,Medicine
354,4271319294949d4ee6c88b653429af9ab7f58a9b,The impact of big data on world-class sustainable manufacturing,,631-645,2016.0,http://mendez.com/search/list/postshomepage.php,Computer Science
355,f289a7872a4b19cf3210da9b0a96e0d2fea4ea95,Big Data and analytics in higher education: Opportunities and challenges,Institutions of higher education are operating in an increasingly complex and competitive environment. This paper identifies contemporary challenges facing institutions of higher education worldwide and explores the potential of Big Data in addressing these challenges. The paper then outlines a number of opportunities and challenges associated with the implementation of Big Data in the context of higher education.The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.,904-920,2015.0,http://www.brown.com/blog/poststerms.htm,Computer Science
356,e301beb0e17805dbabf5add06d99c53e8703ea34,Gaussian Processes for Big Data,We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.,27-135,2013.0,https://allen-taylor.net/wp-content/app/bloghomepage.php,Computer Science
357,81b7e5635241f0fae8eac9f703f604a1a0073038,Learning to Hash for Indexing Big Data—A Survey,"The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.",34-57,2015.0,http://www.hansen-gibson.com/categoryfaq.html,Computer Science
358,69732dcf45024f28e5c43de68d1208f6e737eada,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",D18 - D24,2016.0,http://www.blake-lopez.com/categoryabout.asp,Medicine
359,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",1493-1507,2016.0,https://www.schwartz.com/wp-content/tagprivacy.html,Computer Science
360,5b023b7169217d965be03b6cdf196142bd507806,Big data in health care: using analytics to identify and manage high-risk and high-cost patients.,"The US health care system is rapidly adopting electronic health records, which will dramatically increase the quantity of clinical data that are available electronically. Simultaneously, rapid progress has been made in clinical analytics--techniques for analyzing large quantities of data and gleaning new insights from that analysis--which is part of what is known as big data. As a result, there are unprecedented opportunities to use big data to reduce the costs of health care in the United States. We present six use cases--that is, key examples--where some of the clearest opportunities exist to reduce costs through the use of big data: high-cost patients, readmissions, triage, decompensation (when a patient's condition worsens), adverse events, and treatment optimization for diseases affecting multiple organ systems. We discuss the types of insights that are likely to emerge from clinical analytics, the types of data needed to obtain such insights, and the infrastructure--analytics, algorithms, registries, assessment scores, monitoring devices, and so forth--that organizations will need to perform the necessary analyses and to implement changes that will improve care while reducing costs. Our findings have policy implications for regulatory oversight, ways to address privacy concerns, and the support of research on analytics.","
          1123-31
        ",2014.0,https://lang-smith.org/wp-content/mainpost.htm,Medicine
361,f77331fc287e4bc76c4b3c464121ec6453fd448b,When big data meets software-defined networking: SDN for big data and big data for SDN,"Both big data and software-defined networking (SDN) have attracted great interests from both academia and industry. These two important areas have traditionally been addressed separately in the most of previous works. However, on the one hand, the good features of SDN can greatly facilitate big data acquisition, transmission, storage, and processing. On the other hand, big data will have profound impacts on the design and operation of SDN. In this paper, we present the good features of SDN in solving several issues prevailing with big data applications, including big data processing in cloud data centers, data delivery, joint optimization, scientific big data architectures and scheduling issues. We show that SDN can manage the network efficiently for improving the performance of big data applications. In addition, we show that big data can benefit SDN as well, including traffic engineering, cross-layer design, defeating security attacks, and SDN-based intra and inter data center networks. Moreover, we discuss a number of open issues that need to be addressed to jointly consider big data and SDN in future research.",58-65,2016.0,http://gonzalez.com/blogindex.htm,Computer Science
362,9921cd9e5abf49659f72a1b95cf61368b4243332,Big Data Meet Green Challenges: Big Data Toward Green Applications,"Big data are widely recognized as being one of the most powerful drivers to promote productivity, improve efficiency, and support innovation. It is highly expected to explore the power of big data and turn big data into big values. To answer the interesting question whether there are inherent correlations between the two tendencies of big data and green challenges, a recent study has investigated the issues on greening the whole life cycle of big data systems. This paper would like to discover the relations between the trend of big data era and that of the new generation green revolution through a comprehensive and panoramic literature survey in big data technologies toward various green objectives and a discussion on relevant challenges and future directions.",888-900,2016.0,https://hernandez.com/tag/app/appprivacy.html,Engineering
363,0751d9c1738d4fb71d4624c05913b2dfa162b279,Big data privacy: a technological perspective and review,,56-130,2016.0,https://vargas-gill.biz/listabout.jsp,Computer Science
364,5dfbb89afb2d77c3afcb6a2cc2d24e537963a55b,Big Privacy: Challenges and Opportunities of Privacy Study in the Age of Big Data,"One of the biggest concerns of big data is privacy. However, the study on big data privacy is still at a very early stage. We believe the forthcoming solutions and theories of big data privacy root from the in place research output of the privacy discipline. Motivated by these factors, we extensively survey the existing research outputs and achievements of the privacy field in both application and theoretical angles, aiming to pave a solid starting ground for interested readers to address the challenges in the big data case. We first present an overview of the battle ground by defining the roles and operations of privacy systems. Second, we review the milestones of the current two major research categories of privacy: data clustering and privacy frameworks. Third, we discuss the effort of privacy study from the perspectives of different disciplines, respectively. Fourth, the mathematical description, measurement, and modeling on privacy are presented. We summarize the challenges and opportunities of this promising topic at the end of this paper, hoping to shed light on the exciting and almost uncharted land.",2751-2763,2016.0,https://holmes.com/poststerms.htm,Computer Science
365,31931aa8d814d45f9cfcd1765aff07e443cca99d,Understanding big data,"Give us 5 minutes and we will show you the best book to read today. This is it, the understanding big data that will be your best choice for better reading book. Your five times will not spend wasted by reading this website. You can take the book as a source to make better concept. Referring the books that can be situated with your needs is sometime difficult. But here, this is so easy. You can find the best thing of book that you can read.",26-123,2016.0,http://www.chavez.net/search/app/tagsregister.php,Computer Science
366,93b00fd9a9a5f3424d4f2aad7c821f6b33d1d064,Big Data for Modern Industry: Challenges and Trends [Point of View],"We are living in an era of data deluge and as a result, the term ‘‘big data’’ is appearing in many contexts, from meteorology, genomics, complex physics simulations, biological and environmental research, finance and business to healthcare. Explores ways in business and industry is working to manage data and reports on applications that support these initiatives.",143-146,2015.0,http://www.richardson-hernandez.com/posts/blog/mainlogin.html,Engineering
367,094b19739855efbdacb17a83508c42582391c263,"New games, new rules: big data and the changing context of strategy",,44-57,2015.0,http://sanchez.com/search/tags/wp-contenthome.jsp,Computer Science
368,ac11fc3acdf5233bda411dee6d72a424e0b33c4e,Big data analytics on Apache Spark,,145-164,2016.0,http://holmes.com/wp-contentcategory.php,Computer Science
369,49a46baf194ac92e01f54b2eecc767514854edcb,Big data-driven optimization for mobile networks toward 5G,"Big data offers a plethora of opportunities to mobile network operators for improving quality of service. This article explores various means of integrating big data analytics with network optimization toward the objective of improving the user quality of experience. We first propose a framework of Big Data-Driven (BDD) mobile network optimization. We then present the characteristics of big data that are collected not only from user equipment but also from mobile networks. Moreover, several techniques in data collection and analytics are discussed from the viewpoint of network optimization. Certain user cases on the application of the proposed framework for improving network performance are also given in order to demonstrate the feasibility of the framework. With the integration of the emerging fifth generation (5G) mobile networks with big data analytics, the quality of our daily mobile life is expected to be tremendously enhanced.",44-51,2016.0,https://www.harris.org/tag/exploreindex.htm,Computer Science
370,50f15da80fbc5098db7e6d7aa815e493ea580c54,Big data: A review,"Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.",42-47,2013.0,https://moore.net/wp-content/explore/listcategory.htm,Computer Science
371,131cef9d6caf65a8cf7ad6b771c531c409e8930d,Big Data and supply chain management: a review and bibliometric analysis,,313 - 336,2016.0,http://www.edwards.info/category/blog/wp-contentauthor.htm,Computer Science
372,75b868e844e58db707fb5dbf7acbe2e26ba8c122,A survey on indexing techniques for big data: taxonomy and performance evaluation,,241-284,2016.0,https://barr.com/tag/posts/categoryauthor.asp,Computer Science
373,006c846c72e77cb913be4b2c76664967e9e01ee0,AI^2: Training a Big Data Machine to Defend,"We present AI2, an analyst-in-the-loop security system where Analyst Intuition (AI) is put together with state-of-the-art machine learning to build a complete end-to-end Artificially Intelligent solution (AI). The system presents four key features: a big data behavioral analytics platform, an outlier detection system, a mechanism to obtain feedback from security analysts, and a supervised learning module. We validate our system with a real-world data set consisting of 3.6 billion log lines and 70.2 million entities. The results show that the system is capable of learning to defend against unseen attacks. With respect to unsupervised outlier analysis, our system improves the detection rate in 2.92× and reduces false positives by more than 5×.",49-54,2016.0,http://johnson.com/search/list/appcategory.html,Computer Science
374,6237ed8129f7bdb2ec52482725d5418ad8d61584,Protection of Big Data Privacy,"In recent years, big data have become a hot research topic. The increasing amount of big data also increases the chance of breaching the privacy of individuals. Since big data require high computational power and large storage, distributed systems are used. As multiple parties are involved in these systems, the risk of privacy violation is increased. There have been a number of privacy-preserving mechanisms developed for privacy protection at different stages (e.g., data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a comprehensive overview of the privacy preservation mechanisms in big data and present the challenges for existing mechanisms. In particular, in this paper, we illustrate the infrastructure of big data and the state-of-the-art privacy-preserving mechanisms in each stage of the big data life cycle. Furthermore, we discuss the challenges and future research directions related to privacy preservation in big data.",1821-1834,2016.0,http://parker-wu.info/tags/tagscategory.php,Computer Science
375,3701d3a06cec47dc28b5d6a482a41ced2711bea6,Analytics: The real-world use of big data in financial services studying with judge system events,,210 - 214,2016.0,https://gray.com/main/searchmain.jsp,Engineering
376,1c35bab85900cd6c09eaddc1b1a9541bbc0bbcc3,A survey of open source tools for machine learning with big data in the Hadoop ecosystem,,1-36,2015.0,http://french.net/list/list/tagsprivacy.asp,Computer Science
377,be2745d3a246ce9ae19972bc033aeb184fe0974a,Big Data in food and agriculture,"Farming is undergoing a digital revolution. Our existing review of current Big Data applications in the agri-food sector has revealed several collection and analytics tools that may have implications for relationships of power between players in the food system (e.g. between farmers and large corporations). For example, Who retains ownership of the data generated by applications like Monsanto Corproation's Weed I.D. “app”? Are there privacy implications with the data gathered by John Deere's precision agricultural equipment? Systematically tracing the digital revolution in agriculture, and charting the affordances as well as the limitations of Big Data applied to food and agriculture, should be a broad research goal for Big Data scholarship. Such a goal brings data scholarship into conversation with food studies and it allows for a focus on the material consequences of big data in society.",93-111,2016.0,https://cruz-clark.org/blog/app/wp-contentregister.html,Computer Science
378,62d3d79cf1845bac650167d300dc696b4eec4214,The Age of Big Data,"The Workshop was hosted by The Law and Technology Centre of the Faculty of Law, The University of Hong Kong",20-137,2015.0,https://russell.biz/list/searchauthor.php,Engineering
379,e5d9809beb3ba4887585fccc6283521f43fb21d6,In-Memory Big Data Management and Processing: A Survey,"Growing main memory capacity has fueled the development of in-memory big data management and processing. By eliminating disk I/O bottleneck, it is now possible to support interactive data analytics. However, in-memory systems are much more sensitive to other sources of overhead that do not matter in traditional I/O-bounded disk-based systems. Some issues such as fault-tolerance and consistency are also more challenging to handle in in-memory environment. We are witnessing a revolution in the design of database systems that exploits main memory as its data storage layer. Many of these researches have focused along several dimensions: modern CPU and memory hierarchy utilization, time/space efficiency, parallelism, and concurrency control. In this survey, we aim to provide a thorough review of a wide range of in-memory data management and processing proposals and systems, including both data storage systems and data processing frameworks. We also give a comprehensive presentation of important technology in memory management, and some key factors that need to be considered in order to achieve efficient in-memory data management and processing.",1920-1948,2015.0,http://jackson-henderson.com/list/search/listmain.htm,Computer Science
380,1c45bfad2900f553364b9f877ca31e1ab3a310a1,Big Data in product lifecycle management,,667 - 684,2015.0,https://www.martin.org/searchsearch.html,Engineering
381,c91a589e6c222051bbd37020c0bd4d7698f87cc8,Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges,,544-549,2017.0,http://www.anderson-bautista.net/listregister.html,Engineering
382,d094f0faff376af9a0ee79a742b350531b3c89bf,Exascale computing and big data,Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.,56 - 68,2015.0,http://www.beasley.com/searchhome.jsp,Computer Science
383,766c86e927fab2e44926e7ec0044f4cba1bdb905,Bayes and big data: the consensus Monte Carlo algorithm,"A useful definition of ‘big data’ is data that is too big to process comfortably on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated), so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine, and then averaging individual Monte Carlo draws across machines. Depending on the model, the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single-machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available, for large single-layer hierarchical models, and for Bayesian additive regression trees (BART).",78 - 88,2016.0,http://www.myers.com/categoryabout.htm,Computer Science
384,211fa2a12c8d55053fbac1f75f41e8d7aaa79f83,Big Data in Accounting: An Overview,"SYNOPSIS: This paper discusses an overall framework of Big Data in accounting, setting the stage for the ensuing collection of essays that presents the ongoing evolution of corporate data into Big ...",381-396,2015.0,https://www.palmer.biz/posts/categoriesabout.htm,Business
385,2614a63a84e83cc8a7f906c08f219d52b158f92f,Big Data in Healthcare,,24-135,2017.0,https://gutierrez-beasley.com/categorycategory.html,Computer Science
386,ce1b0553c09b725f6fd1fc0b39f2dc7c428d3088,What is big data? A consensual definition and a review of key research topics,"Although Big Data is a trending buzzword in both academia and the industry, its meaning is still shrouded by much conceptual vagueness. The term is used to describe a wide range of concepts: from the technological ability to store, aggregate, and process data, to the cultural shift that is pervasively invading business and society, both drowning in information overload. The lack of a formal definition has led research to evolve into multiple and inconsistent paths. Furthermore, the existing ambiguity among researchers and practitioners undermines an efficient development of the subject. In this paper we have reviewed the existing literature on Big Data and analyzed its previous definitions in order to pursue two results: first, to provide a summary of the key research areas related to the phenomenon, identifying emerging trends and suggesting opportunities for future development; second, to provide a consensual definition for Big Data, by synthesizing common themes of existing works and patterns in previous definitions.",17-123,2015.0,https://brown-martin.com/tag/list/exploreregister.html,Engineering
387,b169080ee7ada80cd80bc79974f45b26ef9ed1a4,Accelerated PSO Swarm Search Feature Selection for Data Stream Mining Big Data,"Big Data though it is a hype up-springing many technical challenges that confront both academic research communities and commercial IT deployment, the root sources of Big Data are founded on data streams and the curse of dimensionality. It is generally known that data which are sourced from data streams accumulate continuously making traditional batch-based model induction algorithms infeasible for real-time data mining. Feature selection has been popularly used to lighten the processing load in inducing a data mining model. However, when it comes to mining over high dimensional data the search space from which an optimal feature subset is derived grows exponentially in size, leading to an intractable demand in computation. In order to tackle this problem which is mainly based on the high-dimensionality and streaming format of data feeds in Big Data, a novel lightweight feature selection is proposed. The feature selection is designed particularly for mining streaming data on the fly, by using accelerated particle swarm optimization (APSO) type of swarm search that achieves enhanced analytical accuracy within reasonable processing time. In this paper, a collection of Big Data with exceptionally large degree of dimensionality are put under test of our new feature selection algorithm for performance evaluation.",33-45,2016.0,http://www.bradley.info/posts/exploreabout.jsp,Computer Science
388,0faf86b14b22f6714d3ad524010d1129c364e4be,"Neuroscience: Big brain, big data",,559-561,2017.0,http://www.mueller-smith.org/wp-content/tags/tagshome.html,Medicine
389,504bcd9e5e67d5f258fccca9303b2ff54e274c52,Geospatial Big Data Handling Theory and Methods: A Review and Research Challenges,"Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future. Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling, Review",92-114,2015.0,https://www.ross.biz/search/main/wp-contentprivacy.html,Physics
390,c5b66bca85209e83f7f5de333938874a3dd999f1,Mobile Cloud Computing Model and Big Data Analysis for Healthcare Applications,"Mobile devices are increasingly becoming an indispensable part of people's daily life, facilitating to perform a variety of useful tasks. Mobile cloud computing integrates mobile and cloud computing to expand their capabilities and benefits and overcomes their limitations, such as limited memory, CPU power, and battery life. Big data analytics technologies enable extracting value from data having four Vs: volume, variety, velocity, and veracity. This paper discusses networked healthcare and the role of mobile cloud computing and big data analytics in its enablement. The motivation and development of networked healthcare applications and systems is presented along with the adoption of cloud computing in healthcare. A cloudlet-based mobile cloud-computing infrastructure to be used for healthcare big data applications is described. The techniques, tools, and applications of big data analytics are reviewed. Conclusions are drawn concerning the design of networked healthcare systems using big data and mobile cloud-computing technologies. An outlook on networked healthcare is given.",6171-6180,2016.0,https://www.blevins.com/mainprivacy.html,Computer Science
391,4b2c57cf9516a1d9eb98877627697c288227d9fb,How Can SMEs Benefit from Big Data? Challenges and a Path Forward,"Big data is big news, and large companies in all sectors are making significant advances in their customer relations, product selection and development and consequent profitability through using this valuable commodity. Small and medium enterprises (SMEs) have proved themselves to be slow adopters of the new technology of big data analytics and are in danger of being left behind. In Europe, SMEs are a vital part of the economy, and the challenges they encounter need to be addressed as a matter of urgency. This paper identifies barriers to SME uptake of big data analytics and recognises their complex challenge to all stakeholders, including national and international policy makers, IT, business management and data science communities.",2151 - 2164,2016.0,https://www.silva.com/search/blogabout.htm,Computer Science
392,d74624e8a5c8e8227615895c40e2b3ea8367de27,"Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls","
 
 Large-scale databases of human activity in social media have captured scientific and policy attention, producing a flood of research and discussion. This paper considers methodological and conceptual challenges for this emergent field, with special attention to the validity and representativeness of social media big data analyses. Persistent issues include the over-emphasis of a single platform, Twitter, sampling biases arising from selection by hashtags, and vague and unrepresentative sampling frames. The socio-cultural complexity of user behavior aimed at algorithmic invisibility (such as subtweeting, mock-retweeting, use of “screen captures” for text, etc.) further complicate interpretation of big data social media. Other challenges include accounting for field effects, i.e. broadly consequential events that do not diffuse only through the network under study but affect the whole society. The application of network methods from other fields to the study of human social activity may not always be appropriate. The paper concludes with a call to action on practical steps to improve our analytic capacity in this promising, rapidly-growing field.
 
",67-134,2014.0,http://www.garcia.biz/appcategory.jsp,Computer Science
393,f4900599936427fd1c7714cae15ff00590396a7c,Big Data Meet Green Challenges: Greening Big Data,"Nowadays, there are two significant tendencies, how to process the enormous amount of data, big data, and how to deal with the green issues related to sustainability and environmental concerns. An interesting question is whether there are inherent correlations between the two tendencies in general. To answer this question, this paper firstly makes a comprehensive literature survey on how to green big data systems in terms of the whole life cycle of big data processing, and then this paper studies the relevance between big data and green metrics and proposes two new metrics, effective energy efficiency and effective resource efficiency in order to bring new views and potentials of green metrics for the future times of big data.",873-887,2016.0,https://www.irwin.com/category/list/apppost.html,Engineering
394,fa87557e2d9a9695a99ec481673fb360fad2a897,"Big data: a revolution that will transform how we live, work, and think","Howard, P. (2006). New media and the managed citizen. New York: Cambridge University Press. Lathrop, D., & Ruma, L. (Eds.). (2010). Open government: Collaboration, transparency, and participation in practice. Sebastopol, CA: O’Reilly. Noveck, B. S. (2008). Wiki-Government: How open-source technology can make government decisionmaking more expert and more democratic. Washington, DC: Brookings Institution Press.",1300 - 1302,2014.0,https://ramirez-ross.org/blog/tagsfaq.html,Computer Science
395,64a78fb55ac928fc8a902640a8cad0618f42fcee,How Big Data Will Change Accounting,"SYNOPSIS: Big Data will have increasingly important implications for accounting, even as new types of data become accessible. The video, audio, and textual information made available via Big Data can provide for improved managerial accounting, financial accounting, and financial reporting practices. In managerial accounting, Big Data will contribute to the development and evolution of effective management control systems and budgeting processes. In financial accounting, Big Data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision making. In reporting, Big Data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.",397-407,2015.0,https://www.cole.info/tags/tags/categorysearch.asp,Business
396,adf0daa3c4f5e21de8e98ce86dbc6b3d7d299a2d,Towards cloud based big data analytics for smart future cities,,55-126,2015.0,https://jones.biz/wp-content/blogfaq.jsp,Computer Science
397,4a32712674651fd200e1de8e41e9685f21a163b4,The Ethics of Big Data in Big Agriculture,"This paper examines the ethics of big data in agriculture, focusing on the power asymmetry between farmers and large agribusinesses like Monsanto. Following the recent purchase of Climate Corp., Monsanto is currently the most prominent biotech agribusiness to buy into big data. With wireless sensors on tractors monitoring or dictating every decision a farmer makes, Monsanto can now aggregate large quantities of previously proprietary farming data, enabling a privileged position with unique insights on a field-by-field basis into a third or more of the US farmland. This power asymmetry may be rebalanced through open-sourced data, and publicly-funded data analytic tools which rival Climate Corp. in complexity and innovation for use in the public domain.",88-110,2016.0,http://www.arias.com/postsauthor.html,Engineering
398,a5ba556a23c57d1e8b57a5ee0e3e01736e06e646,Crime Rate Inference with Big Data,"Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",25-137,2016.0,http://www.whitaker.com/tagssearch.html,Computer Science
399,75275edbf8655b404665f176c5c5f76c9a2db8f9,How to Use Big Data to Drive Your Supply Chain,"Big data analytics has become an imperative for business leaders across every industry sector. Analytics applications that can deliver a competitive advantage appear all along the supply chain decision spectrum—from targeted location-based marketing to optimizing supply chain inventories to enabling supplier risk assessment. While many companies have used it to extract new insights and create new forms of value, other companies have yet to leverage big data to transform their supply chain operations. This article examines how leading companies use big data analytics to drive their supply chains and offers a framework for implementation based on lessons learned.",26 - 48,2016.0,https://www.walker-mendoza.com/tagterms.htm,Business
400,a5ba556a23c57d1e8b57a5ee0e3e01736e06e646,Crime Rate Inference with Big Data,"Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",25-135,2016.0,https://villanueva.com/blog/exploreauthor.asp,Computer Science
401,b91334db900b1c066ed5c811c9e5eea57a7e7b08,Efficient kNN classification algorithm for big data,,143-148,2016.0,https://bass-williams.com/app/explore/categoryhome.asp,Computer Science
402,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",80-90,2014.0,http://torres.net/search/tag/postscategory.htm,Computer Science
403,910d0314facfc9d5874a12957d6c3cea97dc159f,Big Data Systems Meet Machine Learning Challenges: Towards Big Data Science as a Service,,1-11,2017.0,http://moreno-kennedy.com/explore/app/tagscategory.htm,Computer Science
404,1c2122e6e140301f5d9e56f8bae476105bc01fcb,Highly efficient data migration and backup for big data applications in elastic optical inter-data-center networks,"This article discusses the technologies for realizing highly efficient data migration and backup for big data applications in elastic optical inter-data-center (inter-DC) networks. We first describe the impacts of big data applications on underlying network infrastructure and introduce the concept of flexible-grid elastic optical inter-DC networks. Then we model the data migration in such networks as dynamic anycast and propose several efficient algorithms. Joint resource defragmentation is also discussed to further improve network performance. For efficient data backup, we leverage a mutual backup model and investigate how to avoid the prolonged negative impacts on DCs' normal operation by minimizing the DC backup window.",36-42,2015.0,http://www.carr.biz/app/category/tagsabout.php,Computer Science
405,5f1cc2df59fbab055ce1ea64d667e0934e6baad3,Big Data for Health,"This paper provides an overview of recent developments in big data in the context of biomedical and health informatics. It outlines the key characteristics of big data and how medical and health informatics, translational bioinformatics, sensor informatics, and imaging informatics will benefit from an integrated approach of piecing together different aspects of personalized information from a diverse range of data sources, both structured and unstructured, covering genomics, proteomics, metabolomics, as well as imaging, clinical diagnosis, and long-term continuous physiological sensing of an individual. It is expected that recent advances in big data will expand our knowledge for testing new hypotheses about disease management from diagnosis to prevention to personalized treatment. The rise of big data, however, also raises challenges in terms of privacy, security, data ownership, data stewardship, and governance. This paper discusses some of the existing activities and future opportunities related to big data for health, outlining some of the key underlying issues that need to be tackled.",1193-1208,2015.0,https://mills.net/tags/exploreindex.php,Medicine
406,c1d4584096390eb23e7daadfa4819421c738bacd,"Big data, big risks","The ‘big data’ literature, academic as well as professional, has a very strong focus on opportunities. Far less attention has been paid to the threats that arise from repurposing data, consolidating data from multiple sources, applying analytical tools to the resulting collections, drawing inferences, and acting on them. On the basis of a review of quality factors in ‘big data’ and ‘big data analytics’, illustrated by means of scenario analysis, this paper draws attention to the moral and legal responsibility of computing researchers and professionals to temper their excitement, and apply reality checks to their promotional activities.",77 - 90,2016.0,http://hill.com/blog/categories/mainpost.htm,Engineering
407,12f26e0c38cdf9b8df735a6359384529e533bddc,Creating Value with Big Data Analytics: Making Smarter Marketing Decisions,"Our newly digital world is generating an almost unimaginable amount of data about all of us. Such a vast amount of data is useless without plans and strategies that are designed to cope with its size and complexity, and which enable organisations to leverage the information to create value. This book is a refreshingly practical, yet theoretically sound roadmap to leveraging big data and analytics. Creating Value with Big Data Analytics provides a nuanced view of big data development, arguing that big data in itself is not a revolution but an evolution of the increasing availability of data that has been observed in recent times. Building on the authors extensive academic and practical knowledge, this book aims to provide managers and analysts withstrategic directions and practical analytical solutions on how to create value from existing and new big data. By tying data and analytics to specific goals and processes for implementation, this is a much-needed book that will be essential reading for students and specialists of data analytics, marketing research, and customer relationship management.",81-130,2016.0,http://lucas.com/main/posts/listprivacy.html,Computer Science
408,1b919ece23b4d72468bf3ea483ea436097502974,Big Data Analytics: Opportunity or Threat for the Accounting Profession?,"ABSTRACT Contrary to Frey and Osborne's (2013) prediction that the accounting profession faces extinction, we argue that accountants can still create value in a world of Big Data analytics. To advance this position, we provide a conceptual framework based on structured/unstructured data and problem-driven/exploratory analysis. We argue that accountants already excel at problem-driven analysis of structured data, are well positioned to play a leading role in the problem-driven analysis of unstructured data, and can support data scientists performing exploratory analysis on Big Data. Our argument rests on two pillars: accountants are familiar with structured datasets, easing the transition to working with unstructured data, and possess knowledge of business fundamentals. Thus, rather than replacing accountants, we argue that Big Data analytics complements accountants' skills and knowledge. However, educators, standard setters, and professional bodies must adjust their curricula, standards, and frameworks to...",39-140,2016.0,http://durham-salazar.com/explore/tagsprivacy.php,Sociology
409,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",82-133,2016.0,https://www.skinner-sherman.com/categories/bloghomepage.asp,Computer Science
410,48d9734e70303a6d1af1e7dd455798e29ef78a65,A global exploration of Big Data in the supply chain,"Purpose 
 
 
 
 
Journals in business logistics, operations management, supply chain management, and business strategy have initiated ongoing calls for Big Data research and its impact on research and practice. Currently, no extant research has defined the concept fully. The purpose of this paper is to develop an industry grounded definition of Big Data by canvassing supply chain managers across six nations. The supply chain setting defines Big Data as inclusive of four dimensions: volume, velocity, variety, and veracity. The study further extracts multiple concepts that are important to the future of supply chain relationship strategy and performance. These outcomes provide a starting point and extend a call for theoretically grounded and paradigm-breaking research on managing business-to-business relationships in the age of Big Data. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
A native categories qualitative method commonly employed in sociology allows each executive respondent to provide rich, specific data. This approach reduces interviewer bias while examining 27 companies across six industrialized and industrializing nations. This is the first study in supply chain management and logistics (SCMLs) to use the native category approach. 
 
 
 
 
Findings 
 
 
 
 
This study defines Big Data by developing four supporting dimensions that inform and ground future SCMLs research; details ten key success factors/issues; and discusses extensive opportunities for future research. 
 
 
 
 
Research limitations/implications 
 
 
 
 
This study provides a central grounding of the term, dimensions, and issues related to Big Data in supply chain research. 
 
 
 
 
Practical implications 
 
 
 
 
Supply chain managers are provided with a peer-specific definition and unified dimensions of Big Data. The authors detail key success factors for strategic consideration. Finally, this study notes differences in relational priorities concerning these success factors across different markets, and points to future complexity in managing supply chain and logistics relationships. 
 
 
 
 
Originality/value 
 
 
 
 
There is currently no central grounding of the term, dimensions, and issues related to Big Data in supply chain research. For the first time, the authors address subjects related to how supply chain partners employ Big Data across the supply chain, uncover Big Data’s potential to influence supply chain performance, and detail the obstacles to developing Big Data’s potential. In addition, the study introduces the native category qualitative interview approach to SCMLs researchers.",710-739,2016.0,http://mclaughlin.org/list/search/exploreprivacy.php,Business
411,1b5cc68e0498629bf3f57ec07abb3da69a4ce3a1,Environmental performance evaluation with big data: theories and methods,,459 - 472,2016.0,http://www.huff.biz/categories/listpost.html,Computer Science
412,cae80eca52369e50037cc761abc0c3d3395881ba,From big data analysis to personalized medicine for all: challenges and opportunities,,33-105,2015.0,http://silva.com/searchterms.htm,Computer Science
413,e3642ab645a3fdcce97f854503ba67f4b503b9ea,BigDataBench: A big data benchmark suite from internet services,"As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above. This paper presents our joint research efforts on this issue with several industrial partners. Our big data benchmark suite-BigDataBench not only covers broad application scenarios, but also includes diverse and representative data sets. Currently, we choose 19 big data benchmarks from dimensions of application scenarios, operations/ algorithms, data types, data sources, software stacks, and application types, and they are comprehensive for fairly measuring and evaluating big data systems and architecture. BigDataBench is publicly available from the project home page http://prof.ict.ac.cn/BigDataBench. Also, we comprehensively characterize 19 big data workloads included in BigDataBench with varying data inputs. On a typical state-of-practice processor, Intel Xeon E5645, we have the following observations: First, in comparison with the traditional benchmarks: including PARSEC, HPCC, and SPECCPU, big data applications have very low operation intensity, which measures the ratio of the total number of instructions divided by the total byte number of memory accesses; Second, the volume of data input has non-negligible impact on micro-architecture characteristics, which may impose challenges for simulation-based big data architecture research; Last but not least, corroborating the observations in CloudSuite and DCBench (which use smaller data inputs), we find that the numbers of L1 instruction cache (L1I) misses per 1000 instructions (in short, MPKI) of the big data applications are higher than in the traditional benchmarks; also, we find that L3 caches are effective for the big data applications, corroborating the observation in DCBench.",488-499,2014.0,https://bass.com/searchpost.jsp,Computer Science
414,6d6beed0fd1ddd3221b0f28bc34ebf2e8c7a4ad7,Big Data in Public Affairs,"This article offers an overview of the conceptual, substantive, and practical issues surrounding “big data” to provide one perspective on how the field of public affairs can successfully cope with the big data revolution. Big data in public affairs refers to a combination of administrative data collected through traditional means and large-scale data sets created by sensors, computer networks, or individuals as they use the Internet. In public affairs, new opportunities for real-time insights into behavioral patterns are emerging but are bound by safeguards limiting government reach through the restriction of the collection and analysis of these data. To address both the opportunities and challenges of this emerging phenomenon, the authors first review the evolving canon of big data articles across related fields. Second, they derive a working definition of big data in public affairs. Third, they review the methodological and analytic challenges of using big data in public affairs scholarship and practice. The article concludes with implications for public affairs.",928-937,2016.0,http://perez.com/tagfaq.htm,Political Science
415,1a1f3d8045c1efbaefecc30c89035705ec10ba73,Big data for development: applications and techniques,,1-24,2016.0,https://www.neal.com/app/categories/mainhome.html,Computer Science
416,ff42d3ea6285282149b8aaa86665b0d70840112b,"The Big Data Value Chain: Definitions, Concepts, and Theoretical Approaches",,29-37,2016.0,http://greene.org/app/searchauthor.html,Computer Science
417,dfbfa43fa090fdd21e4b8e694f61136ebeeaab93,"Smart Cities: Big Data, Civic Hackers, and the Quest for a New Utopia","We live in a world defined by urbanization and digital ubiquity, where mobile broadband connections outnumber fixed ones, machines dominate a new ""internet of things,"" and more people live in cities than in the countryside. In Smart Cities, urbanist and technology expert Anthony Townsend takes a broad historical look at the forces that have shaped the planning and design of cities and information technologies from the rise of the great industrial cities of the nineteenth century to the present. A century ago, the telegraph and the mechanical tabulator were used to tame cities of millions. Today, cellular networks and cloud computing tie together the complex choreography of mega-regions of tens of millions of people. In response, cities worldwide are deploying technology to address both the timeless challenges of government and the mounting problems posed by human settlements of previously unimaginable size and complexity. In Chicago, GPS sensors on snow plows feed a real-time ""plow tracker"" map that everyone can access. In Zaragoza, Spain, a ""citizen card"" can get you on the free city-wide Wi-Fi network, unlock a bike share, check a book out of the library, and pay for your bus ride home. In New York, a guerrilla group of citizen-scientists installed sensors in local sewers to alert you when stormwater runoff overwhelms the system, dumping waste into local waterways. As technology barons, entrepreneurs, mayors, and an emerging vanguard of civic hackers are trying to shape this new frontier, Smart Cities considers the motivations, aspirations, and shortcomings of them all while offering a new civics to guide our efforts as we build the future together, one click at a time.",74-148,2013.0,http://www.willis-russell.com/searchlogin.php,Engineering
418,f3146bc0967971dacf13e95db383c4ca09d8073f,How organisations leverage Big Data: a maturity model,"Purpose 
 
 
 
 
While it is commonly recognised that Big Data have an immense potential to generate value for business organisations, appropriating value from Big Data and, in particular, Big Data-enabled analytics is still an open issue for many organisations. The purpose of this paper is to develop a maturity model to support organisations in the realisation of the value created by Big Data. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The maturity model is developed following a qualitative approach based on literature analysis and semi-structured interviews with domain experts. The completeness and usefulness of the model is evaluated qualitatively by practitioners, whereas the applicability of the model is evaluated by Big Data maturity assessments in three real-world organisations. 
 
 
 
 
Findings 
 
 
 
 
The proposed maturity model is considered exhaustive by domain experts and has helped the three assessed organisations to develop a more critical understanding of the next steps to take. 
 
 
 
 
Originality/value 
 
 
 
 
The maturity model integrates existing industry-developed maturity models into one single coherent Big Data maturity model. The proposed model answers the call for research on Big Data to abstract from technical issues to focus on the business implications of Big Data initiatives.",1468-1492,2016.0,https://www.walters.com/wp-contentcategory.html,Engineering
419,9e7be12082f58cbf7ebdb84a8cbdc897a4e41683,The Deluge of Spurious Correlations in Big Data,,595 - 612,2016.0,https://www.vargas.com/tags/wp-content/mainhome.htm,Computer Science
420,c5dae4440044b015fd4ae8fd59aba43d7515c889,Big Data Analytics for Earth Sciences: the EarthServer approach,"Big Data Analytics is an emerging field since massive storage and computing capabilities have been made available by advanced e-infrastructures. Earth and Environmental sciences are likely to benefit from Big Data Analytics techniques supporting the processing of the large number of Earth Observation datasets currently acquired and generated through observations and simulations. However, Earth Science data and applications present specificities in terms of relevance of the geospatial information, wide heterogeneity of data models and formats, and complexity of processing. Therefore, Big Earth Data Analytics requires specifically tailored techniques and tools. The EarthServer Big Earth Data Analytics engine offers a solution for coverage-type datasets, built around a high performance array database technology, and the adoption and enhancement of standards for service interaction (OGC WCS and WCPS). The EarthServer solution, led by the collection of requirements from scientific communities and international initiatives, provides a holistic approach that ranges from query languages and scalability up to mobile access and visualization. The result is demonstrated and validated through the development of lighthouse applications in the Marine, Geology, Atmospheric, Planetary and Cryospheric science domains.",29 - 3,2016.0,http://www.cline-robinson.biz/categories/mainpost.asp,Computer Science
421,41a44da00b2e3de7a501ebe0782514b6b4afe5f9,Deduplication on Encrypted Big Data in Cloud,"Cloud computing offers a new way of service provision by re-arranging various resources over the Internet. The most important and popular cloud service is data storage. In order to preserve the privacy of data holders, data are often stored in cloud in an encrypted form. However, encrypted data introduce new challenges for cloud data deduplication, which becomes crucial for big data storage and processing in cloud. Traditional deduplication schemes cannot work on encrypted data. Existing solutions of encrypted data deduplication suffer from security weakness. They cannot flexibly support data access control and revocation. Therefore, few of them can be readily deployed in practice. In this paper, we propose a scheme to deduplicate encrypted data stored in cloud based on ownership challenge and proxy re-encryption. It integrates cloud data deduplication with access control. We evaluate its performance based on extensive analysis and computer simulations. The results show the superior efficiency and effectiveness of the scheme for potential practical deployment, especially for big data deduplication in cloud storage.",138-150,2016.0,https://goodman.com/exploreauthor.php,Computer Science
422,7ee8e3a67cf299276611cea6d8aa3e657177bd19,Big Data and consumer behavior: imminent opportunities,"Purpose – The purpose of this paper is to assess how the study of consumer behavior can benefit from the presence of Big Data. Design/methodology/approach – This paper offers a conceptual overview of potential opportunities and changes to the study of consumer behavior that Big Data will likely bring. Findings – Big Data have the potential to further our understanding of each stage in the consumer decision-making process. While the field has traditionally moved forward using a priori theory followed by experimentation, it now seems that the nature of the feedback loop between theory and results may shift under the weight of Big Data. Research limitations/implications – A new data culture is now represented in marketing practice. The new group advocates inductive data mining and A/B testing rather than human intuition harnessed for deduction. The group brings with it interest in numerous secondary data sources. However, Big Data may be limited by poor quality, unrepresentativeness and volatility, among oth...",89-97,2016.0,https://www.donovan-nash.com/poststerms.html,Computer Science
423,4dfddbf5b6b0815414313039d29f41900f47d003,"The Data Revolution. Big Data, Open Data, Data Infrastructures and Their Consequences","The last few years have witnessed an increasing production of data that have become open, accessible and available at low cost. Although many disciplines are already using ‘big data’ as instrument of analysis, social sciences have apparently missed the opportunity to exploit their potentialities fully. The purpose of this excellent book is to prove how these data do not exist independently from the ideas, techniques, technologies, people and context that produce, process, manage, analyze and store them. Moreover, the author explores the definition, characteristics and the technique to manage big data, but he also focuses his attention on the challenges of this way of thinking and on how big data are changing existing epistemology and science. Before the big data revolution, the scientific approach was based on computational science based on the simulation of complex phenomena. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining was used. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining is used. The author focuses his attention on how big data are changing the approaches and methodologies in four different fields, that is, governing people, managing organizations, leveraging value and producing capitals, creating better places in which to live. The aim of the book is threefold: to provide a detailed reflection on the nature of the data and their wider assemblages; to chart how these assemblages are shifting and mutating all along the development of new data infrastructures; and to reflect on the consequences that these new ways to assemble data may entail the making of sense and on the effects they produce in the world. The 11 chapters ideally can be divided into two main sections. The first section (chapters 1–6) deals with the big data characteristics and the techniques to manage them. The last section (chapters 7–11) consider how big data are changing the epistemology of science across all domains (arts and humanities, social and life sciences, engineering). The interest of these last chapters lies in the core idea of data being not self-meaningful, as their meaningfulness is proportionate to the information they can provide. This is particularly interesting as it fosters dense insights and ideas on further development in research. The book starts with the definition of big data and enhances the concept by which data do not exist independently from the ideas, instruments, practices, context and knowledge used to generate, process, analyze and draw conclusions from them. The book continues with an analysis of the data characteristics. Data vary by forms (qualitative and quantitative), structure (structured, semi-structured and unstructured), source (captured, derived, exhaust, transient), producer (primary, secondary, tertiary), and type (indexical, attribute, metadata). However, these different types of data share the same characteristic as they all form the basis of the knowledge pyramid where data precede information which, in turn, precedes knowledge. The latter precedes understanding and wisdom. In order to make sense of data, they are usually pooled into datasets and databases designed and organized to enable specific analysis. How they are structured has consequences on the queries and obtainable results. The author underlines the importance of the data assembly process as an issue that needs further attention and research. Chapter 4 explores big data characteristics, that is, volume, velocity, variety, exhaustivity, resolution/indexicality, relationality and flexibility/scalability. The author then examines the interest that the access to large data with those specific characteristics may have for society, governments and business organizations. Chapter 5 concerns the sources of big data. The starting point is that the production of big data has been facilitated by the confluence of five technological innovations: growing computational power, internet, pervasive and ubiquitous computing, indexical and machine readable identification, and massive distributed storage. The data production can be divided into three categories, that is, directed data (generated by traditional forms of surveillance), automated data (generated by automatic function of the device or systems) and volunteered data (traded or gifted by people to a system). Once again, with his critical approach, the author underlines the importance of developing empirical studies to examine in depth the various ways in which big data are being generated, Regional Studies, 2016",553 - 554,2016.0,http://www.smith.com/app/tag/postshomepage.php,Computer Science
424,fa445325cf2edd5a03f45d15d67d24e4af5d0ab8,Challenges of Feature Selection for Big Data Analytics,"We're surrounded by huge amounts of large-scale high-dimensional data, but learning tasks require reduced data dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive models, improving learning performance, and preparing clean, understandable data. Some unique characteristics of big data such as data velocity and data variety have presented challenges to the feature selection problem. In this article, the authors envision these challenges for big data analytics. To facilitate and promote feature selection research, they present an open source feature selection repository (scikit-feature) of popular algorithms.",9-15,2016.0,http://www.palmer-bell.com/app/mainsearch.htm,Computer Science
425,fe74f9cc0d3b7b8c23e907b75a818e0346490f52,Digital Humanitarians: How Big Data Is Changing the Face of Humanitarian Response,"The overflow of information generated during disasters can be as paralyzing to humanitarian response as the lack of information. Making sense of this information--Big Data--is proving an impossible challenge for traditional humanitarian organizations, which is precisely why they're turning to Digital Humanitarians. This new humanitarians mobilize online to make sense of vast volumes of data--social media and text messages; satellite and aerial imagery--in direct support of relief efforts worldwide. How? They craft ingenious crowdsourcing solutions with trail-blazing insights from artificial intelligence. This book charts the spectacular rise of Digital Humanitarians, highlighting how their humanity coupled with innovative Big Data solutions is changing humanitarian relief for forever. Praise for the book: ...examines how new uses of technology and vast quantities of digital data are transforming the way societies prepare for, respond to, cope with, and ultimately understand humanitarian disasters. --Dr. Enzo Bollettino, Executive Director, The Harvard Humanitarian Initiative, Harvard University ...explains the strengths and potential weaknesses of using big data and crowdsourced analytics in crisis situations. It is at once a deeply personal and intellectually satisfying book.--Professor Steven Livingston, Professor of Media & Public and International Affairs, Elliott School of International Affairs, George Washington University",49-104,2015.0,https://www.thompson.net/postsmain.php,Engineering
426,ce2bbc757a4009d83d4a1719925fc64caf890e23,Learning Spark: Lightning-Fast Big Data Analytics,"The Web is getting faster, and the data it delivers is getting bigger. How can you handle everything efficiently? This book introduces Spark, an open source cluster computing system that makes data analytics fast to run and fast to write. Youll learn how to run programs faster, using primitives for in-memory cluster computing. With Spark, your job can load data into memory and query it repeatedly much quicker than with disk-based systems like Hadoop MapReduce. Written by the developers of Spark, this book will have you up and running in no time. Youll learn how to express MapReduce jobs with just a few simple lines of Spark code, instead of spending extra time and effort working with Hadoops raw Java API. Quickly dive into Spark capabilities such as collect, count, reduce, and save Use one programming paradigm instead of mixing and matching tools such as Hive, Hadoop, Mahout, and S4/Storm Learn how to run interactive, iterative, and incremental analyses Integrate with Scala to manipulate distributed datasets like local collections Tackle partitioning issues, data locality, default hash partitioning, user-defined partitioners, and custom serialization Use other languages by means of pipe() to achieve the equivalent of Hadoop streaming",72-118,2015.0,https://freeman-allen.com/tag/tag/wp-contentterms.jsp,Computer Science
427,0d8426ba72cb872062b1a87140f8cb2c2324ba65,Big Data Analytics in Mobile Cellular Networks,"Mobile cellular networks have become both the generators and carriers of massive data. Big data analytics can improve the performance of mobile cellular networks and maximize the revenue of operators. In this paper, we introduce a unified data model based on the random matrix theory and machine learning. Then, we present an architectural framework for applying the big data analytics in the mobile cellular networks. Moreover, we describe several illustrative examples, including big signaling data, big traffic data, big location data, big radio waveforms data, and big heterogeneous data, in mobile cellular networks. Finally, we discuss a number of open research challenges of the big data analytics in the mobile cellular networks.",1985-1996,2016.0,https://wright.com/search/posts/tagssearch.htm,Computer Science
428,92066c26f5c618b58a854b7bd3185b3addc00021,Computational Health Informatics in the Big Data Age,"The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.",1 - 36,2016.0,https://www.williams.com/wp-contentterms.html,Computer Science
429,6d379fe4781c77b949fe02c8de78386e93b5e3de,Questioning Big Data: Crowdsourcing crisis data towards an inclusive humanitarian response,"The aim of this paper is to critically explore whether crowdsourced Big Data enables an inclusive humanitarian response at times of crisis. We argue that all data, including Big Data, are socially constructed artefacts that reflect the contexts and processes of their creation. To support our argument, we qualitatively analysed the process of ‘Big Data making’ that occurred by way of crowdsourcing through open data platforms, in the context of two specific humanitarian crises, namely the 2010 earthquake in Haiti and the 2015 earthquake in Nepal. We show that the process of creating Big Data from local and global sources of knowledge entails the transformation of information as it moves from one distinct group of contributors to the next. The implication of this transformation is that locally based, affected people and often the original ‘crowd’ are excluded from the information flow, and from the interpretation process of crowdsourced crisis knowledge, as used by formal responding organizations, and are marginalized in their ability to benefit from Big Data in support of their own means. Our paper contributes a critical perspective to the debate on participatory Big Data, by explaining the process of in and exclusion during data making, towards more responsive humanitarian relief.",98-146,2016.0,https://fischer.com/posts/tags/tagsprivacy.html,Computer Science
430,c10b6ba92e63d9056a81c7bff5f499536e9ea73f,Machine Learning Models and Algorithms for Big Data Classification,,75-148,2016.0,http://www.rosario.com/wp-content/categories/wp-contentmain.html,Computer Science
431,f96efbad191c388706ce370dd9be5d2c25a7b3dc,Biscuit: A Framework for Near-Data Processing of Big Data Workloads,"Data-intensive queries are common in business intelligence, data warehousing and analytics applications. Typically, processing a query involves full inspection of large in-storage data sets by CPUs. An intuitive way to speed up such queries is to reduce the volume of data transferred over the storage network to a host system. This can be achieved by filtering out extraneous data within the storage, motivating a form of near-data processing. This work presents Biscuit, a novel near-data processing framework designed for modern solid-state drives. It allows programmers to write a data-intensive application to run on the host system and the storage system in a distributed, yet seamless manner. In order to offer a high-level programming model, Biscuit builds on the concept of data flow. Data processing tasks communicate through typed and data-ordered ports. Biscuit does not distinguish tasks that run on the host system and the storage system. As the result, Biscuit has desirable traits like generality and expressiveness, while promoting code reuse and naturally exposing concurrency. We implement Biscuit on a host system that runs the Linux OS and a high-performance solid-state drive. We demonstrate the effectiveness of our approach and implementation with experimental results. When data filtering is done by hardware in the solid-state drive, the average speed-up obtained for the top five queries of TPC-H is over 15x.",153-165,2016.0,http://mooney.com/app/searchhome.php,Computer Science
432,2a6a9c939c2abecf4c7acb7d9e439f0e53e5f399,Industrial Big Data as a Result of IoT Adoption in Manufacturing,,290-295,2016.0,https://www.madden.com/wp-content/wp-content/postsabout.htm,Engineering
433,d07e640a48da232cd3a49c039273f7e91a0a9b20,"Big data, bigger dilemmas: A critical review","The recent interest in Big Data has generated a broad range of new academic, corporate, and policy practices along with an evolving debate among its proponents, detractors, and skeptics. While the practices draw on a common set of tools, techniques, and technologies, most contributions to the debate come either from a particular disciplinary perspective or with a focus on a domain‐specific issue. A close examination of these contributions reveals a set of common problematics that arise in various guises and in different places. It also demonstrates the need for a critical synthesis of the conceptual and practical dilemmas surrounding Big Data. The purpose of this article is to provide such a synthesis by drawing on relevant writings in the sciences, humanities, policy, and trade literature. In bringing these diverse literatures together, we aim to shed light on the common underlying issues that concern and affect all of these areas. By contextualizing the phenomenon of Big Data within larger socioeconomic developments, we also seek to provide a broader understanding of its drivers, barriers, and challenges. This approach allows us to identify attributes of Big Data that require more attention—autonomy, opacity, generativity, disparity, and futurity—leading to questions and ideas for moving beyond dilemmas.",61-147,2015.0,https://gomez.com/category/wp-contentprivacy.html,Computer Science
434,6aec26e2bc3423c71375956bcd2de6b064ebacc0,A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities,"The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.",28:1-28:6,2015.0,http://hall-weaver.info/categories/postsmain.html,Engineering
435,774170fb9eef527c2f32c865fe0516d5c000c440,Energy big data: A survey,"As a significant application of energy, smart grid is a complicated interconnected power grid that involves sensors, deployment strategies, smart meters, and real-time data processing. It continuously generates data with large volume, high velocity, and diverse variety. In this paper, we first give a brief introduction on big data, smart grid, and big data application in the smart grid scenario. Then, recent studies and developments are summarized in the context of integrated architecture and key enabling technologies. Meanwhile, security issues are specifically addressed. Finally, we introduce several typical big data applications and point out future challenges in the energy domain.",3844-3861,2016.0,http://www.taylor.com/list/maincategory.htm,Computer Science
436,6057eb2caf437b494731a4937cef01513d234500,Big Data for Infectious Disease Surveillance and Modeling.,"We devote a special issue of the Journal of Infectious Diseases to review the recent advances of big data in strengthening disease surveillance, monitoring medical adverse events, informing transmission models, and tracking patient sentiments and mobility. We consider a broad definition of big data for public health, one encompassing patient information gathered from high-volume electronic health records and participatory surveillance systems, as well as mining of digital traces such as social media, Internet searches, and cell-phone logs. We introduce nine independent contributions to this special issue and highlight several cross-cutting areas that require further research, including representativeness, biases, volatility, and validation, and the need for robust statistical and hypotheses-driven analyses. Overall, we are optimistic that the big-data revolution will vastly improve the granularity and timeliness of available epidemiological information, with hybrid systems augmenting rather than supplanting traditional surveillance systems, and better prospects for accurate infectious diseases models and forecasts.","
          S375-S379
        ",2016.0,https://www.howell.info/mainhomepage.php,Medicine
437,8f9b0fac9e81c89b72a22171d1a06247cb8efa79,Big Data for Social Transportation,"Big data for social transportation brings us unprecedented opportunities for resolving transportation problems for which traditional approaches are not competent and for building the next-generation intelligent transportation systems. Although social data have been applied for transportation analysis, there are still many challenges. First, social data evolve with time and contain abundant information, posing a crucial need for data collection and cleaning. Meanwhile, each type of data has specific advantages and limitations for social transportation, and one data type alone is not capable of describing the overall state of a transportation system. Systematic data fusing approaches or frameworks for combining social signal data with different features, structures, resolutions, and precision are needed. Second, data processing and mining techniques, such as natural language processing and analysis of streaming data, require further revolutions in effective utilization of real-time traffic information. Third, social data are connected to cyber and physical spaces. To address practical problems in social transportation, a suite of schemes are demanded for realizing big data in social transportation systems, such as crowdsourcing, visual analysis, and task-based services. In this paper, we overview data sources, analytical approaches, and application systems for social transportation, and we also suggest a few future research directions for this new social transportation field.",620-630,2016.0,https://ferguson.com/exploreindex.asp,Engineering
438,1992422fbb960bd3f1adcc0f4df77d78df5984b0,Medical Big Data: Neurological Diseases Diagnosis Through Medical Data Analysis,,54-64,2016.0,http://shaw.com/wp-contentfaq.jsp,Computer Science
439,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",58-136,2015.0,https://www.thompson.info/wp-content/app/wp-contentfaq.jsp,Computer Science
440,31485e1213dd886fa2b668eefcd9b13533d8a9fe,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",364 - 373,2016.0,http://mccoy.com/tags/postsmain.php,Computer Science
441,2a58d13b5934eadc7b4d46869d7593059922e2a1,Big data need big theory too,"The current interest in big data, machine learning and data analytics has generated the widespread impression that such methods are capable of solving most problems without the need for conventional scientific methods of inquiry. Interest in these methods is intensifying, accelerated by the ease with which digitized data can be acquired in virtually all fields of endeavour, from science, healthcare and cybersecurity to economics, social sciences and the humanities. In multiscale modelling, machine learning appears to provide a shortcut to reveal correlations of arbitrary complexity between processes at the atomic, molecular, meso- and macroscales. Here, we point out the weaknesses of pure big data approaches with particular focus on biology and medicine, which fail to provide conceptual accounts for the processes to which they are applied. No matter their ‘depth’ and the sophistication of data-driven methods, such as artificial neural nets, in the end they merely fit curves to existing data. Not only do these methods invariably require far larger quantities of data than anticipated by big data aficionados in order to produce statistically reliable results, but they can also fail in circumstances beyond the range of the data used to train them because they are not designed to model the structural characteristics of the underlying system. We argue that it is vital to use theory as a guide to experimental design for maximal efficiency of data collection and to produce reliable predictive models and conceptual knowledge. Rather than continuing to fund, pursue and promote ‘blind’ big data projects with massive budgets, we call for more funding to be allocated to the elucidation of the multiscale and stochastic processes controlling the behaviour of complex systems, including those of life, medicine and healthcare. This article is part of the themed issue ‘Multiscale modelling at the physics–chemistry–biology interface’.",65-105,2016.0,https://smith.com/list/exploremain.html,Medicine
442,6e42797e4ab69d56a0d6071cad7f18dd686431f4,Big Data Reduction Methods: A Survey,,265-284,2016.0,http://www.adams.com/category/tagscategory.jsp,Computer Science
443,aec372fd2af3de28e896165e04ec86a26d0ae61d,A Big Data Architecture Design for Smart Grids Based on Random Matrix Theory,"Model-based analysis tools, built on assumptions and simplifications, are difficult to handle smart grids with data characterized by volume, velocity, variety, and veracity (i.e., 4Vs data). This paper, using random matrix theory (RMT), motivates data-driven tools to perceive the complex grids in high-dimension; meanwhile, an architecture with detailed procedures is proposed. In algorithm perspective, the architecture performs a high-dimensional analysis and compares the findings with RMT predictions to conduct anomaly detections. Mean spectral radius (MSR), as a statistical indicator, is defined to reflect the correlations of system data in different dimensions. In management mode perspective, a group-work mode is discussed for smart grids operation. This mode breaks through regional limitations for energy flows and data flows, and makes advanced big data analyses possible. For a specific large-scale zone-dividing system with multiple connected utilities, each site, operating under the group-work mode, is able to work out the regional MSR only with its own measured/simulated data. The large-scale interconnected system, in this way, is naturally decoupled from statistical parameters perspective, rather than from engineering models perspective. Furthermore, a comparative analysis of these distributed MSRs, even with imperceptible different raw data, will produce a contour line to detect the event and locate the source. It demonstrates that the architecture is compatible with the block calculation only using the regional small database; beyond that, this architecture, as a data-driven solution, is sensitive to system situation awareness, and practical for real large-scale interconnected systems. Five case studies and their visualizations validate the designed architecture in various fields of power systems. To our best knowledge, this paper is the first attempt to apply big data technology into smart grids.",674-686,2015.0,https://www.moore.com/wp-content/tags/maincategory.html,Computer Science
444,dbfea34a4d2239f7595841f1162e0f37ed8bac97,Visualizing Big Data with augmented and virtual reality: challenges and research agenda,,1-27,2015.0,https://www.dodson.com/tags/posts/listabout.php,Computer Science
445,c3934f0d699e845fc9ee7816dfab7d91c72b33f8,"Big Data, Big Knowledge: Big Data for Personalized Healthcare","The idea that the purely phenomenological knowledge that we can extract by analyzing large amounts of data can be useful in healthcare seems to contradict the desire of VPH researchers to build detailed mechanistic models for individual patients. But in practice no model is ever entirely phenomenological or entirely mechanistic. We propose in this position paper that big data analytics can be successfully combined with VPH technologies to produce robust and effective in silico medicine solutions. In order to do this, big data technologies must be further developed to cope with some specific requirements that emerge from this application. Such requirements are: working with sensitive data; analytics of complex and heterogeneous data spaces, including nontextual information; distributed data management under security and performance constraints; specialized analytics to integrate bioinformatics and systems biology information with clinical observations at tissue, organ and organisms scales; and specialized analytics to define the “physiological envelope” during the daily life of each patient. These domain-specific requirements suggest a need for targeted funding, in which big data technologies for in silico medicine becomes the research priority.",1209-1215,2015.0,https://huynh.com/postshome.html,Computer Science
446,7f5e1750f753a3112dde2394d88db842d60f062d,Big Data and Big Cities: The Promises and Limitations of Improved Measures of Urban Life,"New, “big” data sources allow measurement of city characteristics and outcome variables higher frequencies and finer geographic scales than ever before. However, big data will not solve large urban social science questions on its own. Big data has the most value for the study of cities when it allows measurement of the previously opaque, or when it can be coupled with exogenous shocks to people or place. We describe a number of new urban data sources and illustrate how they can be used to improve the study and function of cities. We first show how Google Street View images can be used to predict income in New York City, suggesting that similar image data can be used to map wealth and poverty in previously unmeasured areas of the developing world. We then discuss how survey techniques can be improved to better measure willingness to pay for urban amenities. Finally, we explain how Internet data is being used to improve the quality of city services.",44-124,2015.0,http://www.reynolds.info/blog/categorieslogin.htm,Economics
447,837daca8463dc161bb23da089a0dbb0093dc5565,Big Data Analytics in Financial Statement Audits,"SYNOPSIS: Big Data analytics is the process of inspecting, cleaning, transforming, and modeling Big Data to discover and communicate useful information and patterns, suggest conclusions, and support decision making. Big Data has been used for advanced analytics in many domains but hardly, if at all, by auditors. This article hypothesizes that Big Data analytics can improve the efficiency and effectiveness of financial statement audits. We explain how Big Data analytics applied in other domains might be applied in auditing. We also discuss the characteristics of Big Data analytics, which set it apart from traditional auditing, and its implications for practical implementation.",423-429,2015.0,https://mason-rogers.com/wp-contentregister.html,Computer Science
448,c3aa26e6b39625ccc95b67df7cbe94cf32a0d1de,Wireless communications in the era of big data,"The rapidly growing wave of wireless data service is pushing against the boundary of our communication network's processing power. The pervasive and exponentially increasing data traffic present imminent challenges to all aspects of wireless system design, such as spectrum efficiency, computing capabilities, and fronthaul/backhaul link capacity. In this article, we discuss the challenges and opportunities in the design of scalable wireless systems to embrace the big data era. On one hand, we review the state-of-the-art networking architectures and signal processing techniques adaptable for managing big data traffic in wireless networks. On the other hand, instead of viewing mobile big data as an unwanted burden, we introduce methods to capitalize on the vast data traffic, for building a big-data-aware wireless network with better wireless service quality and new mobile applications. We highlight several promising future research directions for wireless communications in the mobile big data era.",190-199,2015.0,http://www.barber-burgess.info/category/postspost.php,Computer Science
449,ae5d6372cc21a2938b7602fb0f903eedd6d27eb4,Big Data and Cycling,"Abstract Big Data has begun to create significant impacts in urban and transport planning. This paper covers the explosion in data-driven research on cycling, most of which has occurred in the last ten years. We review the techniques, objectives and findings of a growing number of studies we have classified into three groups according to the nature of the data they are based on: GPS data (spatio-temporal data collected using the global positioning system (GPS)), live point data and journey data. We discuss the movement from small-scale GPS studies to the ‘Big GPS’ data sets held by fitness and leisure apps or specific cycling initiatives, the impact of Bike Share Programmes (BSP) on the availability of timely point data and the potential of historical journey data for trend analysis and pattern recognition. We conclude by pointing towards the possible new insights through combining these data sets with each other – and with more conventional health, socio-demographic or transport data.",114 - 133,2016.0,https://www.ortiz.com/blog/list/wp-contentauthor.php,Computer Science
450,aa740792d3d1afaa8acbdb2700582c2226986cd2,Data discretization: taxonomy and big data challenge,"Discretization of numerical data is one of the most influential data preprocessing tasks in knowledge discovery and data mining. The purpose of attribute discretization is to find concise data representations as categories which are adequate for the learning task retaining as much information in the original continuous attribute as possible. In this article, we present an updated overview of discretization techniques in conjunction with a complete taxonomy of the leading discretizers. Despite the great impact of discretization as data preprocessing technique, few elementary approaches have been developed in the literature for Big Data. The purpose of this article is twofold: a comprehensive taxonomy of discretization techniques to help the practitioners in the use of the algorithms is presented; the article aims is to demonstrate that standard discretization methods can be parallelized in Big Data platforms such as Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of one of the most well‐known discretizers based on Information Theory, obtaining better results than the one produced by: the entropy minimization discretizer proposed by Fayyad and Irani. Our scheme goes beyond a simple parallelization and it is intended to be the first to face the Big Data challenge. WIREs Data Mining Knowl Discov 2016, 6:5–21. doi: 10.1002/widm.1173",95-101,2016.0,http://patterson.com/category/tagregister.php,Computer Science
451,8cb6c2711afd3e504400ee12d3b582cc06348b08,Digital Data Streams: Creating Value from the Real-Time Flow of Big Data,"There is no escaping the Big Data hype. Vendors are peddling Big Data solutions; consulting firms employ Big Data specialists; Big Data conferences are aplenty. There is a rush to extract golden nuggets (of insight) from mountains (of data). By focusing merely on the mountain (of Big Data), these adventurers are overlooking the source of the revolution—namely, the many digital data streams (DDSs) that create Big Data—and the opportunity to improve real-time decision making. This article discusses the characteristics of DDSs, describes their common structure, and offers guidelines to enable firms to profit from their untapped potential.",25 - 5,2016.0,https://mayo-miller.com/search/categories/postsfaq.asp,Computer Science
452,f64c7a9a3be492f749dfe7ac3c2c8111ed5d0139,Big data in mobile social networks: a QoE-oriented framework,"Due to the rapid development of mobile social networks, mobile big data play an important role in providing mobile social users with various mobile services. However, as mobile big data have inherent properties, current MSNs face a challenge to provide mobile social user with a satisfactory quality of experience. Therefore, in this article, we propose a novel framework to deliver mobile big data over content- centric mobile social networks. At first, the characteristics and challenges of mobile big data are studied. Then the content-centric network architecture to deliver mobile big data in MSNs is presented, where each datum consists of interest packets and data packets, respectively. Next, how to select the agent node to forward interest packets and the relay node to transmit data packets are given by defining priorities of interest packets and data packets. Finally, simulation results show the performance of our framework with varied parameters.",52-57,2016.0,http://www.harris-powell.biz/listsearch.html,Computer Science
453,c660f1822cc133f6e48387a36af24747cd2fdeb2,Integrative methods for analyzing big data in precision medicine,"We provide an overview of recent developments in big data analyses in the context of precision medicine and health informatics. With the advance in technologies capturing molecular and medical data, we entered the area of “Big Data” in biology and medicine. These data offer many opportunities to advance precision medicine. We outline key challenges in precision medicine and present recent advances in data integration‐based methods to uncover personalized information from big data produced by various omics studies. We survey recent integrative methods for disease subtyping, biomarkers discovery, and drug repurposing, and list the tools that are available to domain scientists. Given the ever‐growing nature of these big data, we highlight key issues that big data integration methods will face.",56-149,2016.0,https://www.fisher.com/app/exploreauthor.php,Medicine
454,d0d5c4fc59d9e82318f33c75346de6c4f828a7e0,Energy Big Data Analytics and Security: Challenges and Opportunities,"The limited available fossil fuels and the call for sustainable environment have brought about new technologies for the high efficiency in the use of fossil fuels and introduction of renewable energy. Smart grid is an emerging technology that can fulfill such demands by incorporating advanced information and communications technology (ICT). The pervasive deployment of the advanced ICT, especially the smart metering, will generate big energy data in terms of volume, velocity, and variety. The generated big data can bring huge benefits to the better energy planning, efficient energy generation, and distribution. As such data involve end users' privacy and secure operation of the critical infrastructure, there will be new security issues. This paper is to survey and discuss new findings and developments in the existing big energy data analytics and security. Several taxonomies have been proposed to express the intriguing relationships of various variables in the field.",2423-2436,2016.0,http://miller.net/tags/tagpost.htm,Computer Science
455,fb40b7d4a9b7c1040173c0420b69ccbb7bcdbe5c,Forecasting Fine-Grained Air Quality Based on Big Data,"In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.",21-136,2015.0,http://jones.net/wp-content/categorycategory.asp,Computer Science
456,9dee8f8c93e14647124aaf6298aec6eb8f52b921,Remote sensing big data computing: Challenges and opportunities,,47-60,2015.0,https://www.douglas-rodriguez.info/listhome.html,Computer Science
457,174f5142dc4709da03f6c3bd7b6955b08c9762f3,Bottom of the Data Pyramid: Big Data and the Global South,"To date, little attention has been given to the impact of big data in the Global South, about 60% of whose residents are below the poverty line. Big data manifests in novel and unprecedented ways in these neglected contexts. For instance, India has created biometric national identities for her 1.2 billion people, linking them to welfare schemes, and social entrepreneurial initiatives like the Ushahidi project that leveraged crowdsourcing to provide real-time crisis maps for humanitarian relief. While these projects are indeed inspirational, this article argues that in the context of the Global South there is a bias in the framing of big data as an instrument of empowerment. Here, the poor, or the “bottom of the pyramid” populace are the new consumer base, agents of social change instead of passive beneficiaries. This neoliberal outlook of big data facilitating inclusive capitalism for the common good sidelines critical perspectives urgently needed if we are to channel big data as a positive social force in emerging economies. This article proposes to assess these new technological developments through the lens of databased democracies, databased identities, and databased geographies to make evident normative assumptions and perspectives in this under-examined context.",19,2016.0,http://dominguez.com/categoriesmain.php,Sociology
458,494f901ab46bd67de8ce46c7325822d43dffd8ce,On the Meaningfulness of “Big Data Quality” (Invited Paper),,6-20,2016.0,https://www.bates.info/tag/listprivacy.html,Computer Science
459,9a411034c3631e878008599d41b8214c36d95dfe,Is Big Data challenging criminology?,"The advent of ‘Big Data’ and machine learning algorithms is predicted to transform how we work and think. Specifically, it is said that the capacity of Big Data analytics to move from sampling to census, its ability to deal with messy data and the demonstrated utility of moving from causality to correlation have fundamentally changed the practice of social sciences. Some have even predicted the end of theory—where the question why is replaced by what—and an enduring challenge to disciplinary expertise. This article critically reviews the available literature against such claims and draws on the example of predictive policing to discuss the likely impact of Big Data analytics on criminological research and policy.",21 - 39,2016.0,http://www.brown-douglas.org/listhome.htm,Engineering
460,c73c0695ee7b2b6d20d443dca272966172cb6868,Crisis analytics: big data-driven crisis response,,21-150,2016.0,http://chase.net/tags/categories/listsearch.htm,Computer Science
461,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.","
          51-9
        ",2013.0,https://www.allen.biz/category/categoriesindex.html,Psychology
462,503dc3243d206ac86ac13bdc2bf38d1b3272b962,High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications,This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.,1011-1025,2015.0,http://www.jackson.biz/categorymain.htm,Computer Science
463,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",443-448,2014.0,http://www.patterson.info/categorieshomepage.html,Economics
464,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,The role of administrative data in the big data revolution in social science research.,,"
          1-12
        ",2016.0,https://coleman.com/blog/explore/appprivacy.php,Computer Science
465,0882e7abd738267e4f4f38ca37557b916b130f7e,"Big data, Big bang?",,1-14,2016.0,http://www.cannon.com/blog/search/categoriesauthor.htm,Computer Science
466,a8a40298b05466adf011b6f7f842427e6a1e5d02,Big Data: The V's of the Game Changer Paradigm,"The Big Data is the most prominent paradigm now-a-days. The Big Data starts rule slowly from 2003, and expected to rule and dominate the IT industries at least up to 2030. Furthermore, the Big Data conquer the technological war and easily capture the entire market since 2009. The Big Data is blasting everywhere around the World in every domain. The Big Data, a massive amount of data, able to generate billions of revenue. The secret behind of these billions of revenue is ever growing volume. This paper presents the redefinition of volume of Big Data. The volume is redefined by engaging three other V's, namely, voluminosity, vacuum, and vitality. Furthermore, this paper augments two new V's to the Big Data paradigm, namely, vendee and vase. This paper explores all V's of Big Data. There are lots of controversy and confusion regarding V's of Big Data. This paper uncovers the confusions of the V family of the Big Data.",17-24,2016.0,http://www.phillips.net/posts/search/mainpost.php,Computer Science
467,8a5187780cf04dbe202d2a44cac524100a6c3ad8,Perspective: Sustaining the big-data ecosystem,,S16-S17,2015.0,http://peters.com/main/explore/blogauthor.php,Business
468,b95ca56172201797aa5e51200fbf07309613c315,Towards an IoT Big Data Analytics Framework: Smart Buildings Systems,"There is a growing interest in IoT-enabled smart buildings. However, the storage and analysis of large amount of high-speed real-time smart building data is a challenging task. There are a number of contemporary Big Data management technologies and advanced analytics techniques that can be used to deal with this challenge. There is a need for an integrated IoT Big Data Analytics (IBDA) framework to fill the research gap in the Big Data Analytics domain. This paper presents one such IBDA framework for the storage and analysis of real time data generated from IoT sensors deployed inside the smart building. The initial version of the IBDA framework has been developed by using Python and the Big Data Cloudera platform. The applicability of the framework is demonstrated with the help of a scenario involving the analysis of real-time smart building data for automatically managing the oxygen level, luminosity and smoke/hazardous gases in different parts of the smart building. The initial results indicate that the proposed framework is fit for the purpose and seems useful for IoT-enabled Big Data Analytics for smart buildings. The key contribution of this paper is the complex integration of Big Data Analytics and IoT for addressing the large volume and velocity challenge of real-time data in the smart building domain. This framework will be further evaluated and extended through its implementation in other domains.",1325-1332,2016.0,http://www.roman-marsh.net/categoriesmain.asp,Computer Science
469,e9290576b477b684e40a4a7e2fa1fa314da49b94,Drivers of the Use and Facilitators and Obstacles of the Evolution of Big Data by the Audit Profession,"SYNOPSIS: Big Data is one of the most important developments in management practice today, with McKinsey Global Institute (2011) arguing that it will fundamentally change business. Forbes (2013) states that “the market for Big Data will reach $16.1 billion in 2014, growing 6 times faster than the overall information technology (IT) market.” Given the growing significance of Big Data as a business tool, this paper considers the extent to which Big Data will be embraced by the audit profession and how that usage will evolve over time. I put forward the hypothesis that auditors cannot stray too far from the practices of their clients since their credibility with and respect of those clients are the basis of the value added that they provide. Hence, if Big Data becomes an essential business tool, then inevitably it will have the same impact on auditing, albeit, perhaps later and with a more muted reaction. Analysis also indicates that American and international auditing standards, technological advances, and ...",439-449,2015.0,http://www.smith.com/wp-content/categories/tagterms.asp,Business
470,14fd675435f0bc3b76e22efe7a4fe73dcd919ced,"Privacy, Big Data, and the Public Good: Frameworks for Engagement","Big Data is a Big Deal; hence, this Big Book. Before turning to the book itself, I indulge in a bit of background. Most of us have seen claims that Big Data will be our salvation (for instance, by ...",119 - 119,2016.0,https://higgins-taylor.net/tagsmain.asp,Business
471,ffa05258ba9617925c697c99d0dfb5e3ba100844,Big data visualization: Tools and challenges,"In today's world where everything is recorded digitally, right from our web surfing patterns to our medical records, we are generating and processing petabytes of data every day. Big data will be transformative in every sphere of life. But just to process and analyze those data is not enough, human brain tends to find pattern more efficiently when data is represented visually. Data Visualization and Analytics plays important role in decision making in various sectors. It also leads to new opportunities in the visualization domain representing the innovative ideation for solving the big-data problem via visual means. It is quite a challenge to visualize such a mammoth amount of data in real time or in static form. In this paper, we discuss why big data visualization is of utmost importance, what are the challenges related to it and review some big data visualization tools.",656-660,2016.0,http://www.mills.com/tagsregister.htm,Computer Science
472,235af1d1e142cfbecbd0c64ef8126c97c3cd921f,The value of Big Data in servitization,,174-184,2015.0,https://walker.com/posts/app/postsfaq.html,Economics
473,4c7fa768a4c67a5332c0c58ed79043eda3a775d7,Building a Big Data Platform for Smart Cities: Experience and Lessons from Santander,"The Internet of Things (IoT) is now shaping our cities to make them more connected, convenient, and intelligent. However, this change will highly rely on extracted values and insights from the big data generated by our cities via sensors, devices, and human activities. Many existing studies and projects have been done to make our cities smart, focusing more on how to deploy various sensors and devices and then collect data from them. However, this is just the first step towards smart cities and next step will be to make good use of the collected data and enable context-awareness and intelligence into all kinds of applications and services via a flexible big data platform. In this paper, we introduce the system architecture and the major design issues of a live City Data and Analytics Platform, namely CiDAP. More importantly, we share our experience and lessons learned from building this practical system for a large scale running smart city test bed, SmartSantander. Our work provides a valuable example to future Smart City platform designers so that they can foresee some practice issues and refer to our solution when building their own smart city data platforms.",592-599,2015.0,https://www.allen.biz/posts/explorelogin.html,Computer Science
474,1d74b9a3cc22b479e1dab4762b3c16da8a2bda8a,An industrial big data pipeline for data-driven analytics maintenance applications in large-scale smart manufacturing facilities,,1-26,2015.0,http://hill.com/mainauthor.html,Computer Science
475,b0150dd118ebedbc3ece68726e065f9afaaf3b18,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",1-42,2016.0,http://www.stewart-nguyen.com/postssearch.html,Computer Science
476,c8ce1460f584a6fe1197d03f1e71d6cd9d9147a8,"Big data related technologies, challenges and future prospects",,283 - 285,2015.0,https://quinn.com/posts/postsauthor.htm,Computer Science
477,3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,Big Self-Supervised Models are Strong Semi-Supervised Learners,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",19-121,2020.0,http://www.rivera.info/categories/main/bloghomepage.jsp,Computer Science
478,62ed4d3b414792a5d5cbddad37869281de9908be,"Jose Maria Cavanillas, Edward Curry, and Wolfgang Wahlster (editors): new horizons for a data-driven economy: a roadmap for usage and exploitation of big data in Europe",,245 - 247,2016.0,http://proctor.com/searchterms.html,Political Science
479,95d559c91d01e28450653847a1d682aacd324ca0,Ethical Issues in the Big Data Industry,"Big Data combines information from diverse sources to create knowledge, make better predictions and tailor services. This article analyzes Big Data as an industry, not a technology, and identifies the ethical issues it faces. These issues arise from reselling consumers’ data to the secondary market for Big Data. Remedies for the issues are proposed, with the goal of fostering a sustainable Big Data Industry.",17-132,2015.0,http://white.info/tag/appabout.html,Computer Science
480,8bcc25651b62532f639fe713da9d7b67c742d562,Efficient Machine Learning for Big Data: A Review,,87-93,2015.0,https://www.holt-white.com/search/tags/postslogin.php,Computer Science
481,c49131dc223cdf72a63e02a996cec61b1f71e9dd,Managing data lakes in big data era: What's a data lake and why has it became popular in data management ecosystem,"The concept of a data lake is emerging as a popular way to organize and build the next generation of systems to master new big data challenges, but there are lots of concerns and questions for large enterprises to implement data lakes. The paper discusses the concept of data lakes and shares the author's thoughts and practices of data lakes.",820-824,2015.0,https://shah.com/tags/category/postsfaq.jsp,Geography
482,5d2f4d6516686fb337605057452058d56444c53d,Efficient Online Evaluation of Big Data Stream Classifiers,"The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.",52-102,2015.0,http://lucero.com/tag/categoryhome.html,Computer Science
483,adc180e1fe404b650fca3bb7970e43bdce34a611,A Big Data Modeling Methodology for Apache Cassandra,"Apache Cassandra is a leading distributed database of choice when it comes to big data management with zero downtime, linear scalability, and seamless multiple data center deployment. With increasingly wider adoption of Cassandra for online transaction processing by hundreds of Web-scale companies, there is a growing need for a rigorous and practical data modeling approach that ensures sound and efficient schema design. This work i) proposes the first query-driven big data modeling methodology for Apache Cassandra, ii) defines important data modeling principles, mapping rules, and mapping patterns to guide logical data modeling, iii) presents visual diagrams for Cassandra logical and physical data models, and iv) demonstrates a data modeling tool that automates the entire data modeling process.",238-245,2015.0,https://www.marquez-jensen.com/explore/listhomepage.html,Computer Science
484,1a69ec1864d490e121e6574b2bfe7817f475b7b9,Big data stream computing in healthcare real-time analytics,"The healthcare industry is changing at a dramatic rate. There are multiple processes going on within the health sector. These processes not only impact the care of individuals but also help medical practitioners and the delivery of care and services. The industry can take advantage of big data analytics to ensure that all the multiple processes within the industry are running smoothly. Big data analytics is not just an opportunity but a necessity. Recently, big data stream computing has been studied in order to improve the quality of healthcare services and reduce costs by capability support prediction, thus making decisions in real-time. This paper proposes a generic architecture for big data healthcare analytic by using open sources, including Hadoop, Apache Storm, Kafka and NoSQL Cassandra. The combination of high throughput publish-subscribe messaging for streams, distributed real-time computing, and distributed storage system can effectively analyze a huge amount of healthcare data coming with a rapid rate.",37-42,2016.0,http://www.campbell-simpson.com/tags/explore/tagfaq.html,Computer Science
485,fbc1c0375b33a868cb662548bbde23e840167fc3,The Human Face of Big Data,"Sandy Smolan, director; Rick Smolan, executive producer Premieres February 24, 2016, on PBS; check local listings Combining beautifully animated data visualizations with commentary from scientists, futurists, tech creators, and other experts, this 1-hour documentary highlights how researchers are using big data to tackle everything from malaria outbreaks and traffic jams to poverty and recidivism.",673 - 673,2016.0,https://www.lewis.com/postsmain.php,Political Science
486,17c2cda2cb039eddb6751696e7c079b1b15f1138,Life beyond big data: governing with little analytics,"Abstract The twenty-first-century rise of big data marks a significant break with statistical notions of what is of interest or concern. The vast expansion of digital data has been closely intertwined with the development of advanced analytical algorithms with which to make sense of the data. The advent of techniques of knowledge discovery affords some capacity for the analytics to derive the object or subject of interest from clusters and patterns in large volumes of data, otherwise imperceptible to human reading. Thus, the scale of the big in big data is of less significance to contemporary forms of knowing and governing than what we will call the little analytics. Following Henri Bergson's analysis of forms of perception which ‘cut out’ a series of figures detached from the whole, we propose that analytical algorithms are instruments of perception without which the extensity of big data would not be comprehensible. The technologies of analytics focus human attention and decision on particular persons and things of interest, whilst annulling or discarding much of the material context from which they are extracted. Following the algorithmic processes of ingestion, partitioning and memory, we illuminate how the use of analytics engines has transformed the nature of analysis and knowledge and, thus, the nature of the governing of economic, social and political life.",341 - 366,2015.0,https://jenkins.com/searchpost.jsp,Sociology
487,a0ff514a8a64ba5a7cd7430ca04245fd037d040c,Business Analytics in the Context of Big Data: A Roadmap for Research,"This paper builds on academic and industry discussions from the 2012 and 2013 pre-ICIS events: BI Congress III and the Special Interest Group on Decision Support Systems (SIGDSS) workshop, respectively. Recognizing the potential of “big data” to offer new insights for decision making and innovation, panelists at the two events discussed how organizations can use and manage big data for competitive advantage. In addition, expert panelists helped to identify research gaps. While emerging research in the academic community identifies some of the issues in acquiring, analyzing, and using big data, many of the new developments are occurring in the practitioner community. We bridge the gap between academic and practitioner research by presenting a big data analytics framework that depicts a process view of the components needed for big data analytics in organizations. Using practitioner interviews and literature from both academia and practice, we identify the current state of big data research guided by the framework and propose potential areas for future research to increase the relevance of academic research to practice.",23,2015.0,https://osborn-myers.com/list/wp-content/listterms.html,Computer Science
488,654e10be1113c6d73c9b65163b6f71d414406b6e,Big Data Analysis: Apache Spark Perspective,"the boom in the technology has resulted in emergence of new concepts and challenges. Big data is one of those spoke about terms today. Big data is becoming a synonym for competitive advantages in business rivalries. Despite enormous benefits, big data accompanies some serious challenges and when it comes to analyzing of big data, it requires some serious thought. This study explores Big Data terminology and its analysis concepts using sample from Twitter data with the help of one of the most industry trusted real time processing and fault tolerant tool called Apache Storm.",16-144,2015.0,http://ellis.com/wp-contentauthor.html,Computer Science
489,4b2b061a7e004216e61dadbf076c80a8301c4361,BigDansing: A System for Big Data Cleansing,"Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.",96-122,2015.0,https://martinez.net/tag/wp-contentregister.html,Computer Science
490,405d646cab73cfcbf441d239f9942bf1529e27ce,Big data in manufacturing: a systematic mapping study,,1-22,2015.0,http://martin.com/mainregister.htm,Computer Science
491,0e834f8594abda09b005aaf241a1e6b25abe7eb0,"Big Data Fundamentals: Concepts, Drivers & Techniques","This text should be required reading for everyone in contemporary business. --Peter Woodhull, CEO, Modus21 The one book that clearly describes and links Big Data concepts to business utility. --Dr. Christopher Starr, PhD Simply, this is the best Big Data book on the market! --Sam Rostam, Cascadian IT Group ...one of the most contemporary approaches Ive seen to Big Data fundamentals... --Joshua M. Davis, PhD The Definitive Plain-English Guide to Big Data for Business and Technology Professionals Big Data Fundamentals provides a pragmatic, no-nonsense introduction to Big Data. Best-selling IT author Thomas Erl and his team clearly explain key Big Data concepts, theory and terminology, as well as fundamental technologies and techniques. All coverage is supported with case study examples and numerous simple diagrams. The authors begin by explaining how Big Data can propel an organization forward by solving a spectrum of previously intractable business problems. Next, they demystify key analysis techniques and technologies and show how a Big Data solution environment can be built and integrated to offer competitive advantages. Discovering Big Datas fundamental concepts and what makes it different from previous forms of data analysis and data science Understanding the business motivations and drivers behind Big Data adoption, from operational improvements through innovation Planning strategic, business-driven Big Data initiatives Addressing considerations such as data management, governance, and security Recognizing the 5 V characteristics of datasets in Big Data environments: volume, velocity, variety, veracity, and value Clarifying Big Datas relationships with OLTP, OLAP, ETL, data warehouses, and data marts Working with Big Data in structured, unstructured, semi-structured, and metadata formats Increasing value by integrating Big Data resources with corporate performance monitoring Understanding how Big Data leverages distributed and parallel processing Using NoSQL and other technologies to meet Big Datas distinct data processing requirements Leveraging statistical approaches of quantitative and qualitative analysis Applying computational analysis methods, including machine learning",94-101,2015.0,https://www.montoya-clark.com/search/tagsindex.htm,Computer Science
492,6729aeba2d6d72bc75476b4ecd1f5556dce40342,Big Data Analysis: Apache Storm Perspective,"— the boom in the technology has resulted in emergence of new concepts and challenges. Big data is one of those spoke about terms today. Big data is becoming a synonym for competitive advantages in business rivalries. Despite enormous benefits, big data accompanies some serious challenges and when it comes to analyzing of big data, it requires some serious thought. This study explores Big Data terminology and its analysis concepts using sample from Twitter data with the help of one of the most industry trusted real time processing and fault tolerant tool called Apache Storm.",9-14,2015.0,https://www.fitzpatrick.com/wp-content/blogcategory.jsp,Computer Science
493,7d2534e27d25c3e860890852fefe62e85c5e5f42,Customer Engagement in a Big Data World,"This paper aims to propose that the literature on customer engagement has emphasized the benefits of customer engagement to the firm and, to a large extent, ignored the customers’ perspective. By drawing upon co-creation and other literature, this paper attempts to alleviate this gap by proposing a strategic framework that aligns both the customer and firm perspectives in successfully creating engagement that generates value for both the customer and the bottom line.,A strategic framework is proposed that includes the necessary firm resources, data, process, timeline and goals for engagement, and captures customers’ motives, situational factors and preferred engagement styles.,The authors argue that sustainability of data-driven customer engagement requires a dynamic and iterative value generation process involving customers recognizing the value of engagement behaviours and firm’s ability to capture and passing value back to customers.,This paper proposes a dynamic strategic value-creation framework that comprehensively captures both the customer and firm perspectives to data-driven customer engagement.",18-116,2016.0,http://smith.com/searchcategory.htm,Business
494,6985316725136e7b66ba6b886fce92194387841f,Architecting Time-Critical Big-Data Systems,"Current infrastructures for developing big-data applications are able to process –via big-data analytics- huge amounts of data, using clusters of machines that collaborate to perform parallel computations. However, current infrastructures were not designed to work with the requirements of time-critical applications; they are more focused on general-purpose applications rather than time-critical ones. Addressing this issue from the perspective of the real-time systems community, this paper considers time-critical big-data. It deals with the definition of a time-critical big-data system from the point of view of requirements, analyzing the specific characteristics of some popular big-data applications. This analysis is complemented by the challenges stemmed from the infrastructures that support the applications, proposing an architecture and offering initial performance patterns that connect application costs with infrastructure performance.",310-324,2016.0,http://bradford.com/app/search/categoryfaq.php,Computer Science
495,f56cdf52f7bb51a85a33323dffda5dd2e3ae440a,Big Data - What is it and why it matters.,"Big data, like MOOCs, altmetrics and open access, is a term that has been commonplace in the library community for some time yet, despite its prevalence, many in the library and information sector remain unsure of the relationship between big data and their roles. This editorial explores what big data could mean for the day-to-day practice of health library and information workers, presenting examples of big data in action, considering the ethics of accessing big data sets and the potential for new roles for library and information workers.","
          89-91
        ",2016.0,https://www.robinson.info/category/tag/tagsmain.asp,Medicine
496,def0cdb0398081295dec0a6d913105f8d5d94cac,Toward a manifesto for the ‘public understanding of big data’,"In this article, we sketch a ‘manifesto’ for the ‘public understanding of big data’. On the one hand, this entails such public understanding of science and public engagement with science and technology–tinged questions as follows: How, when and where are people exposed to, or do they engage with, big data? Who are regarded as big data’s trustworthy sources, or credible commentators and critics? What are the mechanisms by which big data systems are opened to public scrutiny? On the other hand, big data generate many challenges for public understanding of science and public engagement with science and technology: How do we address publics that are simultaneously the informant, the informed and the information of big data? What counts as understanding of, or engagement with, big data, when big data themselves are multiplying, fluid and recursive? As part of our manifesto, we propose a range of empirical, conceptual and methodological exhortations. We also provide Appendix 1 that outlines three novel methods for addressing some of the issues raised in the article.",104 - 116,2016.0,http://www.thomas-charles.com/wp-content/tags/appfaq.html,Medicine
497,4df00051e84846933299cd823a40c15e47adb5c4,"Big data for education data mining, data analytics and web dashboards","In this era of big data, school and universities are gathering tons of information. But much of that data is stored in ways that make it difficult for teachers and managers to access it. Usually written reports tell only one story or display just one piece of information. Many educational institutions use Moodle as educational environment in the process of learning and when it comes to a bigger number of users and course participants it becomes hard to follow their activity in the courses. To make studying more effective, it is important to supply personalization of the participants, based on their activity, an opportunity to analyze their activities in different courses, predict the results of the participants and get better survey of the activities of the students. The goal of this work is, by the use of data mining techniques to describe the process of selection and acquiring data from the Moodle database, and to create dashboard - web based application, that would communicate with Moodle and supply multilevel approach, and practically improve the approach to evaluation of larger groups of participants in the learning process and will help teachers to learn more about how students learn.",39-46,2015.0,https://murray.org/listregister.jsp,Computer Science
498,1719c6baf1a95492d47fe6dac0db677045ce2f96,Big data as complementary audit evidence,"SYNOPSIS In this paper we argue for the use of Big Data as complementary audit evidence. We evaluate the applicability of Big Data using the audit evidence criteria framework and provide cost-benefit analysis for sufficiency, reliability, and relevance considerations. Critical challenges, including integration with traditional audit evidence, information transfer issues, and information privacy protection, are discussed and possible solutions are provided.",431-438,2015.0,http://www.walton.biz/tags/tag/searchauthor.php,Business
499,ca8f5eac152531426396e1e0b7e6b62658c72060,Big Data and Journalism,"Big data is a social, cultural, and technological phenomenon—a complex amalgamation of digital data abundance, emerging analytic techniques, mythology about data-driven insights, and growing critique about the overall consequences of big-data practices for democracy and society. While media and communication scholars have begun to examine and theorize about big data in the context of media and public life broadly, what are the particular implications for journalism? This article introduces and applies four conceptual lenses—epistemology, expertise, economics, and ethics—to explore both contemporary and potential applications of big data for the professional logic and industrial production of journalism. These distinct yet inter-related conceptual approaches reveal how journalists and news media organizations are seeking to make sense of, act upon, and derive value from big data during a time of exploration in algorithms, computation, and quantification. In all, the developments of big data potentially have great meaning for journalism’s ways of knowing (epistemology) and doing (expertise), as well as its negotiation of value (economics) and values (ethics). Ultimately, this article outlines future directions for journalism studies research in the context of big data.",447 - 466,2015.0,http://www.heath.net/searchlogin.html,Sociology
500,133bcd7488a3c07cb0f493a87564c30e5433768c,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,852 - 857,2019.0,http://www.sullivan.org/blog/categoryhomepage.jsp,Engineering
501,fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",1591 - 1594,2020.0,https://www.young.com/tags/blog/listindex.php,Physics
502,c082ccfcfe1afc696e371374146ba9380b84061e,The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",62,2023.0,https://www.pierce.com/categoryterms.htm,Computer Science
503,ed6473fd5294a0639d661e02092768f364d80f39,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",12 - 13,2023.0,http://www.black.com/wp-content/mainindex.html,Sociology
504,8a4fc5f00cd4aca61e148e46a2125c3a406719f1,DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",18319-18345,2022.0,http://peters.com/explore/list/tagregister.php,Computer Science
505,fb29359d794265c0931d756858a70c9265b5693d,The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",38-114,2022.0,https://www.orozco.org/category/tag/wp-contentlogin.html,Medicine
506,8bb6a6802027c7f2489accb0559e6f02984535c9,"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",1-14,2022.0,http://smith-johnson.biz/categorieshomepage.asp,Computer Science
507,70fe060c12b3f100148d1a2be1e8f4254022543e,The case for data science in experimental chemistry: examples and recommendations,,357 - 370,2022.0,http://www.davis.biz/list/exploreterms.htm,Medicine
508,88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",58-117,2020.0,http://www.smith.com/categoriesterms.asp,Computer Science
509,108acf9a358512a40191d857e2456aeaaac3303b,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",1231 - 1241,2022.0,https://www.anderson-jordan.info/categoryindex.html,Computer Science
510,4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",3048-3061,2021.0,http://lewis.org/posts/explore/tagfaq.jsp,Computer Science
511,370d248f97b75c4040e5828a658bbe4c3b80bf1e,Eleven grand challenges in single-cell data science,,64-147,2020.0,http://norman.com/list/tags/wp-contentterms.html,Biology
512,72d3ddf1f7210d7e70144bbc09f770ec411fe909,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",46-130,2020.0,http://www.valentine-wong.net/explore/categoriescategory.htm,Computer Science
513,ede0a8039a561905f40777ec2ae66c2010e3f2bc,Cybersecurity data science: an overview from machine learning perspective,,63-145,2020.0,https://www.beasley.com/main/app/mainhomepage.html,Computer Science
514,b79ca6fd3df135a9bcf778844be625b764fbcfb3,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,,1247 - 1253,2019.0,https://brown.com/appcategory.jsp,Medicine
515,9bcf291c6245a3c2ee101babf4c1f0bbfa166f92,Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),,34-109,2021.0,https://www.hernandez-tyler.com/app/postsprivacy.html,Medicine
516,7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",64-101,2021.0,http://www.rivas.com/wp-content/tag/categorieshomepage.php,Computer Science
517,44321686d59d889af1760357940f04fbb6629597,"The role of data science in healthcare advancements: applications, benefits, and future prospects",,1473 - 1483,2021.0,http://www.glover.biz/wp-content/blog/searchhomepage.html,Medicine
518,f9d403c58db99e2214f43e5b1740694b9c79002f,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",2091-2103,2021.0,https://www.burnett-sanchez.com/search/list/blogpost.html,Computer Science
519,3df3bacc84593e9efb86d2c4d3ce30463048159f,Data Science,,16-109,2022.0,https://little-hawkins.com/posts/search/mainlogin.htm,Technology
520,8bba999de25bfb288b3f7f88e1d907aab02638b6,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",8066 - 8129,2020.0,http://www.miller-colon.net/categoryindex.php,Materials Science
521,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",566-806,2020.0,http://www.bailey-wilson.com/categoryregister.asp,Computer Science
522,f9b0b10713044c146caa84704b66804aa1e82d5e,Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",76 - 87,2021.0,http://coleman.org/tags/tags/tagauthor.html,Computer Science
523,c13147ef0b86d5ec833c272840f8f3bdacf96e7f,Data science: a game changer for science and innovation,,263 - 278,2021.0,https://www.bell-martin.com/listhomepage.html,Computer Science
524,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",1 - 23,2020.0,https://keller.com/explore/search/searchsearch.jsp,Computer Science
525,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",85-103,2020.0,http://www.reyes.info/blog/categorieshome.html,Computer Science
526,1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",1,2018.0,http://www.nichols.org/explore/mainpost.php,Computer Science
527,f42c69dbd792155fee6f4d2c525971f8d43f138b,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",21-116,2020.0,http://www.lee.com/tag/categoryindex.php,Medicine
528,5b9ea2abf1c5a04b3024367409284edceb741ef2,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",30-112,2020.0,http://park.com/main/taghome.html,Computer Science
529,12f62537251cf8eb76fa11c59df68d2211008898,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",743 - 767,2020.0,http://yu.com/main/tags/searchabout.php,Engineering
530,a4b6f802b3f416fb1af6d723e0549c5e6d34faae,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",54-120,2020.0,https://www.williams.biz/searchregister.jsp,Computer Science
531,7dcd9585d08eb40f043ae2ffccb86897a6a031e6,Supervised and Unsupervised Learning for Data Science,,61-107,2020.0,https://www.espinoza.com/listlogin.php,Technology
532,648ba966b63975c6859e1948ae3ddc30053884e4,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",61-123,2020.0,http://www.oconnor-williams.com/listhome.htm,Computer Science
533,deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",3878-3905,2020.0,https://hardin.com/explore/mainhomepage.jsp,Computer Science
534,0669286d8d4ca8ec2fdf16b7813157c21eb690be,Heidelberg colorectal data set for surgical data science in the sensor operating room,,41-113,2020.0,http://www.thompson.com/search/tag/categorymain.html,Medicine
535,8ece479b5dfed4727d2d9b9763f777bb9a94096e,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",1 - 24,2019.0,https://lewis.com/explorepost.htm,Computer Science
536,82620503cacf8ff6f8f3490e7bdf7508f1ab2021,Opening practice: supporting reproducibility and critical spatial data science,,477 - 496,2020.0,https://davenport.biz/wp-contentregister.php,Computer Science
537,398b154013db9d8025bf60f910bc156dedd9b40e,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",91-109,2019.0,http://www.galloway-mccall.com/wp-contentlogin.html,Computer Science
538,a11e157cb828b800426223f0a3d79e8fb122c8cc,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",37-108,2019.0,http://www.villa.com/blog/categories/appabout.jsp,Computer Science
539,e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,Data science and AI in FinTech: an overview,,81 - 99,2020.0,http://www.davis.com/mainregister.html,Computer Science
540,8d082fcbfb3dd56d52faabe7903f3b0734b1f613,Data science:,"The ability to harness data for actionable insight is increasingly essential for nearly every sector of the economy. We are awash in data, yet companies and organizations don’t always know how best to leverage their data to meet strategic goals, improve outcomes, or simply gain deeper understanding of their operations. This stackable graduate certificate in analytics and modeling focuses on foundational skills and knowledge for those working in or hoping to work in data science and analytics in any industry.",73-141,2021.0,http://castillo.org/blog/wp-contentabout.htm,Art
541,3b16bcb226bb1c87a6e63e0658be30067ed03f57,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,,58-139,2019.0,https://www.collins.biz/searchhome.htm,Computer Science
542,41cf91ee13a1d15983ede066ddf6b67cc94a41f4,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",51-123,2020.0,https://www.randolph-williams.com/blogauthor.php,Engineering
543,840b60da93c2776230d3e6123d708e1c7e66ebc0,Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",S27 - S39,2020.0,http://www.hill-park.com/wp-content/listregister.htm,Engineering
544,d88d39dd9c910105e7503aa43698c806d42d5198,Statistical Foundations of Data Science,,62-116,2020.0,https://www.lindsey.org/mainfaq.htm,Computer Science
545,c016852106ac787678105fd9dd22e57ba620517c,Human Data Science,,58-120,2020.0,http://www.savage.com/tag/tags/tagpost.asp,Medicine
546,579b64d2179a58a8bc586c30850ea238d3c14164,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",4586-4608,2020.0,https://www.beltran.com/tag/categories/wp-contentabout.asp,Computer Science
547,d9a984e15b1a86a66ecbac9e66d458dae4cb616c,What Is Data Science,,29-58,2018.0,https://heath-weaver.org/categoryhomepage.php,Sociology
548,27b9d1182e913decc7ef6a3509245fa6b6fd509d,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",3920 - 3929,2019.0,https://www.chen.com/tagsindex.asp,Computer Science
549,62a9d4f1763c5071cb2476c100614ba9741f036b,Data science applications to string theory,,1-117,2020.0,https://pearson-hudson.biz/app/exploreindex.jsp,Physics
550,e2055b85dab66c922ccf25a28046e8e559074824,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",448-460,2019.0,http://bruce.com/tag/categorymain.php,Computer Science
551,b017bf6879e57077b4b4e180a02747b89878d7a1,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",S16 - S26,2020.0,http://www.simpson.com/explore/list/poststerms.html,Computer Science
552,4b5505a54799d796ae94115409b01ee33a7e2b20,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",612 - 616,2020.0,https://thompson.biz/list/tags/listauthor.php,Medicine
553,28cc044d5ba938472bc53d87240583982ad21663,Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",74-120,2020.0,https://brown-terrell.com/tag/posts/categorieslogin.asp,Computer Science
554,8e234be4cdc34ea8deba609c31858198ad941797,Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers,"Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",1-11,2020.0,https://mccoy-pope.net/explore/categories/appregister.php,Technology
555,38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",S40 - S50,2020.0,https://www.holmes.net/explore/wp-contentterms.htm,Engineering
556,3746152e023e79b7d03cf12a560e473de2945d67,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",71-111,2020.0,https://sutton-davis.biz/app/wp-content/categoryprivacy.php,Computer Science
557,4271faaa82eb722d079222211c30ab642bc734be,The data science life cycle,A cycle that traces ways to define the landscape of data science.,58 - 66,2020.0,https://grant.org/categories/listauthor.asp,Computer Science
558,89f41c87c8849ce37e609c1010087291a4679a37,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",91-119,2019.0,http://www.harris.com/category/explore/blogauthor.htm,History
559,9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,The State of the Art of Data Science and Engineering in Structural Health Monitoring,,84-118,2019.0,http://www.miller.com/main/categoriesindex.html,Computer Science
560,f56425ec56586dcfd2694ab83643e9e76f314e91,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",745 - 766,2017.0,https://gibson.org/tagsfaq.htm,Engineering
561,edca4813d7da7507ff5a8a89e6c0481f8967ac84,Data Science Challenges & Enhanced Data Science Methodology,,35-126,2021.0,http://nelson.com/blogmain.php,Engineering
562,1ec4d0e29455e47245edaa17368257df3efb6562,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",53-133,2019.0,http://roberts.net/wp-content/search/mainfaq.htm,Computer Science
563,2ad13329d44c74041626a60898ccf921b0bdacd3,SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",17-104,2019.0,https://www.mosley.org/exploreterms.html,Computer Science
564,1ac524c713423bc50822b34e0aa1bbfab42d2b00,Data science for entrepreneurship research: studying demand dynamics for entrepreneurial skills in the Netherlands,,651 - 672,2019.0,https://white.biz/searchprivacy.php,Business
565,678da221aa156807bc2c191ed5f4bcbb0b25d421,Data science ethical considerations: a systematic literature review and proposed project framework,,1-12,2019.0,http://shaw.org/app/listhomepage.htm,Computer Science
566,0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,,68-125,2019.0,http://martinez.com/search/mainhome.html,Medicine
567,5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",85-138,2019.0,https://duarte.biz/tagshome.asp,Technology
568,2081ed6854290a479f796f2432c7951ff24232fe,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",77-112,2019.0,https://www.andrews.com/search/search/mainindex.jsp,Sociology
569,0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",19-129,2019.0,https://www.meadows.com/category/postshomepage.php,Engineering
570,e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",771 - 780,2019.0,https://hernandez.net/category/app/wp-contentregister.php,Computer Science
571,b00f836c62d0ea7678d0f20aeec3397138633060,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",80-101,2019.0,http://macdonald-lam.com/postshomepage.html,Medicine
572,2ab2796390ac12df283e218907ed0ffef232dbc7,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",1 - 10,2019.0,http://barnes-schaefer.com/appterms.php,Sociology
573,e799d31e1c2d80a971c1f956d62b98c0a9f27031,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",101-113,2019.0,https://www.ward-chaney.com/blog/blogindex.php,Computer Science
574,b134d892f4e76081f5fa36b0b7c2e7118be53907,Genomics and data science: an application within an umbrella,,97-102,2019.0,http://moss.net/categories/explore/blogsearch.html,Biology
575,eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,Bayesian Optimization and Data Science,,96-101,2019.0,http://www.weaver.com/categories/categories/listhome.htm,Computer Science
576,702cd9a7a128706b8a6ec88e7424e06c326021e5,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",26-123,2019.0,http://www.flores.com/tagsauthor.html,Geography
577,863a35bdd1ae803491801e283c2ae79fe973cf68,Microbiome data science,,61-139,2019.0,https://www.gray.com/wp-content/list/appauthor.php,Medicine
578,747359803e9a734fa4f1338a83121a942f3da60e,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",65-112,2019.0,http://morris-andersen.biz/tagslogin.htm,Technology
579,d6801d0ffd08ba56bccfa01884bb6a126f99de2e,"Our data, our society, our health: A vision for inclusive and transparent health data science in the United Kingdom and beyond","The last 6 years have seen sustained investment in health data science in the United Kingdom and beyond, which should result in a data science community that is inclusive of all stakeholders, working together to use data to benefit society through the improvement of public health and well‐being.",29-105,2019.0,https://www.dougherty-taylor.org/tags/tagsauthor.php,Business
580,9d76e69f54bdf739fcd61d0fd25f92c7dd3923c2,Data Science for Wind Energy,,91-144,2019.0,http://www.smith-roman.com/blog/list/postshome.jsp,Environmental Science
581,590ead4aeddbf8fea8414998b2dc3b74576a71cb,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",42 - 49,2018.0,http://bradford.net/categories/categories/apphomepage.htm,Computer Science
582,f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,From hype to reality: data science enabling personalized medicine,,40-105,2018.0,https://www.butler.org/categories/explore/wp-contentabout.asp,Medicine
583,8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",78-145,2019.0,http://www.evans-castillo.com/blogauthor.jsp,Computer Science
584,bb44d1472bb281c699ef556f6eb6ccc66889f2d3,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",27-102,2019.0,http://www.griffin-wolf.com/categories/blog/wp-contentcategory.htm,Technology
585,bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.","
          23-41
        ",2019.0,https://bowman.org/category/searchauthor.asp,Computer Science
586,daec8baf1740a09725b375729d95caebc42f61c8,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",91-105,2019.0,http://thornton.biz/tag/search/mainsearch.html,Computer Science
587,b85ac20631159ca3e370afa9c1f81a4618242b4f,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",1 - 7,2019.0,https://meadows-watson.com/tags/category/searchcategory.php,Political Science
588,31de1d23b2f1f1c14ce7025489d2892aa10fb1ef,A review of data science in business and industry and a future view,"The aim of this paper is to frame Data Science, a fashion and emerging topic nowadays in the context of business and industry. We open with a discussion about the origin of Data Science, and its requirement for a challenging mix of capability in data analytics, information technology and business know-how. The mission of Data Science is to provide new or revised computational theory able to extract useful information from the massive volumes of data collected at an accelerating pace. In fact, besides the traditional measurements, digital data obtained from images, text, audio, sensors, etc complement the survey. Then we review the different and most popular methodologies amongst the practitioners of Data Science research and applications. And since the emerging field requires personnel with new competences, we attempt to describe the Data Scientist profile, one of the sexiest jobs of the 21st Century according to Davenport and Patil. Most people are aware of the need to embrace Data Science, but they feel intimidated that they don’t understand it and they worry that their jobs will disappear. We want to encourage them: Data Science is more likely to add value to jobs and enrich the lives of working people by helping them make better, more informed business decisions. We conclude the paper by presenting examples of Data Science in action in business and industry, to demonstrate the collection of specialist skills that must come together for this new science to be effective.",76-124,2019.0,https://www.woods-greene.org/categoryterms.asp,Engineering
589,46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",422-441,2019.0,https://jimenez.info/main/categories/categorieshomepage.htm,Computer Science
590,36708c11c2fde2efb50e75d81f174b2c205082c8,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",83-140,2019.0,http://www.blair.com/posts/mainauthor.php,Business
591,4f0218eb9ed62d5acc03f02bfa24b388a66067e8,Distance geometry and data science,,271-339,2019.0,http://www.ward-kaiser.com/wp-contentlogin.htm,Computer Science
592,4aeda303fa0b9beae3f6d65e052dace9d4540116,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",241 - 257,2019.0,http://sanders.info/list/appcategory.htm,Computer Science
593,fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",32-103,2019.0,http://richardson-jones.info/search/categoriesauthor.html,Engineering
594,08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",75-115,2019.0,https://www.peterson.org/app/exploresearch.html,Political Science
595,e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",2318-2331,2016.0,https://owens-campbell.com/postscategory.php,Computer Science
596,bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",67 - 72,2018.0,https://ryan-roberts.com/categoriesmain.htm,Computer Science
597,ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",249-265,2018.0,http://www.fisher.biz/tags/wp-contentfaq.jsp,Political Science
598,6bec0106bebc93fc30ec47af9779d7e327639034,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",34-101,2018.0,http://www.smith.org/list/mainabout.htm,Medicine
599,3335c340c20609b4e6de481c9eaf67ecd6c960dc,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",30-142,2016.0,https://diaz-jackson.com/tags/search/wp-contentcategory.php,Computer Science
600,c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,"Introduction to Data Science - A Python Approach to Concepts, Techniques and Applications",,1-215,2018.0,https://vega-johnson.com/taghomepage.php,Computer Science
601,0a4b3c33e830d8cde364443a52e673c2c07dcfe8,Open Data Science,,31-39,2018.0,https://www.marshall.info/taglogin.php,Computer Science
602,577564ac25a12b37972d77a35b589f6b2270a45f,Big Data and Data Science in Critical Care.,,"
          1239-1248
        ",2018.0,http://www.drake-adams.com/category/blog/exploreauthor.html,Medicine
603,140a6476f7b8dde9e7bbcd199d248fc629721faa,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",1 - 28,2018.0,http://www.smith.org/categoriesauthor.asp,Sociology
604,305600f3cba8a63bad1bedeab34a299bf748754b,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",2150-2164,2018.0,http://www.padilla.org/app/blogmain.html,Computer Science
605,f968cdd4637e7b26ca6c057a2f7f593b8cea2d18,Fundamentals of Clinical Data Science,,63-130,2018.0,https://www.smith-brown.com/list/tagsauthor.asp,Technology
606,6bf9d589f80823735084956f056728ae1a7bcfa8,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",42-103,2018.0,https://cain.com/main/categories/searchindex.html,Computer Science
607,a2d7efb8b174702111e713765cbf741dff2bf9b8,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",45-144,2018.0,https://www.dean.org/search/wp-contentpost.php,Physics
608,ff6586ab32e9ed45d20a486ec7c5be02da5d3f1f,Data Science: the impact of statistics,,189 - 194,2018.0,https://johnson.org/posts/categoryhome.html,Computer Science
609,2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",64-105,2017.0,http://kelley-williams.net/tags/explorehome.htm,Mathematics
610,2146edb37621d80f53c1261c8a53c94d3dda84c8,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",1-5,2018.0,https://miller.com/mainprivacy.asp,Computer Science
611,0ec2d4c804dd2b4446e1808dc85b4fe4a27b1766,Surgical data science for next-generation interventions,,691 - 696,2017.0,https://www.wilson.com/wp-content/listcategory.html,Computer Science
612,79be83a308a9a75ef4e64f63a938b201531c0bbf,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",1 - 42,2017.0,https://moore.com/search/listabout.html,Computer Science
613,c0225f99c9b1619c3be74b63241faffe02d275d7,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",8689 - 8692,2017.0,http://www.hodges-arias.info/mainregister.html,Medicine
614,e4c66275e46a66586365c851f0974a3c88baf3d7,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",26-123,2018.0,https://johnson.com/category/search/tagprivacy.php,Medicine
615,89535aa63bc5dac6f3beb60b813abb77aa4309d1,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",85 - 97,2017.0,https://www.ramirez.com/wp-contentmain.html,Medicine
616,a1dbdc2ce338d694a720163f591e4eb5c4070140,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",50-141,2018.0,https://www.stevenson.com/main/appindex.jsp,Computer Science
617,b154d9ce0a551be90557d7a24a49b1988add2a81,"Three principles of data science: predictability, computability, and stability (PCS)","In this talk, I'd like to discuss the intertwining importance and connections of three principles of data science in the title and the PCS workflow that is built on the three principles. The principles will be demonstrated in the context of two collaborative projects in neuroscience and genomics for interpretable data results and testable hypothesis generation.",4-4,2018.0,https://baker-archer.net/tag/blog/listauthor.html,Computer Science
618,5a44f70130875b212452ad777ab02a4eb5cd35d9,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",85-119,2018.0,http://bell.com/search/categoryfaq.asp,Medicine
619,3c51a892ce5a8fc78d57ea290c6e5144ee9db579,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",31-126,2018.0,https://gonzalez-ferguson.com/categories/blogregister.php,Computer Science
620,fde0b586e3bc9e5139a14493044bce9ff61706d4,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",197 - 261,2017.0,http://www.sanchez-thomas.com/main/search/exploreauthor.html,Physics
621,afe79672aa99b7f606cbff234ec2454cf2295554,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.","
          95-106
        ",2017.0,https://herrera.net/postslogin.htm,Medicine
622,cdd5d0a3e2ba0e1f4b5bcd3115e5f5b6536e24f9,The Data Science Design Manual,,1-425,2017.0,http://www.rosales.com/categories/posts/tagsmain.html,Computer Science
623,f4e66bd035e195f539f1b65a5aaec0e873cdee29,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,1066 - 1078,2017.0,https://www.smith-nichols.net/posts/tagsauthor.htm,Computer Science
624,b0fbdffb9733e7857afbb21ccbcd9cd74803ca1d,"Data Science and symbolic AI: Synergies, challenges and opportunities","Symbolic approaches to Artificial Intelligence (AI) represent things within a domain of knowledge through physical symbols, combine symbols into symbol expressions, and manipulate symbols and symbol expressions through inference processes. While a large part of Data Science relies on statistics and applies statistical approaches to AI, there is an increasing potential for successfully applying symbolic approaches as well. Symbolic representations and symbolic inference are close to human cognitive representations and therefore comprehensible and interpretable; they are widely used to represent data and metadata, and their specific semantic content must be taken into account for analysis of such information; and human communication largely relies on symbols, making symbolic representations a crucial part in the analysis of natural language. Here we discuss the role symbolic representations and inference can play in Data Science, highlight the research challenges from the perspective of the data scientist, and argue that symbolic methods should become a crucial component of the data scientists’ toolbox.",27-38,2017.0,http://www.gordon-ballard.com/tag/category/mainpost.asp,Computer Science
625,d2f83aa22def149095f1dd89b4cf36d09a748a87,Data science is science's second chance to get causal inference right: A classification of data science tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term""data science""provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",62-115,2018.0,https://www.harper.biz/searchfaq.php,Computer Science
626,b34b9758b36c92c023c3c10f3a39aeb8f5c83927,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",44-125,2018.0,http://www.brown.com/category/tags/listauthor.asp,Engineering
627,88761dffd173cd0e75e88c02d68f866f8cc43c14,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",19 - 25,2017.0,http://www.hernandez.com/posts/wp-content/tagterms.htm,Computer Science
628,94c52a7516ef8955f76c3ee1319ff4fd8bf071fd,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",76-118,2016.0,http://www.brown.com/list/main/explorecategory.html,Computer Science
629,5de20ffb7852ae0665c382084c8a56918f23dc0b,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",42-123,2018.0,http://www.pierce-myers.com/listmain.html,Computer Science
630,798e5e09c20b0270701b194a3198427fec6a4fcd,What makes Data Science different? A discussion involving Statistics2.0 and Computational Sciences,,167 - 175,2018.0,https://www.weber.net/categories/explore/appcategory.html,Computer Science
631,dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",1243-1257,2018.0,http://www.phillips.net/category/listindex.html,Computer Science
632,c9ed1ad1a3a08bf5ebebe8105805dd102546b8f3,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,,54-68,2017.0,https://griffith-vasquez.com/search/app/listindex.html,Engineering
633,e36022198f21f46d066007ee5cf901ea55080e21,"Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications","This accessible and classroom-tested textbook/reference presents an introduction to the fundamentals of the emerging and interdisciplinary field of data science. The coverage spans key concepts adopted from statistics and machine learning, useful techniques for graph analysis and parallel programming, and the practical application of data science for such tasks as building recommender systems or performing sentiment analysis. Topics and features: provides numerous practical case studies using real-world data throughout the book; supports understanding through hands-on experience of solving data science problems using Python; describes techniques and tools for statistical analysis, machine learning, graph analysis, and parallel programming; reviews a range of applications of data science, including recommender systems and sentiment analysis of text data; provides supplementary code resources and data at an associated website.",21-112,2017.0,http://www.castaneda-russell.com/list/wp-contentauthor.html,Computer Science
634,d01c02801212f823cdc11e329b3a1afa63f3a2d5,Shifting to Data Savvy: The Future of Data Science In Libraries,"The Data Science in Libraries Project is funded by the Institute for Museum and Library Services (IMLS) and led by Matt Burton and Liz Lyon, School of Computing & Information, University of Pittsburgh; Chris Erdmann, North Carolina State University; and Bonnie Tijerina, Data & Society. The project explores the challenges associated with implementing data science within diverse library environments by examining two specific perspectives framed as ‘the skills gap,’ i.e. where librarians are perceived to lack the technical skills to be effective in a data-rich research environment; and ‘the management gap,’ i.e. the ability of library managers to understand and value the benefits of in-house data science skills and to provide organizational and managerial support. 
 
This report primarily presents a synthesis of the discussions, findings, and reflections from an international, two-day workshop held in May 2017 in Pittsburgh, where community members participated in a program with speakers, group discussions, and activities to drill down into the challenges of successfully implementing data science in libraries. Participants came from funding organizations, academic and public libraries, nonprofits, and commercial organizations with most of the discussions focusing on academic libraries and library schools.",67-119,2018.0,http://white.com/list/blog/blogfaq.htm,Sociology
635,1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,"STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",514 - 539,2018.0,https://www.turner.com/wp-contentabout.htm,Sociology
636,04d7b3457dc78b2d2282e6af2c787308f75c9b26,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",62-136,2018.0,https://www.wolfe.com/tagshome.jsp,Computer Science
637,843793928e308b5414d2883ac869e813ec16f65d,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",21-129,2018.0,http://www.thomas.biz/listfaq.jsp,Computer Science
638,b4332aaabec46e386fff31d066f278fc27cfa1cb,How data science can advance mental health research,,24 - 32,2018.0,http://duke-cobb.com/tag/tagsearch.php,Medicine
639,a0ef3467c09acc3106b915258b7b8db7bb663b77,Data Science Methodology for Cybersecurity Projects,"Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.",26-127,2018.0,http://allison-smith.com/mainterms.html,Computer Science
640,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,,36-53,2017.0,https://www.phillips-baker.com/listterms.php,Materials Science
641,9b54c9a7d2060f800961c2f9195fcf5408288f17,Data Science for Undergraduates,,52-118,2018.0,http://www.romero.com/search/tags/bloglogin.htm,Computer Science
642,1d045f4f347409f0635c9d15d538dbfabb6b38fa,Environmental Data Science,,4-12,2018.0,https://rodriguez.com/categoryterms.jsp,Computer Science
643,818c9fd2df1229f962af3c50ef493e8633433fb9,Data Science Thinking,,59-90,2018.0,https://valenzuela.com/exploreindex.asp,Computer Science
644,e420259fd53d15c2b6cb8906027d5a100ca356d7,Data science for building energy management: A review,,598-609,2017.0,http://www.keith-thomas.org/main/tags/categoriesindex.php,Engineering
645,ea07f64ad84542e04acc41db6b171007f344efd7,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",211-215,2018.0,https://www.jackson-young.com/searchmain.html,Computer Science
646,e78be911203960b3b2a417465d726734367f8e30,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",93-101,2018.0,http://perry.com/category/categorylogin.php,Sociology
647,010f65dd2fa979892a8229db825954871652fb8f,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",235-251,2018.0,https://www.good-wells.org/main/categories/blogindex.asp,Computer Science
648,00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",1-10,2017.0,http://taylor.net/blog/postsauthor.jsp,Computer Science
649,6e8d94181832771bc5dca8d288c52b6ad5914029,Data Science as Machinic Neoplatonism,,253 - 272,2017.0,https://castaneda-wells.com/main/category/categoriessearch.htm,Sociology
650,972edbd8bd19486a37c0a9f34508634fc8733529,Our path to better science in less time using open data science tools,,88-145,2017.0,https://le.com/tagsregister.php,Computer Science
651,8d89159249e0faf5deae508cc8533010898bbda5,Data Science in Action,,3-23,2016.0,http://jimenez.net/wp-content/app/searchregister.php,Computer Science
652,82feed9f0f8d077046b9b8be36e664483a66e33b,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",89 - 96,2018.0,https://schneider-brown.com/explore/tags/tagssearch.htm,Computer Science
653,a1dc9a3df54ac24712fc47ac5f0b116f1043b95e,R – Data Science,,52-136,2017.0,http://richardson.com/explore/categoriesprivacy.htm,Computer Science
654,bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",57-113,2017.0,https://bailey.com/category/listlogin.htm,Computer Science
655,61b3ce156347a7f107df75924a45f81f12a0ef14,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",109 - 121,2017.0,https://freeman-kelley.org/explore/exploreterms.jsp,Computer Science
656,796d70a6eb0428ae19f1187ae1c81185d4ae6701,Automating Biomedical Data Science Through Tree-Based Pipeline Optimization,,123-137,2016.0,https://anderson.com/tag/tag/bloglogin.htm,Computer Science
657,843149b649b888fdb3649b8d4852263b62356799,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.","
          292-303
        ",2018.0,http://www.thompson-fowler.com/tagmain.htm,Computer Science
658,f447afeccbdb9ed5df15c44011aec9c018d4b2c4,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",1 - 18,2017.0,http://www.lane.com/app/categoryhomepage.php,Computer Science
659,b2114228411d367cfa6ca091008291f250a2c490,Deep learning and process understanding for data-driven Earth system science,,195 - 204,2019.0,http://www.carr-dominguez.com/tag/explore/postsprivacy.jsp,Medicine
660,d83a99bfb6f81565a186e0eb86858864568c1327,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",59 - 68,2017.0,http://www.benson-mosley.com/bloghomepage.htm,Computer Science
661,19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",35-110,2017.0,http://www.fields.org/main/explore/searchhome.html,Engineering
662,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",8872 - 8877,2017.0,http://www.roman-lambert.org/posts/blog/tagslogin.php,Physics
663,021865bb9fcc59814d2ce84d086554e5e0259779,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",19 - 36,2017.0,http://ashley.com/postscategory.php,Computer Science
664,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",1493-1507,2016.0,https://www.long.com/category/wp-contentsearch.htm,Computer Science
665,96f5a9360ccfd1c5c4210dc62948baac234c372d,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",52-113,2017.0,http://www.jordan.org/tags/tagabout.htm,Computer Science
666,97a3726b3f9395c8919c6271540d87d1c44e10ac,Deep feature synthesis: Towards automating data science endeavors,"In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.",1-10,2015.0,https://www.ramos.com/appfaq.html,Computer Science
667,6cb8c88e0fbc0676f89a55e1e0f399d2f1f85c86,A Data Science and Engineering Solution for Fast K-Means Clustering of Big Data,"With advances in technology, high volumes of a wide variety of valuable data of different veracity can be easily collected or generated at a high velocity in the current era of big data. Embedded in these big data are implicit, previously unknown and potentially useful information. Hence, fast and scalable big data science and engineering solutions that mine and discover knowledge from these big data are in demand. A popular and practical data mining task is to group similar data into clusters (i.e., clustering). To cluster very large data or big data, k-means based algorithms have been widely used. Although many existing k-means algorithms give quality results, they also suffer from some problems. For instance, there are risks associated with randomly selecting the k centroids, there is a tendency to produce roughly equal circular clusters, and the runtime complexity is very high. To deal with these problems, we present in this paper a big data science and engineering solution that applies heuristic prototype-based algorithm. Evaluation results show the efficiency and scalability of this solution.",925-932,2017.0,http://morgan.com/wp-content/exploreprivacy.jsp,Computer Science
668,3a8da09a87f06273c19fb61573b299388f8d1673,Data science vs. statistics: two cultures?,,117 - 138,2017.0,https://vance.com/categories/blog/wp-contentsearch.html,Mathematics
669,427a613d349d305726e1c4c7935b33c79de5850a,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",94-130,2016.0,https://jennings.org/search/list/tagsmain.jsp,Computer Science
670,9141efc0d91ab0bda9b264ff6d1df5f20fd1dbb0,Transdisciplinary Foundations of Geospatial Data Science,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.",395,2017.0,https://www.howell.biz/list/tag/postsauthor.htm,Computer Science
671,12e17fa5dd5715c563aadf705427da84817f100f,Data science: Data science tutorials,,53-142,2019.0,https://dunlap.info/explore/app/searchpost.html,Engineering
672,5b42e8ab6542fbbc11d84b07b34443a6853f96f1,Responsible Data Science,,311-313,2017.0,https://perry-scott.biz/appregister.php,Computer Science
673,af1fed4f5226292afffc0b736ddfa777acb8eb86,An Introduction to Data Science,"An Introduction to Data Scienceby Jeffrey S. Saltz and Jeffrey M. Stanton is an easy-to-read, gentle introduction for people with a wide range of backgrounds into the world of data science. Needing no prior coding experience or a deep understanding of statistics, this book uses the R programming language and RStudio platform to make data science welcoming and accessible for all learners. After introducing the basics of data science, the book builds on each previous concept to explain R programming from the ground up. Readers will learn essential skills in data science through demonstrations of how to use data to construct models, predict outcomes, and visualize data.",94-134,2017.0,http://www.floyd.org/tags/explorepost.htm,Computer Science
674,4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,Geographic Data Science,"Data science methods and approaches address all stages of transition from data to knowledge and action. Visualization of this data is essential for human understanding of the subject under study, analytical reasoning about it, and generating new knowledge. Geographic data science deals with data that incorporates spatial and, often, temporal elements. The articles selected for this special issue represent a mix of theoretical approaches and novel applications of geographic data science.",15-17,2017.0,http://www.king-miller.biz/tags/searchsearch.htm,Computer Science
675,f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",80-105,2015.0,https://gutierrez.com/exploreprivacy.jsp,Business
676,0a9b30386408595ff0b3155d4de4a56dad80a97b,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",2355-2361,2017.0,http://woodard.com/searchmain.htm,Computer Science
677,cd247b7830fb58c6f019a79ae9679251176e8342,Game Theory for Data Science: Eliciting Truthful Information,,86-144,2017.0,http://alexander.net/explore/blog/bloghomepage.html,Computer Science
678,dd340315c44a9c68391d8d2f600a0adc76b70c09,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",80-111,2017.0,https://mejia.info/main/tags/appmain.html,Computer Science
679,3af056b2aed8724dcddea074eb68aff6dd11c926,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",18-149,2017.0,http://joseph.info/category/exploreterms.asp,Biology
680,38fadf7c21c32b183fa3dcf32da1044e8441b813,The Data Science Handbook,"microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on",97-110,2017.0,https://www.rowe.net/tag/categories/categoriesregister.html,Computer Science
681,ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",44-133,2016.0,http://hart-vincent.com/wp-content/tagscategory.php,Computer Science
682,224eb3407b50533668b6c1caa55a720688b8b532,"A review and future direction of agile, business intelligence, analytics and data science",,700-710,2016.0,http://michael.com/main/searchabout.htm,Engineering
683,78a1e55ffb2ca07a612c033bb1b359f56f4ca9c3,Data Science,,20-127,2018.0,http://simpson-wilson.biz/wp-contentcategory.html,Technology
684,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",48-130,2015.0,https://www.gilbert.com/list/categories/exploreprivacy.html,Computer Science
685,31485e1213dd886fa2b668eefcd9b13533d8a9fe,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",364 - 373,2016.0,http://www.jones.com/category/postssearch.html,Computer Science
686,e1a1ad4025e2c7a82882c7389d937cbdfd10b799,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",8,2015.0,http://martin.net/appterms.asp,Computer Science
687,47134cbdfa7c44a55de5697a35b6652d0fcfee30,Data Science in Libraries,"EDITOR'S SUMMARY 
 
The new field of data science involves advanced knowledge in statistics and computer science, combined with copious amounts of data. A report from the Big Data and Research Initiative under the Obama Administration, The Federal Big Data Research and Development Strategic Plan, calls attention to the roles that librarians will play in the future of data science. However, there are skills and management gaps librarians face that inhibit their ability to move forward in data science. A number of educational programs are now offered to remedy this problem, such as the Data and Visualization Institute for Librarians from North Carolina State University, the volunteer-led Library Carpentry program, and most recently, the Data Sciences in Libraries Project, funded by the IMLS. This project aims to get librarians and library managers together to discuss the world of data science and create a roadmap for strategic planning.",33-35,2017.0,https://monroe-flynn.info/tag/posts/tagcategory.htm,Technology
688,0ec1992151e28c5678832c0923e56aeb58caad53,From Data Science to Value Creation,,173-181,2017.0,http://grant-powell.info/categories/tags/categoryhome.html,Computer Science
689,3b963487cbf944d51f33c2a0b41eb2aed7c68b89,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",31-121,2016.0,http://www.hodges-taylor.com/categoryterms.html,Medicine
690,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",443-448,2014.0,http://hughes.info/categories/appabout.html,Economics
691,ae118a88ada51dfdb2296cbaa948eb4a467942b6,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",31-128,2016.0,https://www.tucker.com/app/postsfaq.php,Computer Science
692,62806c60226d54ba1a4455bb1d7d2f034ef7c29a,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",48-109,2016.0,https://www.cervantes-lee.biz/posts/poststerms.php,Computer Science
693,217905fe7b549d5f3d45896072518b06adb8dd98,Data Science from Scratch: First Principles with Python,"Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today's messy glut of data holds answers to questions no one's even thought to ask. This book provides you with the know-how to dig those answers out.",19-112,2015.0,http://lopez.org/category/categorymain.php,Computer Science
694,52ff64f7f26b28447af255fedeb2216a70b48d66,Large Scale Distributed Data Science using Apache Spark,"Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",51-128,2015.0,https://friedman.biz/posts/app/categorieshomepage.asp,Computer Science
695,7c614fe86cc11c0430dd12b44e018e16e5dcf742,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",382 - 391,2016.0,https://finley-hall.info/postsregister.php,Medicine
696,b26c93eba9e1d99a5c99b07d2476714b386c4d54,Agile big data analytics: AnalyticsOps for data science,"Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process — completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics — to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.",2331-2339,2017.0,http://gonzalez.net/explore/taghomepage.html,Computer Science
697,4c05d4410c0023e14f2bb0cbcf7613468855430b,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",334 - 342,2015.0,http://www.moody-leonard.org/blogauthor.htm,Mathematics
698,c740a6816155fd123081d2f78926a0d3819926e7,LibGuides: *Data Science: Data Science Resources,"Data science resources, from finding ebooks and blogs, to finding raw datasets and analysis. Learn about data science resources, analysis, communities and data management. Also learn about hte datasets openly available and dataset purchase program.",43-111,2018.0,http://www.kent-henson.com/categorylogin.htm,Computer Science
699,37095b714dad5895d946b1f8435a3a38dee1be8b,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",,72-80,2014.0,http://www.mcintosh.com/app/wp-contentprivacy.php,Computer Science
700,0442b04b4e8741900b65de0721f0c3e152e044ef,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",171-193,2015.0,https://fleming.com/tags/tag/explorecategory.htm,Computer Science
701,85af074cd545b18e23793989d79b801fdbb31d85,Data Science,,64-113,2018.0,https://orozco-flowers.com/categorypost.html,Engineering
702,c694c6a685a067393204f36e21c8917a49f02a9b,Big data at work : the data science revolution and organizational psychology,"Foreword by Richard Klimoski 1. Building Understanding of the Data Science Revolution and IO Psychology Eden B. King, Scott Tonidandel, Jose M. Cortina, & Alexis A. Fink Part I: Big Issues for Big Data Methods 2. Big Data Platform Jacqueline Ryan 3. Statistical Methods for Big Data: A Scenic Tour Frederick L. Oswald & Dan J. Putka 4. Twitter Analysis: Methods for Data Management and a Word Count Dictionary to Measure City-Level Job Satisfaction Ivan Hernandez, Daniel A. Newman, & Gahyun Jeon 5. Data Visualization Evan F. Sinar 6. Sensing Big Data: Multimodal Information Interfaces for Exploration of Large Data Sets Jeffrey Stanton Part II: Big Ideas for Big Data in Organization 7. Implications of the Big Data Movement for the Advancement I-O Science and Practice Dan J. Putka & Frederick L. Oswald 8. Big Data in Talent Selection and Assessment A. James Illingworth, Michael Lippstreu, & Anne-Sophie Deprez-Sims 9. Big Data in Turnover/Retention John P. Hausknecht & Huisi (Jessica) Li 10. Using Big Data to Advance the Science of Team Effectiveness Steve W. J. Kozlowski, Georgia T. Chao, Chu-Hsiang (Daisy) Chang, & Rosemarie Fernandez 11. Using Big Data to Create Diversity and Inclusion in Organizations Whitney Botsford Morgan, Eric Dunleavy, & Peter D. DeVries 12. How Big Data Matters Richard A. Guzzo",47-103,2016.0,https://brown-rosario.net/category/listcategory.php,Psychology
703,9c1b9598f82f9ed7d75ef1a9e627496759aa2387,"Data Science, Predictive Analytics, and Big Data: A Revolution that Will Transform Supply Chain Design and Management","We illuminate the myriad of opportunities for research where supply chain management intersects with data science, predictive analytics, and big data, collectively referred to as DPB. We show that these terms are not only becoming popular but are also relevant to supply chain research and education. Data science requires both domain knowledge and a broad set of quantitative skills, but there is a dearth of literature on the topic and many questions. We call for research on skills that are needed by SCM data scientists and discuss how such skills and domain knowledge affect the effectiveness of a SCM data scientist. Such knowledge is crucial to developing future supply chain leaders. We propose definitions of data science and predictive analytics as applied to supply chain management. We examine possible applications of DPB in practice and provide examples of research questions from these applications, as well as examples of research questions employing DPB that stem from management theories. Finally, we propose specific steps interested researchers can take to respond to our call for research on the intersection of supply chain management and DPB.",17-113,2013.0,http://www.phelps.info/appterms.asp,Business
704,c04aaf36c8587e40747212e316d9bf44186ef64a,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",98-127,2016.0,https://campbell.info/main/categorieshomepage.asp,Computer Science
705,259d81ced1837bb74f3eeeb30ca3217d535e0c31,Introduction to HPC with MPI for Data Science,,3-259,2016.0,https://www.mccoy-moore.info/categories/mainregister.htm,Computer Science
706,405691093089ebcd1ea136a521c371534f85b863,Data Science,,24-136,2015.0,https://wilson-lucero.com/posts/list/tagauthor.htm,Computer Science
707,b9888bb70d6f246c7ffb53dcb9498bfafe113d8f,Thinking by classes in data science: the symbolic data analysis paradigm,"Data Science, considered as a science by itself, is in general terms, the extraction of knowledge from data. Symbolic data analysis (SDA) gives a new way of thinking in Data Science by extending the standard input to a set of classes of individual entities. Hence, classes of a given population are considered to be units of a higher level population to be studied. Such classes often represent the real units of interest. In order to take variability between the members of each class into account, classes are described by intervals, distributions, set of categories or numbers sometimes weighted and the like. In that way, we obtain new kinds of data, called ‘symbolic’ as they cannot be reduced to numbers without losing much information. The first step in SDA is to build the symbolic data table where the rows are classes and the variables can take symbolic values. The second step is to study and extract new knowledge from these new kinds of data by at least an extension of Computer Statistics and Data Mining to symbolic data. SDA is a new paradigm which opens up a vast domain of research and applications by giving complementary results to classical methods applied to standard data. SDA also gives answers to big data and complex data challenges as big data can be reduced and summarized by classes and as complex data with multiple unstructured data tables and unpaired variables can be transformed into a structured data table with paired symbolic‐valued variables. WIREs Comput Stat 2016, 8:172–205. doi: 10.1002/wics.1384",30-115,2016.0,https://mathews.info/listregister.php,Computer Science
708,071036abe55e7247d7e6ec28a4afc8ef2670f479,A Comparison of Open Source Tools for Data Science,"The next decade of competitive advantage revolves around the ability to make predictions and discover patterns in data. Data science is at the center of this revolution. Data science has been termed the sexiest job of the 21st century. Data science combines data mining, machine learning, and statistical methodologies to extract knowledge and leverage predictions from data. Given the need for data science in organizations, many small or medium organizations are not adequately funded to acquire expensive data science tools. Open source tools may provide the solution to this issue. While studies comparing open source tools for data mining or business intelligence exist, an update on the current state of the art is necessary. This work explores and compares common open source data science tools. Implications include an overview of the state of the art and knowledge for practitioners and academics to select an open source data science tool that suits the requirements of specific data science projects.",4-12,2016.0,http://www.ward.net/posts/appfaq.php,Computer Science
709,6b705d7ef453d42d87a9099b31344adad2367f40,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",620-626,2016.0,http://www.hicks.info/searchfaq.asp,Computer Science
710,ca9f74a1a7b69214c670202bb4f66eb16194f836,Datathons: An Experience Report of Data Hackathons for Data Science Education,Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.,75-139,2016.0,http://mcguire.com/blogmain.html,Computer Science
711,8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",345 - 356,2016.0,https://cox.com/mainlogin.jsp,Computer Science
712,dc86a7295d737fc2f195b67a273e90b549bd6272,Business Analytics and Data Science: Once Again?,,77 - 79,2016.0,http://www.byrd-parker.com/mainmain.html,Computer Science
713,12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",596-602,2016.0,https://www.roberts.com/tag/mainmain.asp,Engineering
714,3e209c705350761fe676ac330503e8662279fbf2,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",810-819,2015.0,http://www.torres.com/posts/category/listhomepage.html,Computer Science
715,6ca1476156926fe2664f6723c1281493426a593a,Data Science,"The Bachelor of Science in Data Science studies the collection, manipulation, storage, retrieval, and computational analysis of data in its various forms, including numeric, textual, image, and video data from small to large volumes. The program combines computer science, information science, mathematics, statistics, and probability theory into an integrated curriculum that prepares students for careers or graduate studies in big data analysis, data science, and data analytics. The coursework covers exploratory data analysis, data manipulation in a variety of programming languages, large-scale data storage, predictive analytics, machine learning, data mining, and information visualization and presentation. Data science has emerged as a discipline due to the confluence of two major events:",285-286,2017.0,http://www.cole.org/wp-content/wp-contentauthor.htm,Computer Science
716,2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.","
          249-66
        ",2015.0,http://www.woodward.net/category/categories/tagshomepage.html,Computer Science
717,659890e52fe234cde0e02a2305e213d3e8cb14b2,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",150 - 168,2015.0,https://jones.com/postshome.html,Engineering
718,f152a4008f114ac19076ee6b98d431268f4aea9e,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",48-136,2016.0,http://lang.com/mainpost.jsp,Computer Science
719,5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,Data science from scratch,"This is a first-principles-based, practical introduction to the fundamentals of data science aimed at the mathematically-comfortable reader with some programming skills. The book covers: The important parts of Python to know The important parts of Math / Probability / Statistics to know The basics of data science How commonly-used data science techniques work (learning by implementing them) What is Map-Reduce and how to do it in Python Other applications such as NLP, Network Analysis, and more",34-141,2015.0,http://morales-sosa.info/tags/taghome.php,Computer Science
720,8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,Coding the biodigital child: the biopolitics and pedagogic strategies of educational data science,"Abstract Educational data science is an emerging transdisciplinary field formed from an amalgamation of data science and elements of biological, psychological and neuroscientific knowledge about learning, or learning science. This article conceptualises educational data science as a biopolitical strategy focused on the evaluation and management of the corporeal, emotional and embrained lives of children. Such strategies are enacted through the development of new kinds of digitally-mediated ‘biopedagogies’ of body optimisation, ‘psychopedagogies’ of emotional maximisation, and ‘neuropedagogies’ of brain empowerment. The data practices, scientific knowledges, digital devices and pedagogies that constitute educational data science produce new systems of knowledge about the child that are consequential to their formation as ‘biodigital’ subjects, whose assumed qualities and capacities are defined through expert practices of biosensing, emotion analytics, and neurocomputation, combined with associated scientific knowledges. The article develops the concept of transcoding to account for the processes involved in the formation of the biodigital child.",401 - 416,2016.0,http://smith.com/blog/searchsearch.jsp,Sociology
721,e12b5363078a6d435bfba80e9d5cbab6b2cac897,Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",32-101,2015.0,http://cardenas-hale.biz/category/exploremain.htm,Art
722,49522df4fab1ebbeb831fc265196c2c129bf6087,Survey on data science with population-based algorithms,,1-20,2016.0,http://www.castro.com/search/tagscategory.htm,Computer Science
723,b0150dd118ebedbc3ece68726e065f9afaaf3b18,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",1-42,2016.0,http://www.meyer-brown.com/explore/categories/categoryterms.asp,Computer Science
724,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.","
          51-9
        ",2013.0,http://www.davis-murray.com/listabout.htm,Psychology
725,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,Teaching Data Science,,1947-1956,2016.0,https://www.hall.com/searchmain.asp,Computer Science
726,3eb5f2d152ead21ce528f9781c66197010eea3c8,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",10-20,2016.0,http://jensen.net/mainprivacy.htm,Medicine
727,469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",1 - 6,2016.0,https://www.thomas.net/tags/blogterms.html,Engineering
728,061c3291d817076dbb3e5a41c51f99800a390e94,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",74-149,2015.0,http://davis-watkins.com/searchterms.php,Computer Science
729,e926ef463fa96b3d06a321fcbcccdab9ff5f3da0,Statistics and computing: the genesis of data science,,705 - 711,2015.0,https://www.kim.com/main/wp-contentabout.html,Computer Science
730,de84e808462b8240c75987364a6d518eff7d8813,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",84-110,2015.0,http://www.alexander-ramsey.com/postsindex.php,Computer Science
731,8f6a4609531ca9ff35915c32dae5cd146fc57c40,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",1-18,2015.0,https://aguirre.org/blog/categoryhome.jsp,Computer Science
732,b885916e9af51010ca7ebafbc9270f0e5e207b38,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",304–310,2015.0,https://roberts.org/posts/wp-contentfaq.jsp,Medicine
733,e78d7fa72a5dbe5f3bc93f6e200826004f23530b,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,66-75,2016.0,https://www.barrett.net/listcategory.jsp,Computer Science
734,fa15d626d8905d08953abe646a75a31417ad61fa,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",44-110,2016.0,https://www.brock.org/category/categorieshomepage.htm,Computer Science
735,d4dd5eb7a6081c0857ddae0caca1fdcf288553e1,Gait biomechanics in the era of data science.,,"
          3759-3761
        ",2016.0,http://wang.com/app/searchhomepage.php,Engineering
736,bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",1-16,2016.0,http://anderson.com/posts/blogfaq.asp,Computer Science
737,d343c9823bcacf31ea4aca105d0366f3f18a75e5,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",102-107,2016.0,http://www.ortiz.com/mainauthor.html,Computer Science
738,def43235dba7eb98659fb8879fa9d27695029df2,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",84-89,2016.0,http://www.cortez-moran.com/mainauthor.html,Computer Science
739,b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",100-144,2016.0,https://www.trujillo.com/categories/tag/appauthor.asp,Computer Science
740,23a57b1e2beb4235d2020ed57f484c947e3d0816,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.","
          85-99
        ",2013.0,https://estrada.com/searchlogin.htm,Computer Science
741,682b105746238d3c39bd4f6cd0baa375dc0c2534,Perspectives on Data Science for Software Engineering,,3-6,2016.0,http://www.adams.com/category/categoryauthor.php,Computer Science
742,520515cfffcd2f439469398d7c959f8baa9ccc8b,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",468-477,2015.0,https://dennis-boone.com/bloghomepage.html,Computer Science
743,57039a11d9fb423595a4e16129f7cc7f3ff2cac7,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",69-149,2016.0,http://www.brown.com/category/app/mainpost.htm,Medicine
744,5ea0821f37481dafab363a47bf9b904e986f5a20,Data science and analytics: a new era,,1-2,2016.0,https://velasquez.com/listindex.html,Computer Science
745,04831fedd16110da4cbd0798d16e21fbbc34ad06,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",232-261,2015.0,http://www.brown-rodriguez.com/blog/main/wp-contentlogin.html,Computer Science
746,57c82a005ae353f4683938b15a52e1b0561f6e43,"R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",22-150,2014.0,http://thomas.com/tagshomepage.htm,Computer Science
747,44a840a63fc0c3edda389bbaf9c10c94e91fbd5a,Data Science,,99-117,2017.0,https://www.payne.info/tagsfaq.html,Computer Science
748,dd153ebd44a07dde2259c22d43bb9cd18db44d2a,Modelling and Simulation in Materials Science and Engineering Visualization and analysis of atomistic simulation data with OVITO – the Open Visualization Tool,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/. (Some figures in this article are in colour only in the electronic version)",76-117,2009.0,http://www.morris.com/posts/appauthor.htm,Technology
749,1a95f1e8ff32488f228a25764af64531cb758ff0,Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters,,1-19,2014.0,http://castillo-hill.org/explore/categoriesterms.php,Engineering
750,6f989651c4f592613e92c9e37a8c4ac205998cfe,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",343 - 353,2014.0,https://www.mcmahon.com/blogregister.asp,Mathematics
751,9ba08d45d60130c7e5880f63a980b185a86e177c,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.","
          155-163
        ",2014.0,http://bowen.com/tags/listpost.html,Computer Science
752,c7d7d579d94b7fc67c75b68361e01ba8f59b1d40,Intelligent services for Big Data science,,267-281,2014.0,https://sanchez.com/wp-contentabout.php,Computer Science
753,92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",99-129,2014.0,https://www.williams.com/main/blog/postscategory.htm,Computer Science
754,0040c830969302a8c88c0c083aee5051e405bfe5,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",38 - 51,2014.0,https://www.mcgee.com/postsauthor.jsp,Sociology
755,516a53c59a53b0a471cd8a277b229925e0582114,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",61-140,2014.0,http://manning.com/app/explorefaq.htm,Computer Science
756,f707f6d7c3f874cb1a8aa961a50e50706731cd2d,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",100-128,2014.0,https://www.guerrero.com/tag/app/listregister.htm,Computer Science
757,b9111489ec08b50bc573982ede11f5bc2d7a4e88,Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",23-104,2018.0,http://www.hanson.biz/tag/categories/searchcategory.asp,Mathematics
758,9d653160d048eecf1a8138407994bfc69952324b,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",64-106,2014.0,https://www.torres.net/blogsearch.php,Computer Science
759,0e341b2a181f71dea088dbba800e70262f91a79e,"Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,32-148,2000.0,http://kelly.biz/posts/wp-contentindex.htm,Computer Science
760,5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",557-574,2014.0,https://www.rodriguez.biz/posts/search/tagscategory.php,Computer Science
761,0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",235-242,2014.0,https://gray-ayers.biz/tag/categoriesregister.asp,Computer Science
762,92b68d5a59262971d0f4a563c6abe1be6f2dab56,Data Science,,3-251,2016.0,http://gonzales.com/categories/wp-contenthome.htm,Computer Science
763,5bbb90ae23803b8bb115d5d7f60c8defc5376e2a,"Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication","Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.
 In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",192,2014.0,http://www.barker-shields.com/blog/exploreabout.htm,Computer Science
764,f3eda875e14bf933759f3b777131a4a9973537b4,"Data-driven science and engineering: machine learning, dynamical systems, and control",,21-131,2019.0,https://www.salinas-myers.com/list/explore/listfaq.php,Computer Science
765,3d7fcd1399573fb5cb455de6f85f149e0ab53828,The Science of Data Science,,"
          68-70
        ",2014.0,https://gill.info/list/main/tagscategory.htm,Medicine
766,1163c2996dfd0a46639b094e34ad783e969a0692,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,81-106,2012.0,https://macdonald.com/listabout.html,Computer Science
767,3858f600d0187c28f381b034a70226213e82a54e,Network analysis of multivariate data in psychological science,,18-148,2021.0,http://www.copeland.com/tags/tags/searchlogin.php,Computer Science
768,3402835f33e3e1342eb86b4d13907e3c9121c82b,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",58-107,2013.0,https://www.gates.org/blog/search/categoriessearch.asp,Computer Science
769,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",36-140,2013.0,https://harrington-medina.info/tagssearch.html,Computer Science
770,0636653b82e152ba99b1d921b0aa2798aa845d1e,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Abstract Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",377-386,2020.0,https://www.price-freeman.info/appfaq.php,Computer Science
771,cbf6a6d8fff87b74f36c5e4ede09f55e7a71506c,Numerical data and functional relationships in science and technology,,86-143,1969.0,https://brooks-cross.com/tags/searchregister.htm,Materials Science
772,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,ImageJ2: ImageJ for the next generation of scientific image data,,23-130,2017.0,http://www.payne.com/tagshomepage.html,Computer Science
773,e9931ea8ae9b8db38b519ef9ae32ec41a06d8445,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",27-104,2013.0,https://rowe.com/posts/postscategory.html,Computer Science
774,379e9576dea9690cf88d9132287edbefb7626232,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",80-143,2013.0,http://www.gonzalez.biz/tags/apphomepage.jsp,Engineering
775,6a324214a73610d8819e004e7ebd7dd23107d1f8,Computing: A vision for data science,,473-475,2013.0,http://www.collins-richards.org/posts/appindex.html,Medicine
776,010a8ed71c6a80c2c02c7f55e1718151f91ff35a,Web of Science as a data source for research on scientific and scholarly activity,"Abstract Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",363-376,2020.0,https://gallegos.net/app/postssearch.jsp,Computer Science
777,dd4f9aa21cf34994c07d2d74fc7f633194564224,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",1-22,2020.0,https://www.hicks-ford.com/app/search/listhome.html,Computer Science
778,425744cb05e854071d06af0da2b8ef2d677f33d5,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",89-111,2018.0,https://www.mccarthy.com/explore/category/searchterms.html,Geology
779,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,Data-Driven Science and Engineering,,88-148,2019.0,https://jackson.com/list/wp-contentauthor.php,Computer Science
780,46d55edae7cf5f065bb037462a7951c220f42618,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",8410 - 8415,2014.0,https://www.gibbs.com/wp-contentsearch.html,Medicine
781,b1691f718aedc8e6b31105c50773761c753676f4,“Invisible Sportswomen”: The Sex Data Gap in Sport and Exercise Science Research,"This study aimed to conduct an updated exploration of the ratio of male and female participants in sport and exercise science research. Publications involving humans were examined from The European Journal of Sports Science, Medicine & Science in Sport & Exercise, The Journal of Sport Science & Medicine, The Journal of Physiology, The American Journal of Sports Medicine, and The British Journal of Sports Medicine, 2014–2020. The total number of participants, the number of male and female participants, the title, and the topic, were recorded for each publication. Data were expressed in frequencies and percentages. Chi-square analyses were used to assess the differences in frequencies in each of the journals. About 5,261 publications and 12,511,386 participants were included in the analyses. Sixty-three percentage of publications included both males and females, 31% included males only, and 6% included females only (p < .0001). When analyzing participants included in all journals, a total of 8,253,236 (66%) were male and 4,254,445 (34%) were female (p < .0001). Females remain significantly underrepresented within sport and exercise science research. Therefore, at present most conclusions made from sport and exercise science research might only be applicable to one sex. As such, researchers and practitioners should be aware of the ongoing sex data gap within the current literature, and future research should address this.",65-104,2021.0,https://www.knight.net/tags/blogauthor.html,Technology
782,abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",65-130,2019.0,https://www.sanders-french.com/categorycategory.html,Physics
783,1110da1c238a7b09258136e7a2e7d558fb16f272,TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology,,43-102,2019.0,https://williams-moore.com/list/postscategory.jsp,Computer Science
784,ee5825861645ec9b9d11a2882f3aa15ec9e6e4dd,"ENDF/B-VII.1 Nuclear Data for Science and Technology: Cross Sections, Covariances, Fission Product Yields and Decay Data",,2887-2996,2011.0,http://www.morris.org/tags/tagindex.htm,Physics
785,e60d9464935582cda41becd7c1455c09392a2a93,The Science of Visual Data Communication: What Works,"Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust—especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.",110 - 161,2021.0,https://www.oneal.com/categoryhome.htm,Medicine
786,ee013b1477e8f81cb5c66a9a93a342281f740042,Assessing data quality in citizen science (preprint),"Ecological and environmental citizen science projects have enormous potential to advance science, influence policy, and guide resource management by producing datasets that are otherwise infeasible to generate. This potential can only be realized, though, if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen science dataset should therefore be judged individually, according to project design and application, rather than assumed to be substandard simply because volunteers generated it.",64-108,2016.0,http://nguyen.com/postsabout.asp,Computer Science
787,40de1c316a7f4de8e547a717c905013642378996,The Critical Importance of Citizen Science Data,"Citizen science is an important vehicle for democratizing science and promoting the goal of universal and equitable access to scientific data and information. Data generated by citizen science groups have become an increasingly important source for scientists, applied users and those pursuing the 2030 Agenda for Sustainable Development. Citizen science data are used extensively in studies of biodiversity and pollution; crowdsourced data are being used by UN operational agencies for humanitarian activities; and citizen scientists are providing data relevant to monitoring the sustainable development goals (SDGs). This article provides an International Science Council (ISC) perspective on citizen science data generating activities in support of the 2030 Agenda and on needed improvements to the citizen science community's data stewardship practices for the benefit of science and society by presenting results of research undertaken by an ISC-sponsored Task Group.",62-121,2021.0,https://allen.com/app/categorylogin.htm,Technology
788,e257edf34abd9a191fea1023a423abb497cca70f,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",39-145,2012.0,http://www.gibson.com/wp-content/wp-contentcategory.html,Computer Science
789,00a4bdc5158945a0b9463a29da4810838e474875,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",053208,2016.0,http://smith.biz/posts/mainlogin.asp,Materials Science
790,7ec947261f5a3eabdaddb8e53d58a36b986c4e71,ENDF/B-VII.0: Next Generation Evaluated Nuclear Data Library for Nuclear Science and Technology,,2931-3060,2006.0,https://www.hill.net/categoriesprivacy.asp,Physics
791,8e981ddb4877615f7d5f944a8d64789d1388ee87,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",30-136,2008.0,https://www.cox.com/main/tagprivacy.html,Physics
792,7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",66-135,2019.0,http://www.smith-parker.info/app/exploreabout.php,Psychology
793,81e1dbe1e8152d0ddbf89e861468d799dbebe367,Social media data for conservation science: A methodological overview,,63-139,2019.0,https://smith-moore.com/category/posts/categorymain.htm,Computer Science
794,40b2324cde863db7670178f0151fae400a9a2b93,Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation,"We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that “multiple imputation” is a superior approach to the problem of missing data scattered through one’s explanatory and dependent variables than the methods currently used in applied data analysis. The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise. We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples. Finally, we offer easy-to-use software that implements all methods discussed.",49 - 69,2001.0,http://gillespie-vega.net/main/explore/exploremain.htm,Technology
795,adc22a722f2ad1d972de507779041e340f20a6a2,Kadi4Mat: A Research Data Infrastructure for Materials Science,"The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license.",8,2021.0,https://martin-mason.com/main/categories/tagfaq.htm,Computer Science
796,ca5e7580993170b1fe621bc16383ad2dfa6803b5,Utilization of text mining as a big data analysis tool for food science and nutrition.,"Big data analysis has found applications in many industries due to its ability to turn huge amounts of data into insights for informed business and operational decisions. Advanced data mining techniques have been applied in many sectors of supply chains in the food industry. However, the previous work has mainly focused on the analysis of instrument-generated data such as those from hyperspectral imaging, spectroscopy, and biometric receptors. The importance of digital text data in the food and nutrition has only recently gained attention due to advancements in big data analytics. The purpose of this review is to provide an overview of the data sources, computational methods, and applications of text data in the food industry. Text mining techniques such as word-level analysis (e.g., frequency analysis), word association analysis (e.g., network analysis), and advanced techniques (e.g., text classification, text clustering, topic modeling, information retrieval, and sentiment analysis) will be discussed. Applications of text data analysis will be illustrated with respect to food safety and food fraud surveillance, dietary pattern characterization, consumer-opinion mining, new-product development, food knowledge discovery, food supply-chain management, and online food services. The goal is to provide insights for intelligent decision-making to improve food production, food safety, and human nutrition.","
          875-894
        ",2020.0,https://www.frost-adams.net/search/app/postshomepage.php,Medicine
797,fff51615943e08d05080682009c9c656321ef0b2,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",676-682,2018.0,http://hull.com/blog/blogregister.php,Materials Science
798,0e00f0dbfc381661826f8ddbafe73e33bcfe040f,Using Semistructured Surveys to Improve Citizen Science Data for Monitoring Biodiversity,"Abstract Biodiversity is being lost at an unprecedented rate, and monitoring is crucial for understanding the causal drivers and assessing solutions. Most biodiversity monitoring data are collected by volunteers through citizen science projects, and often crucial information is lacking to account for the inevitable biases that observers introduce during data collection. We contend that citizen science projects intended to support biodiversity monitoring must gather information about the observation process as well as species occurrence. We illustrate this using eBird, a global citizen science project that collects information on bird occurrences as well as vital contextual information on the observation process while maintaining broad participation. Our fundamental argument is that regardless of what species are being monitored, when citizen science projects collect a small set of basic information about how participants make their observations, the scientific value of the data collected will be dramatically improved.",170 - 179,2019.0,http://www.baker.com/app/blogterms.htm,Geography
799,e27acaf97f5b2eae4257bb5d8278fbe0e6405c39,Creating the CIPRES Science Gateway for inference of large phylogenetic trees,"Understanding the evolutionary history of living organisms is a central problem in biology. Until recently the ability to infer evolutionary relationships was limited by the amount of DNA sequence data available, but new DNA sequencing technologies have largely removed this limitation. As a result, DNA sequence data are readily available or obtainable for a wide spectrum of organisms, thus creating an unprecedented opportunity to explore evolutionary relationships broadly and deeply across the Tree of Life. Unfortunately, the algorithms used to infer evolutionary relationships are NP-hard, so the dramatic increase in available DNA sequence data has created a commensurate increase in the need for access to powerful computational resources. Local laptop or desktop machines are no longer viable for analysis of the larger data sets available today, and progress in the field relies upon access to large, scalable high-performance computing resources. This paper describes development of the CIPRES Science Gateway, a web portal designed to provide researchers with transparent access to the fastest available community codes for inference of phylogenetic relationships, and implementation of these codes on scalable computational resources. Meeting the needs of the community has included developing infrastructure to provide access, working with the community to improve existing community codes, developing infrastructure to insure the portal is scalable to the entire systematics community, and adopting strategies that make the project sustainable by the community. The CIPRES Science Gateway has allowed more than 1800 unique users to run jobs that required 2.5 million Service Units since its release in December 2009. (A Service Unit is a CPU-hour at unit priority).",1-8,2010.0,https://www.smith.com/wp-contentprivacy.html,Technology
800,8a9f26a4cee210e51c96f4016737605e31d490ee,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning (ML) to materials science problems requires enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific ML models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with ML models and how users can access those capabilities through web and programmatic interfaces.",1125-1133,2019.0,http://fry.net/app/tagcategory.html,Physics
801,3f809897b51d824846cf5a56f2a7b4292f7bc4a4,Materials science: Share corrosion data,,441-442,2015.0,https://knox.com/appmain.html,Medicine
802,3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",48-117,2020.0,http://marsh-mitchell.info/categories/explore/appfaq.html,Computer Science
803,652c77a90d84df639622efdc9cd7475e96a248c9,Data-driven modeling and learning in science and engineering,,69-103,2019.0,http://bennett.org/tag/main/categoryauthor.html,Computer Science
804,4a6e74d4bf4fd0106891e5518692a77c7aa8811d,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",52-104,2021.0,https://www.williams-vazquez.com/categories/blogfaq.php,Computer Science
805,6a697a4b3bdbbfb7681d9f9a518fc0be73744037,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.","
          105503
        ",2014.0,https://www.gonzalez.biz/explore/main/blogindex.php,Medicine
806,13c56c63385e84bca0e045133afe2c0a5d25d2d4,Improving big citizen science data: Moving beyond haphazard sampling,"Citizen science is mainstream: millions of people contribute data to a growing array of citizen science projects annually, forming massive datasets that will drive research for years to come. Many citizen science projects implement a “leaderboard” framework, ranking the contributions based on number of records or species, encouraging further participation. But is every data point equally “valuable?” Citizen scientists collect data with distinct spatial and temporal biases, leading to unfortunate gaps and redundancies, which create statistical and informational problems for downstream analyses. Up to this point, the haphazard structure of the data has been seen as an unfortunate but unchangeable aspect of citizen science data. However, we argue here that this issue can actually be addressed: we provide a very simple, tractable framework that could be adapted by broadscale citizen science projects to allow citizen scientists to optimize the marginal value of their efforts, increasing the overall collective knowledge.",40-128,2019.0,http://www.scott.com/blog/postsabout.htm,Medicine
807,4e2f43dab69d690dc86422949e410ebf37f522d4,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.","
          658-676
        ",2010.0,http://www.hernandez-ross.com/tag/listauthor.html,Computer Science
808,22737046fbbe822deaaffddddb8f16be076d3f95,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",15-127,2019.0,https://moore-hartman.com/app/mainregister.htm,Medicine
809,0db731c99879bb74c3850c53923d1df2c510f8c3,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",253-264,2003.0,https://williams.com/wp-content/tagterms.asp,Environmental Science
810,f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",838 - 854,2016.0,https://www.hopkins.com/explorecategory.jsp,Medicine
811,4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",477 - 480,2017.0,http://fox-ramirez.com/tagsprivacy.htm,Medicine
812,2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",122-130,2020.0,https://horton-buchanan.org/search/postshome.htm,Computer Science
813,d1db1c83c64f556fd4005cc12bddf7963f82a77f,Open science resources for the discovery and analysis of Tara Oceans data,,64-126,2015.0,https://www.mcgee-cox.com/tagcategory.html,Geography
814,bd8a307efcffbf57d2e5c3c23577de44d883d865,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",25-30,2016.0,http://www.bentley-lewis.info/wp-content/main/mainterms.htm,Computer Science
815,02a9428b5b28d85ea330033fb990dc10cd15cc4e,Occupancy models for citizen‐science data,"Large‐scale citizen‐science projects, such as atlases of species distribution, are an important source of data for macroecological research, for understanding the effects of climate change and other drivers on biodiversity, and for more applied conservation tasks, such as early‐warning systems for biodiversity loss. However, citizen‐science data are challenging to analyse because the observation process has to be taken into account. Typically, the observation process leads to heterogeneous and non‐random sampling, false absences, false detections, and spatial correlations in the data. Increasingly, occupancy models are being used to analyse atlas data. We advocate a dual approach to strengthen inference from citizen science data for the questions the programme is intended to address: (a) the survey design should be chosen with a particular set of questions and associated analysis strategy in mind and (b) the statistical methods should be tailored not only to those questions but also to the specific characteristics of the data. We review the consequences of particular survey design choices that typically need to be made in atlas‐style citizen‐science projects. These include spatial resolution of the sampling units, allocation of effort in space, and collection of information about the observation process. On the analysis side, we review extensions of the basic occupancy models that are frequently necessary with atlas data, including methods for dealing with heterogeneity, non‐independent detections, false detections, and violation of the closure assumption. New technologies, such as cell‐phone apps and fixed remote detection devices, are revolutionizing citizen‐science projects. There is an opportunity to maximize the usefulness of the resulting datasets if the protocols are rooted in robust statistical designs and data analysis issues are being considered. Our review provides guidelines for designing new projects and an overview of the current methods that can be used to analyse data from such projects.",21 - 8,2019.0,http://martinez.com/tagsmain.htm,Computer Science
816,3e02906da7498b5fdbc6f0eea4b6bb9f2d86dd00,ON PATIENT FLOW IN HOSPITALS: A DATA-BASED QUEUEING-SCIENCE PERSPECTIVE,"Hospitals are complex systems with essential societal benefits and huge mounting costs. These costs are exacerbated by inefficiencies in hospital processes, which are often manifested by congestion and long delays in patient care. Thus, a queueing-network view of patient flow in hospitals is natural for studying and improving its performance. The goal of our research is to explore patient flow data through the lens of a queueing scientist. The means is exploratory data analysis (EDA) in a large Israeli hospital, which reveals important features that are not readily explainable by existing models. Questions raised by our EDA include: Can a simple (parsimonious) queueing model usefully capture the complex operational reality of the Emergency Department (ED)? What time scales and operational regimes are relevant for modeling patient length of stay in the Internal Wards (IWs)? How do protocols of patient transfer between the ED and the IWs influence patient delay, workload division and fairness? EDA also unde...",146-194,2015.0,http://www.wells-anderson.com/category/tagmain.jsp,Medicine
817,2660fbc3b666145a87f05de10066fc2a3e7467dd,The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",93-149,2016.0,https://berg-reyes.com/list/explorelogin.jsp,Computer Science
818,9445423239efb633f5c15791a7abe352199ce678,General Data Protection Regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",28-139,2018.0,http://hendrix.com/wp-contentmain.asp,Business
819,51995dc568874ea34911833355234b1f696dacfc,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",1 - 40,2017.0,https://middleton.com/tag/app/searchterms.htm,Engineering
820,87f7c170aecf8f3465b26a11b9a384fef934337b,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",82-146,2017.0,https://www.fitzpatrick.com/wp-contentregister.php,Computer Science
821,2809d4876e34b8c64fc1783fe6a0a278770505b0,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",31-36,2005.0,https://solomon.net/search/categorieslogin.php,Computer Science
822,439ede62248e5f6202982afead02b33d3feffae7,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",e71 - e71,2015.0,http://brown.org/categories/category/wp-contentcategory.php,Medicine
823,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,The role of administrative data in the big data revolution in social science research.,,"
          1-12
        ",2016.0,https://www.foster.com/categories/tags/searchabout.html,Computer Science
824,33aeb033401ec748633bdd5b806db4f58288ee69,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",278-290,2017.0,https://mcconnell-callahan.net/exploreindex.jsp,Sociology
825,16f4135a229c79e60fa25259100c8cdcedfab8cc,Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",56-143,2016.0,https://duarte-miller.com/posts/list/searchsearch.htm,Political Science
826,233e702fa7ccfd55061680e3af9bd2f7efe5e08f,Science of Science,"The whys and wherefores of SciSci The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science—from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole. Science, this issue p. eaao0185 BACKGROUND The increasing availability of digital data on scholarly inputs and outputs—from research funding, productivity, and collaboration to paper citations and scientist mobility—offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data–based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems. ADVANCES Science can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact. OUTLOOK SciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science. ILLUSTRATION: NICOLE SAMAY Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.",1-2,2018.0,http://mason.com/categories/listauthor.asp,Sociology
827,362f50f59a280d7cc526fb626fdf44ad382cee57,The journal coverage of Web of Science and Scopus: a comparative analysis,,213 - 228,2015.0,http://www.franco.com/app/category/appabout.html,Computer Science
828,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,Deep learning applications and challenges in big data analytics,,66-149,2015.0,https://harris.com/posts/mainprivacy.htm,Computer Science
829,08a2ef1648fa5ea539ebe1718da577dc79124a21,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",63,2015.0,http://hall.com/wp-content/posts/explorepost.html,Sociology
830,06a81f63fc4ccfcf02934647a7c17454b91853b0,Machine Learning - The Art and Science of Algorithms that Make Sense of Data,"As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.","I-XVII, 1-396",2012.0,http://butler.com/posts/postsindex.htm,Computer Science
831,30d6f200f8b4bae78dbb4f69f1730bcad131d523,The Materials Data Facility: Data Services to Advance Materials Science Research,,2045-2052,2016.0,http://www.smith-harmon.com/search/postssearch.asp,Engineering
832,ad3d83248eae66580d4deada76e72e3be9a9b44c,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",66-73,2014.0,http://wong.biz/main/categoryhomepage.htm,Biology
833,391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",97-107,2016.0,http://www.castro.com/posts/explorefaq.jsp,Computer Science
834,54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",34-124,2014.0,https://johnson.biz/main/categoriesregister.jsp,Biology
835,accb5390f75ef6daa91cf5441333a7ebcc42a41f,Citizen science in environmental and ecological sciences,,81-135,2022.0,http://www.cook-peterson.com/tags/tagslogin.jsp,Technology
836,16cecb0173adc68762b6e70daecb25089a5a6b6a,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,,223 - 226,2014.0,http://www.jones.com/listregister.jsp,Medicine
837,a461233e56079fc5af6e48d75f38be8c9ff87c1e,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",17-122,2021.0,https://www.gomez-henderson.com/categoryfaq.php,Medicine
838,d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,Machine learning for data-driven discovery in solid Earth geoscience,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",16-109,2019.0,https://www.cross.com/main/blog/appabout.asp,Medicine
839,c36991759325bedd19f69264f72d1cbf59a6158c,Data Mining: Concepts and Techniques,"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",75-150,2000.0,http://lopez-shah.org/categoryregister.php,Computer Science
840,8d76672d52622d9c45014d630717ce911d1292ba,Bayesian Data Analysis,"Inference is the process of going from observed effects to underlying causes, and is the inverse process to deduction. Whereas deduction is exact, inference is imprecise, and necessarily probabilistic. Inference is the basis of science: we are always faced with observations we would like to explain in terms of underlying physical causes. Bayesian inference is an approach to the problem based on an identity in conditional probability (Bayes’s theorem). Notable Bayesians have included Pierre-Simon Laplace (who inferred the mass of Saturn from contemporary observations using Bayesian methods, and obtained a value consistent with modern estimates), the economist John Maynard Keynes, and the applied mathematician and geophysicist Harold Jeffreys. Bayesian inference has at times been controversial, because of its incorporation of subjective prior information into the process of inference. Historically the Bayesian approach was referred to as “subjective probability.” In recent decades there has",121-142,2010.0,http://www.peck.com/search/tagterms.html,Computer Science
841,969f983d000ac68ca77548b5bba2e8d1b89086c4,Materials science with large-scale data and informatics: Unlocking new opportunities,"Universal access to abundant scientific data, and the software to analyze the data at scale, could fundamentally transform the field of materials science. Today, the materials community faces serious challenges to bringing about this data-accelerated research paradigm, including diversity of research areas within materials, lack of data standards, and missing incentives for sharing, among others. Nonetheless, the landscape is rapidly changing in ways that should benefit the entire materials research enterprise. We provide an overview of the current state of the materials data and informatics landscape, highlighting a few selected efforts that make more data freely available and useful to materials researchers.",399-409,2016.0,https://www.gomez.com/categorycategory.htm,Technology
842,2f35b305b6c56046b631b0cdb6d2f08e4ee577a7,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,,433 - 442,2016.0,https://www.marshall-jones.com/explorepost.asp,Computer Science
843,b48a917258f4e7e2b78a41289d005513db1de8c9,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",147,2019.0,https://martin-ford.net/appauthor.htm,Computer Science
844,3aa1b70fdc97ae96091c5fb39cd911015ac5253e,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",129-151,2006.0,http://www.perez-nguyen.biz/main/explore/searchprivacy.html,Computer Science
845,88bcdfd021d935a28f245e178792207881b14794,Learning from Imbalanced Data Sets,,1-377,2018.0,http://www.gregory-humphrey.info/apppost.html,Computer Science
846,9b18fbe281496ad72bdd18e0a5883d235ebdfd87,"Biolink Model: A universal schema for knowledge graphs in clinical, biomedical, and translational science","Within clinical, biomedical, and translational science, an increasing number of projects are adopting graphs for knowledge representation. Graph‐based data models elucidate the interconnectedness among core biomedical concepts, enable data structures to be easily updated, and support intuitive queries, visualizations, and inference algorithms. However, knowledge discovery across these “knowledge graphs” (KGs) has remained difficult. Data set heterogeneity and complexity; the proliferation of ad hoc data formats; poor compliance with guidelines on findability, accessibility, interoperability, and reusability; and, in particular, the lack of a universally accepted, open‐access model for standardization across biomedical KGs has left the task of reconciling data sources to downstream consumers. Biolink Model is an open‐source data model that can be used to formalize the relationships between data structures in translational science. It incorporates object‐oriented classification and graph‐oriented features. The core of the model is a set of hierarchical, interconnected classes (or categories) and relationships between them (or predicates) representing biomedical entities such as gene, disease, chemical, anatomic structure, and phenotype. The model provides class and edge attributes and associations that guide how entities should relate to one another. Here, we highlight the need for a standardized data model for KGs, describe Biolink Model, and compare it with other models. We demonstrate the utility of Biolink Model in various initiatives, including the Biomedical Data Translator Consortium and the Monarch Initiative, and show how it has supported easier integration and interoperability of biomedical KGs, bringing together knowledge from multiple sources and helping to realize the goals of translational science.",1848 - 1855,2022.0,http://adams.org/mainfaq.asp,Computer Science
847,7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",30-106,2020.0,https://www.juarez.net/poststerms.html,Computer Science
848,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",1 - 49,1993.0,http://www.knapp-gonzalez.com/categoriessearch.htm,Computer Science
849,efa5558bddd68abe4adc81adbbef6f739e648392,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",85-129,2015.0,http://www.thompson.com/tag/categoryfaq.html,Medicine
850,b7d5dda24d0c540929cd58b0226eac8a85e9769b,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",549-560,1998.0,http://www.roach.com/main/app/bloghomepage.html,Mathematics
851,5703617b9d9d40e90b6c8ffa21a52734d9822d60,Defining Computational Thinking for Mathematics and Science Classrooms,,127-147,2016.0,http://www.cuevas-edwards.com/wp-content/search/tagsterms.asp,Computer Science
852,da619a6c524f5ab800b44c8728db3cef3d3b25d9,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",94-143,2014.0,http://www.sims.info/category/listpost.php,Sociology
853,7fc70d4cc5118fdbc8e8807979eae8b61948ff91,"The elements of statistical learning: data mining, inference and prediction",,83-85,2005.0,https://jacobson-simpson.org/posts/wp-content/blogterms.htm,Mathematics
854,88dde718acafeaedbe9768883d274a81fd8313d7,DLHub: Model and Data Serving for Science,"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.",283-292,2018.0,http://www.williams.com/tagsterms.html,Computer Science
855,0870c1ea2b7d5a515c7b5b954f1433b379fe1e02,Principles and methods of scaling geospatial Earth science data,,57-148,2019.0,https://www.smith.com/search/blog/listhomepage.php,Computer Science
856,f2b66923db74a16169d040a51ada555d5b6f8851,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",78-106,2014.0,https://white-reed.biz/blog/app/categoryindex.php,Computer Science
857,fb3140c9766a5bc92400ac8ce9d48a4272bba69e,A New Kind of Science,"nationwide data set of losses from 1975 to 1998 was compiled to assess the trends. Temporal patterns of deaths and injuries, monetary damages, and—in some cases—the number of events are systematically examined by year in chapter 5, and the authors undertake a systematic spatial assessment of the statewide totals in chapter 6. Explanations for some of the patterns are offered, particularly for the most significant disasters and for the states with most events or the greatest losses. Further refinement and evaluation of patterns of economic losses and death are undertaken by normalizing losses by population, land area, and gross domestic product (GDP). The authors advance the discussion from simple descriptions of loss patterns to explanations of the patterns of disaster-loss burden, and some surprises emerge from the arithmetic. For instance, North Dakota, Iowa, andMississippi not only suffered the greatest monetary losses per capita during the period, but also suffered the greatest losses of property and crops compared to their state GDP!For afinal analysis, the authors created an overall hazard score (averaged proportion of the states’ contributions to the national totals of events, deaths, and damages) and used it to rank the states. Using this ranking, states were assigned to categories of ‘‘proneness,’’ from highest (Florida, Texas, andCalifornia) to lowest (Rhode Island, Delaware, Alaska and other small or lightly populated states). The conclusion we are to draw is that the amount of loss a state has experienced indicates its disaster proneness. Finally, ‘‘Charting a Course for theNext Two Decades’’ by Cutter describes what is needed to produce the models and data appropriate for mitigation and planning assessments. In order for an effective assessment of events and losses to occur, progress is required in several areas: development of vulnerability science, the creation of a national hazard events and losses database, and the establishment of a national loss inventory and events clearinghouse. To do so, Cutter argues, we need to rethink thewaywe monitor, assess, andmanage our vulnerabilities. She briefly describes the shifts needed in data gathering and provision, sustainability and distributive justice, strategic planning, research funding, and societal awareness of issues that influence the prospects for disaster. While American Hazardscapes is intended to provide a broadunderstanding of the geography of loss due to hazards in the United States, it suffers from its openly acknowledged limitations. Though criticizing the quality of currently available data, the authors use those data to indicate the prospects for future disasters. The elimination of extreme events is no longer believed tobe the key loss reduction. Instead,we must identify and avoid places too dynamic for permanent occupation and adjust to the inevitable events in ways that limit prospects for loss. Mitigation must address the vulnerabilities that cause greater exposure and profound upset of our social systems and create more complex catastrophes. The data employed in this assessment describe (however imperfectly) the losses suffered over two and a half decades. The largest disasters overwhelm the patterns of loss in their analysis. The authors imply, based on proneness rankings, that those who lost the most are the most prone to loss. But in reality, losses are byproducts of the interplay of two dynamic geographies: the pattern of extreme events and the pattern of human use of the landscape. The former is often poorly understood, may not behave consistently, andmay operate on greater than twenty-five-year cycles. The latter may change so rapidly that it surpasses our capacity to measure it and map it, and postdisaster land use and human perception may be radically changed. These geographies were outside the scope of this book, however, and given new homeland security efforts and reorganization of the Federal Emergency Management Agency, the past is an even poorer indicator of the future.",237 - 239,2003.0,http://weiss.info/explore/blogauthor.asp,Computer Science
858,7579330e89bffd736fee19d25359ab3ae65bf5f7,rioja: Analysis of Quaternary Science Data,,24-127,2012.0,https://allen.com/main/list/blogcategory.html,History
859,daaf02de10f338d98ed6f58c13987b63b275825a,Gaia Early Data Release 3,"Context. Since July 2014, the Gaia mission has been engaged in a high-spatial-resolution, time-resolved, precise, accurate astrometric, and photometric survey of the entire sky.
Aims. We present the Gaia Science Alerts project, which has been in operation since 1 June 2016. We describe the system which has been developed to enable the discovery and publication of transient photometric events as seen by Gaia.
Methods. We outline the data handling, timings, and performances, and we describe the transient detection algorithms and filtering procedures needed to manage the high false alarm rate. We identify two classes of events: (1) sources which are new to Gaia and (2) Gaia sources which have undergone a significant brightening or fading. Validation of the Gaia transit astrometry and photometry was performed, followed by testing of the source environment to minimise contamination from Solar System objects, bright stars, and fainter near-neighbours.
Results. We show that the Gaia Science Alerts project suffers from very low contamination, that is there are very few false-positives. We find that the external completeness for supernovae, CE = 0.46, is dominated by the Gaia scanning law and the requirement of detections from both fields-of-view. Where we have two or more scans the internal completeness is CI = 0.79 at 3 arcsec or larger from the centres of galaxies, but it drops closer in, especially within 1 arcsec.
Conclusions. The per-transit photometry for Gaia transients is precise to 1% at G = 13, and 3% at G = 19. The per-transit astrometry is accurate to 55 mas when compared to Gaia DR2. The Gaia Science Alerts project is one of the most homogeneous and productive transient surveys in operation, and it is the only survey which covers the whole sky at high spatial resolution (subarcsecond), including the Galactic plane and bulge.",43-122,2021.0,http://www.hernandez.com/tagabout.html,Physics
860,24931dc3ddedfc2db5405af236e3ca84944d66d7,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",96-139,2016.0,http://jones-lopez.org/searchcategory.htm,Computer Science
861,6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.","
          1-8
        ",2019.0,https://hunter.com/postsprivacy.htm,Sociology
862,a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",79-120,2015.0,https://www.patterson.biz/app/categoriescategory.jsp,Medicine
863,b970f9c088beee99666a40374dd5ccb06eeda112,Understanding the paradigm shift to computational social science in the presence of big data,,67-80,2014.0,http://sandoval-carlson.com/tagsfaq.asp,Computer Science
864,cebed64039064dfe950587b919ddc01dee7d871f,From Little Science to Big Science,"In Little Science, Big Science (1963), Derek J. de Solla Price undertook a sociology of science that dealt with the growth and changing shape of scientific publishing and the social organization of science. The focus of Price’s work was on the long-term, gradual shift from “little science,” with the solo scientist, small laboratory, and minimal research funds, to “big science,” with collaborative research teams, large-scale research hardware, extensive funding, and corporate-political suitors of scientists. We extend Price’s focus on scientific publications by moving beyond his analysis of practices in physics and chemistry to examine a social science; namely, sociology. Specifically, we analyze 3,000 articles in four long-standing sociology journals over the fifty-year period from 1960-2010 to determine the gender of authors, the prestige of authors’ departments, length of articles, number of references, sources of data for studies, and patterns of funding for research. We find that sociology is not immune from the shift from “little science” to “big science.”",64-125,2017.0,https://www.lopez-bailey.net/postspost.jsp,Political Science
865,eaf5a5e0b32a055e288d5edcc5cd39f9f4d335ad,The misuse of colour in science communication,,67-111,2020.0,https://mcdaniel.com/search/tagspost.html,Medicine
866,cf83811d697dc3419a52c9853807afb410eb3943,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",16-116,2018.0,http://www.smith.com/main/taghome.htm,Political Science
867,0b510ee69a507407008661aacb2345f73c70f8cb,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",2,2016.0,http://alvarado.com/main/tagsindex.htm,Political Science
868,b598b8dd79654dc865b02c2af0a0bdb565d24049,Taking a ‘Big Data’ approach to data quality in a citizen science project,,601 - 611,2015.0,http://harris.org/list/categoriesprivacy.jsp,Computer Science
869,ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,13 - 6,2015.0,http://morgan.com/categoriesmain.htm,Sociology
870,46d71d947231f86e1f9d4581e61212385debbe14,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",27-128,2014.0,http://wright.com/search/categories/wp-contentabout.html,Computer Science
871,b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,A review of machine learning applications in wildfire science and management,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.",86-125,2020.0,https://www.sims.biz/category/searchsearch.htm,Computer Science
872,500b73ecdf8ff5590718edb03367e3836a368485,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",619-626,2017.0,https://www.phillips.com/tagpost.htm,Computer Science
873,938a6209fe95dd4e5f801a14b6b650dc7b2f6108,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",43-109,2015.0,https://www.flores.info/tag/exploreindex.php,Medicine
874,993c9eb9bba80e2d8993e8c99acca1825cd0302f,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",1436 - 1437,2014.0,https://www.warner.com/postsfaq.php,Medicine
875,48fc9c42522184c652742255fdf31f7b9ed7ebae,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",57 - 69,2020.0,https://www.yang.com/list/listterms.htm,Medicine
876,fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",133 - 153,2014.0,http://www.hunter-hawkins.net/category/main/categoriesterms.jsp,Mathematics
877,cf9ecfbbd0095687c4cfbbbfa0546914e651b109,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.","
          777-81
        ",1998.0,https://www.leon-hoffman.net/category/categoriesabout.asp,Computer Science
878,2daffab3ebd3fc034f8f78d6a546606c33a5d398,"Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison",,787 - 804,2015.0,http://www.brown-williams.com/taghomepage.php,Computer Science
879,5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",345-405,1991.0,http://johnson.biz/main/postscategory.jsp,Computer Science
880,8e600778160ff986b5460bc2584066148e55e5d4,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",294 - 298,2017.0,https://www.martin.com/main/tag/mainauthor.htm,Medicine
881,28aecd08b2488c5300abf399feeb83a1f9c19890,Open Government Data,"Story Slides Slide 1 W3C eGovernment Community: Data Science Slide 2 Agenda Slide 3 The Changing Landscape of Federal Information Technology Slide 4 Cloud: SOA, Semantic, & Data Science: September 10-11th Slide 5 Opportunities for Data Science Slide 6 Discussion 1 Slide 7 Discussion 2 Slide 8 Discussion 3 Slide 9 Discussion 4 Slide 10 Discussion 5 Spotfire Dashboard Research Notes Joshua Tauberer’s Blog Open Government Data 1 Big Data Meets Open Government Figure 1 The New Federal Register 2.0 Figure 2 This animated visualization of live wind speeds and directions Figure 3 Data is like refrigerator poetry Figure 4 http://www.GovTrack.us Figure 5 John Oliver parodies Schoolhouse Rock’s “I’m Just a Bill” References 1 2 3 4 5 6 7",57-104,2019.0,https://dalton.org/tagsprivacy.htm,Business
882,b41fd82432999628e34d07e64ccda783273c15c0,Data integration enables global biodiversity synthesis,"Significance As anthropogenic impacts to Earth systems accelerate, biodiversity knowledge integration is urgently required to support responses to underpin a sustainable future. Consolidating information from disparate sources (e.g., community science programs, museums) and data types (e.g., environmental, biological) can connect the biological sciences across taxonomic, disciplinary, geographical, and socioeconomic boundaries. In an analysis of the research uses of the world’s largest cross-taxon biodiversity data network, we report the emerging roles of open-access data aggregation in the development of increasingly diverse, global research. These results indicate a new biodiversity science landscape centered on big data integration, informing ongoing initiatives and the strategic prioritization of biodiversity data aggregation across diverse knowledge domains, including environmental sciences and policy, evolutionary biology, conservation, and human health. The accessibility of global biodiversity information has surged in the past two decades, notably through widespread funding initiatives for museum specimen digitization and emergence of large-scale public participation in community science. Effective use of these data requires the integration of disconnected datasets, but the scientific impacts of consolidated biodiversity data networks have not yet been quantified. To determine whether data integration enables novel research, we carried out a quantitative text analysis and bibliographic synthesis of >4,000 studies published from 2003 to 2019 that use data mediated by the world’s largest biodiversity data network, the Global Biodiversity Information Facility (GBIF). Data available through GBIF increased 12-fold since 2007, a trend matched by global data use with roughly two publications using GBIF-mediated data per day in 2019. Data-use patterns were diverse by authorship, geographic extent, taxonomic group, and dataset type. Despite facilitating global authorship, legacies of colonial science remain. Studies involving species distribution modeling were most prevalent (31% of literature surveyed) but recently shifted in focus from theory to application. Topic prevalence was stable across the 17-y period for some research areas (e.g., macroecology), yet other topics proportionately declined (e.g., taxonomy) or increased (e.g., species interactions, disease). Although centered on biological subfields, GBIF-enabled research extends surprisingly across all major scientific disciplines. Biodiversity data mobilization through global data aggregation has enabled basic and applied research use at temporal, spatial, and taxonomic scales otherwise not possible, launching biodiversity sciences into a new era.",81-113,2021.0,https://lucas.com/main/category/appsearch.asp,Medicine
883,c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations andsor conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. © 2007 Wiley Periodicals, Inc.",2105-2125,2007.0,http://www.spence.info/search/explore/searchauthor.htm,Computer Science
884,b9921fb4d1448058642897797e77bdaf8f444404,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",267 - 297,2013.0,http://www.escobar.biz/tag/list/listcategory.html,Political Science
885,bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",90-134,1979.0,https://gallagher-smith.com/tags/tag/blogfaq.htm,Business
886,88a55ee54aae117f06441459d1ad2330ce18d7e0,Emerging problems of data quality in citizen science,,43-101,2016.0,https://www.alvarez.biz/categories/listabout.php,Medicine
887,bf96377353bf9daa8dc0e98eee17335f54cbcc60,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",163-164,2006.0,https://www.herman.com/maincategory.html,Computer Science
888,1842a5fb9739149dadba962c94dd7243a5f62242,What is Data Science ? Fundamental Concepts and a Heuristic Example,,40-51,1998.0,https://www.pierce-king.org/tag/tagscategory.html,Computer Science
889,116927fbe4c9732fd1e392035a100c33b14e9d59,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",13 - 53,2017.0,http://miller.info/tagshomepage.php,Computer Science
890,06ba782753bad19254db5d28ad4155556f286ee0,Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",32-131,2000.0,http://www.luna.org/exploreprivacy.php,Sociology
891,e281464d9a558cc1d25084687efb75683e65d4f0,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",61-136,2014.0,https://www.hansen.biz/posts/list/categoriesfaq.jsp,Geography
892,ff7a79011e4ddba98474efe776432ac2b4431473,Citizen science and the United Nations Sustainable Development Goals,,922 - 930,2019.0,http://www.jones.biz/tag/categories/postsauthor.html,Political Science
893,ff1068a7e2acaa41fae2a8e1b180264434f06ce8,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",1024 - 1026,2016.0,https://www.schmidt.net/app/blog/exploreabout.jsp,Medicine
894,c3665722a7cc81caca8c90ac3c5b0572f7bba055,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",16 - 2,2016.0,https://www.martin.com/tags/wp-contentprivacy.asp,Medicine
895,ecb81c5d18e38b29316da77f69c8a36d5b98f196,scmap: projection of single-cell RNA-seq data across data sets,,359-362,2018.0,http://vaughn.com/explore/listpost.asp,Biology
896,cff7b1b98da6de583bf2d5ffd496c2e6d70a794c,From DFT to machine learning: recent approaches to materials science–a review,"Recent advances in experimental and computational methods are increasing the quantity and complexity of generated data. This massive amount of raw data needs to be stored and interpreted in order to advance the materials science field. Identifying correlations and patterns from large amounts of complex data is being performed by machine learning algorithms for decades. Recently, the materials science community started to invest in these methodologies to extract knowledge and insights from the accumulated data. This review follows a logical sequence starting from density functional theory as the representative instance of electronic structure methods, to the subsequent high-throughput approach, used to generate large amounts of data. Ultimately, data-driven strategies which include data mining, screening, and machine learning techniques, employ the data generated. We show how these approaches to modern computational materials science are being used to uncover complexities and design novel materials with enhanced properties. Finally, we point to the present research problems, challenges, and potential future perspectives of this new exciting field.",59-110,2019.0,http://www.holmes.com/listterms.php,Physics
897,2aca01aa1d0a6083986dcd4614b1d0733028dcc2,"Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects","In various disciplines, information about the same phenomenon can be acquired from different types of detectors, at different conditions, in multiple experiments or subjects, among others. We use the term “modality” for each such acquisition framework. Due to the rich characteristics of natural phenomena, it is rare that a single modality provides complete knowledge of the phenomenon of interest. The increasing availability of several modalities reporting on the same system introduces new degrees of freedom, which raise questions beyond those related to exploiting each modality separately. As we argue, many of these questions, or “challenges,” are common to multiple domains. This paper deals with two key issues: “why we need data fusion” and “how we perform it.” The first issue is motivated by numerous examples in science and technology, followed by a mathematical framework that showcases some of the benefits that data fusion provides. In order to address the second issue, “diversity” is introduced as a key concept, and a number of data-driven solutions based on matrix and tensor decompositions are discussed, emphasizing how they account for diversity across the data sets. The aim of this paper is to provide the reader, regardless of his or her community of origin, with a taste of the vastness of the field, the prospects, and the opportunities that it holds.",1449-1477,2015.0,http://hicks.com/categories/main/bloghomepage.jsp,Engineering
898,952241d28abed7d221fc059845043a6463a522bc,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",16 - 25,2016.0,https://miller-smith.com/tags/appsearch.html,Medicine
899,915cd8e2b39eb02723553913d592b2237d4d9960,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",40-118,2001.0,https://wheeler-wilson.biz/category/apphome.html,Business
900,43d75d3a22db904d052d4c435e2d1f22be3887e0,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",2250-2267,2014.0,https://ortega.com/categoryindex.htm,Computer Science
901,7ac8f533a18f584387dd412a0a27feb9af1c5c93,A Systematic Review on Imbalanced Data Challenges in Machine Learning,"In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.",1 - 36,2019.0,https://martinez.org/explorecategory.php,Computer Science
902,2e096b5fe420e09e3a7ea3b1e8f1501495d8b07e,Operationalizing the CARE and FAIR Principles for Indigenous data futures,,40-121,2021.0,https://martin-rowe.com/wp-contentregister.htm,Medicine
903,023eb29b711014b1a3d2895e19a0fc2aed7a6ab4,Data Science and Classification,,31-114,2006.0,http://simon-young.com/tag/app/searchlogin.htm,Computer Science
904,e048f6fdb0a728638af5d8684a32b3dc2ee83259,Big Data and Science: Myths and Reality,,49-52,2015.0,https://jackson.net/postshome.asp,History
905,0efa865dd45bcee8194bfe709b0f81789f6d5341,Data sharing practices and data availability upon request differ across scientific disciplines,,44-127,2021.0,https://www.johnson-bailey.biz/tag/wp-content/postspost.htm,Medicine
906,de5acd80c5fd8db442a4a5e5ffbc3f3f51161237,"Data Science, Classification and Related Methods",,86-109,1998.0,https://www.sweeney.net/main/appauthor.asp,Computer Science
907,f03a847c6325d7d5973efd687d2ca86a9c06dd76,Advances in data science and classification,,54-110,1998.0,http://mccormick.com/explore/categories/listcategory.php,Computer Science
908,197b30ab1460fe200dba90dc3392ad49a92c2ca4,Between Data Science and Applied Data Analysis,,75-104,2003.0,https://www.campos-villarreal.com/tagssearch.jsp,Computer Science
909,4d12b00963aa6e0d9b9b84a62f0543de608fccb5,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",58-107,2013.0,http://potter-pierce.net/listauthor.html,Business
910,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,A survey of machine learning for big data processing,,57-101,2016.0,http://parks-hunt.com/categoriessearch.asp,Computer Science
911,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",1-18,2018.0,https://stewart-martinez.net/tags/listfaq.html,Economics
912,db019eec15d8080086bbc7dc8f5832e431202e0e,Jupyter: Thinking and Storytelling With Code and Data,"Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.",7-14,2021.0,http://west-ramos.com/postspost.htm,Computer Science
913,c1e49d830e67269d4d2053a5f124ea773c79b740,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",1060 - 1062,2020.0,https://martinez.com/exploreterms.html,Medicine
914,846883b7761cb5fe4468d42bf9d328b5d1030175,"The Zwicky Transient Facility: Data Processing, Products, and Archive","The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of ∼3760 square degrees/hour to median depths of g ∼ 20.8 and r ∼ 20.6 mag (AB, 5σ in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The real-time pipeline employs a novel image-differencing algorithm, optimized for the detection of point-source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. We present some preliminary results of the calibration performance delivered by the real-time pipeline. The reconstructed astrometric accuracy per science image with respect to Gaia DR1 is typically 45 to 85 milliarcsec. This is the RMS per-axis on the sky for sources extracted with photometric S/N ≥ 10 and hence corresponds to the typical astrometric uncertainty down to this limit. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. The high end of these ranges corresponds to an airmass approaching ∼2—the limit of the public survey. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2%. The products support a broad range of scientific applications: fast and young supernovae; rare flux transients; variable stars; eclipsing binaries; variability from active galactic nuclei; counterparts to gravitational wave sources; a more complete census of Type Ia supernovae; and solar-system objects.",73-132,2018.0,https://www.tucker-ramirez.info/search/categoriesfaq.html,Physics
915,3859aef8d52ef1bad6351ec25c4fe4009b184689,Characterization of the LIGO detectors during their sixth science run,"In 2009-2010, the Laser Interferometer Gravitational-wave Observa- tory (LIGO) operated together with international partners Virgo and GEO600 as a network to search for gravitational waves of astrophysical origin. The sensitiv- ity of these detectors was limited by a combination of noise sources inherent to the instrumental design and its environment, often localized in time or frequency, that couple into the gravitational-wave readout. Here we review the performance of the LIGO instruments during this epoch, the work done to characterize the de- tectors and their data, and the effect that transient and continuous noise artefacts have on the sensitivity of LIGO to a variety of astrophysical sources.",78-104,2014.0,http://www.austin.net/search/main/tagsregister.jsp,Physics
916,09ee0ba924ffd21fc7e14ad3147284133cf2f576,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",353-353,1967.0,https://www.guerra.biz/blog/main/poststerms.php,Engineering
917,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",76-150,2019.0,https://harrington.net/tagpost.html,Medicine
918,d33d879ea94fd36363dc7f015896ac6c0236acac,Data Preprocessing in Data Mining,,1-313,2014.0,http://www.huffman-stevens.com/searchsearch.html,Computer Science
919,86b05bc7e953e683fa839ad01d6100a8f99558df,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.","I-XIII, 1-623",1991.0,https://www.brooks.biz/search/search/tagprivacy.html,Computer Science
920,04e5f980428e1ec35429356b3e43ea611fc0e975,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",390 - 421,2014.0,https://www.kelly.com/tag/search/bloglogin.jsp,Psychology
921,f4156a05a47fdeda30638e10954d3674cc056ab6,Discovering Knowledge in Data: An Introduction to Data Mining,"This book is the first volume of a three-volume series on data mining, which introduces the reader to this rapidly growing field. Data mining, which has gained noticeable popularity in the past decade, is essentially an interdisciplinary field bringing together techniques from machine learning, pattern recognition, statistics, databases, and visualization (Cabena et al., 1998) to address the issue of exploring large and complicated databases to identify “interesting” relationships, e.g., high order interactions, or very non-linear relationships that ordinarily would not be detected by standard statistical analyses (Borok, 1997; Szolvits, 1995). This area has been approached by computer scientists and statisticians from slightly different perspectives. The author of the book is a statistician, but has tried to include a computer science theme throughout the book, in which I think he has been successful. As he mentions in the preface, the book is intended to be used either by analysts, managers, and decision makers in industry or as a textbook for an introductory course in data mining for graduate or advanced undergraduate students (in computer science or statistics). Chapter 1 is a short introductory chapter, in which in addition to a brief description of the Cross-Industry Standard Process for Data Mining (CRISP-DM), several real-world case studies are covered to motivate the topics of subsequent chapters. These case studies are also used to describe the first phase of the CRISPDM process, namely business understanding. Chapters 2 and 3 examine the next two phases of the CRISP-DM process, i.e., data understanding and data preparation. Chapter 2 is on data preprocessing, which is divided into the two major tasks of data cleaning and data transformation. In data cleaning, general methods for handling missing data, identifying misclassified records in the data, and also a graphical method for detecting outliers are described. In the data transformation section, min-max normalization and z-score standardization methods are discussed. A numerical method for detecting outliers based on z-score standardization is also covered. Exploratory data analysis is the topic of Chapter 3, which focuses on data understanding. The chapter begins with making a contrast between hypothesis testing and exploratory data analysis, and is followed by the basic ideas of dealing with correlated variables in the data set. Most of this chapter is dedicated to exploring the variables in a real data set, in which by using several diagrams a number of intuitive approaches for obtaining a high level understanding of the data are proposed.",411-412,2005.0,https://www.stevenson.com/list/categories/tagfaq.html,Computer Science
922,4b4b63405efd22a96cc45b22c08124d62a475d6f,Big healthcare data: preserving security and privacy,,26-135,2018.0,https://gutierrez.com/list/listterms.html,Computer Science
923,db8335198bd47c8865d0b3408b97e547abfd9ba2,The Fourth Paradigm: Data-Intensive Scientific Discovery,"This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.",57-110,2009.0,http://edwards.net/tagslogin.htm,Computer Science
924,c278f3e91bf11c72be6808972f00810f15d877a4,Mapping citizen science contributions to the UN sustainable development goals,,1735 - 1751,2020.0,http://www.jones-sexton.org/postsauthor.html,Business
925,fd40e458a67f9a3854834fd42b66b0d6ed43ab8d,Educational Data Mining and Learning Analytics,,61-75,2014.0,http://lozano.net/tag/categories/blogpost.html,Computer Science
926,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",255-308,2009.0,http://www.smith-bentley.info/exploresearch.php,Computer Science
927,0578dfb2a28b77abde19b32de777e0365df3020e,Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",52-114,2020.0,http://daugherty-clark.info/main/explore/wp-contentregister.jsp,Computer Science
928,851a4c4e9d9bf8f023bc4cd29e023e4c43957b7d,The Art and Science of Data-Driven Journalism,"Journalists have been using data in their stories for as long as the profession has existed. A revolution in computing in the 20th century created opportunities for data integration into investigations, as journalists began to bring technology into their work. In the 21st century, a revolution in connectivity is leading the media toward new horizons. The Internet, cloud computing, agile development, mobile devices, and open source software have transformed the practice of journalism, leading to the emergence of a new term: data journalism. Although journalists have been using data in their stories for as long as they have been engaged in reporting, data journalism is more than traditional journalism with more data. Decades after early pioneers successfully applied computer-assisted reporting and social science to investigative journalism, journalists are creating news apps and interactive features that help people understand data, explore it, and act upon the insights derived from it. New business models are emerging in which data is a raw material for profit, impact, and insight, co-created with an audience that was formerly reduced to passive consumption. Journalists around the world are grappling with the excitement and the challenge of telling compelling stories by harnessing the vast quantity of data that our increasingly networked lives, devices, businesses, and governments produce every day. While the potential of data journalism is immense, the pitfalls and challenges to its adoption throughout the media are similarly significant, from digital literacy to competition for scarce resources in newsrooms. Global threats to press freedom, digital security, and limited access to data create difficult working conditions for journalists in many countries. A combination of peer-to-peer learning, mentorship, online training, open data initiatives, and new programs at journalism schools rising to the challenge, however, offer reasons to be optimistic about more journalists learning to treat data as a source.",75-149,2014.0,http://www.silva.net/explore/tag/exploreregister.php,Political Science
929,8cd71d704f9d3eeb5eb697e412ba54b680f00636,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",17-123,2014.0,https://harris.com/tagsfaq.html,Medicine
930,8958efba7a02e3653f27c0e759882b2f3352e896,"Materials Cloud, a platform for open computational science",,36-141,2020.0,http://www.vega.com/tags/tagregister.htm,Materials Science
931,fbd9ddc0a3862512ce7a0ba2bb9cb159da0a9d2f,Editorial - Marketing Science and Big Data,"This article was downloaded by: [128.97.27.20] On: 25 May 2016, At: 09:44 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA Marketing Science Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Editorial—Marketing Science and Big Data Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser To cite this article: Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser (2016) Editorial—Marketing Science and Big Data. Marketing Science 35(3):341-342. http://dx.doi.org/10.1287/mksc.2016.0996 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",341-342,2016.0,http://www.duke-hayes.info/tags/wp-content/listlogin.asp,Computer Science
932,8807a8327e27298fd601fc65e6a9ccfae1cca195,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",59-148,2016.0,http://bowers-adams.com/app/categories/appabout.htm,Medicine
933,53834f0ee8df731cf0e629cd594dce0afaaa3d97,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer","
          1351-2
        ",2013.0,https://wilkins.org/search/list/listhomepage.php,Medicine
934,64ad643e8084486ca7d3312ed491a814d3fe440c,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",399-410,2016.0,https://santiago.com/search/tagspost.htm,Computer Science
935,2e888654c68524163fbf7a54396488249e73a702,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",977 - 984,2009.0,http://johnson.com/main/categorysearch.asp,Biology
936,d43e2d9b90c0f509c9f569b9d4bd431ebd711f4f,Sharing Data and Materials in Psychological Science,,699 - 702,2017.0,https://www.hill.org/app/blog/explorehome.jsp,Medicine
937,83b2bc2583862fa662cdfeb6cc7950bb2972347d,ImmPort: disseminating data to the public for the future of immunology,,234 - 239,2014.0,https://molina.com/categories/postsregister.html,Medicine
938,25e0d93ca47d86510d6a0f9cda9ae3594f3d05b2,"Color Science: Concepts and Methods, Quantitative Data and Formulas","Eventually, you will agreed discover a further experience and achievement by spending more cash. still when? attain you acknowledge that you require to acquire those every needs in imitation of having significantly cash? Why don't you try to acquire something basic in the beginning? That's something that will lead you to comprehend even more in the region of the globe, experience, some places, later than history, amusement, and a lot more?",128,1968.0,http://adkins.com/category/posts/blogmain.asp,Psychology
939,c8b3f78bdead3596c4e7cb3aaad07a79cfa86ce4,Calling Bullshit: The Art of Skepticism in a Data-Driven World,"This week on the Science podcast, evolutionary biologist Carl Bergstrom explains how to identify data-driven misinformation and disinformation.",1064 - 1064,2020.0,http://www.bullock-mitchell.com/category/posts/wp-contentlogin.asp,Technology
940,760d38a08bff329ff67719935c18fa1631e3ded8,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",171-198,2016.0,http://www.fox-hamilton.com/main/blog/tagslogin.html,Technology
941,0131258a516da6f9d86795fc6ed4968206dba005,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",1 - 21,2019.0,https://www.lee.com/wp-content/apphomepage.php,Computer Science
942,377f1e43c5a48f12b0592b09a142322e74729409,Genetic Algorithms in the Fields of Artificial Intelligence and Data Sciences,,1007 - 1018,2021.0,https://prince.net/tagsindex.jsp,Computer Science
943,1a46465ab69ec13d3c84d66166e979989afa596d,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",1037 - 1037,2016.0,http://www.dixon.biz/postsprivacy.php,Medicine
944,e1ababf08c9ec103db854a2c1b4db611142cfdb7,Linear Mixed Models for Longitudinal Data,,53-132,2001.0,https://tran-butler.com/categories/categories/searchauthor.asp,Medicine
945,0bc97adfb3c77f27397d19395af2fdff9f04aaa0,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",96-147,2016.0,https://www.anderson.net/category/search/postslogin.htm,Physics
946,720400bf69c1af50795d7ec1b58e95c682d217aa,Best Practices in Data Analysis and Sharing in Neuroimaging using MRI,"Neuroimaging enables rich noninvasive measurements of human brain activity, but translating such data into neuroscientific insights and clinical applications requires complex analyses and collaboration among a diverse array of researchers. The open science movement is reshaping scientific culture and addressing the challenges of transparency and reproducibility of research. To advance open science in neuroimaging the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), charged with creating a report that collects best practice recommendations from experts and the entire brain imaging community. The purpose of this work is to elaborate the principles of open and reproducible research for neuroimaging using Magnetic Resonance Imaging (MRI), and then distill these principles to specific research practices. Many elements of a study are so varied that practice cannot be prescribed, but for these areas we detail the information that must be reported to fully understand and potentially replicate a study. For other elements of a study, like statistical modelling where specific poor practices can be identified, and the emerging areas of data sharing and reproducibility, we detail both good practice and reporting standards. For each of seven areas of a study we provide tabular listing of over 100 items to help plan, execute, report and share research in the most transparent fashion. Whether for individual scientists, or for editors and reviewers, we hope these guidelines serve as a benchmark, to raise the standards of practice and reporting in neuroimaging using MRI.",95-139,2016.0,https://johnson-cook.info/tagcategory.jsp,Biology
947,edacaedb1b2312023c4b0cf1d42bbdbed2793c65,The Electric and Magnetic Field Instrument Suite and Integrated Science (EMFISIS) on RBSP,,127-181,2013.0,http://moore.info/explore/tagssearch.htm,Physics
948,929607741b2a12656ff8d3360ca96fe76a6557a4,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",47-104,2013.0,http://www.mason.com/tag/wp-contentabout.php,Psychology
949,d90f276316589f503690d541392989031f9d046b,Online Citizen Science: A Systematic Review of Effects on Learning and Scientific Literacy,"Participation in online citizen science is increasingly popular, yet studies that examine the impact on participants’ learning are limited. The aims of this paper are to identify the learning impact on volunteers who participate in online citizen science projects and to explore the methods used to study the impact. The ten empirical studies, examined in this systematic review, report learning impacts on citizens’ attitudes towards science, on their understanding of the nature of science, on topic-specific knowledge, on science knowledge, and on generic knowledge. These impacts were measured using self-reports, content analysis of contributed data and of forum posts, accuracy checks of contributed data, science and project-specific quizzes, and instruments for measuring scientific attitudes and beliefs. The findings highlight that certain technological affordances in online citizen science projects can cultivate citizens’ knowledge and skills, and they point to unexplored areas, including the lack of experimental and long-term studies, and studies in formal education settings.",37-116,2020.0,http://www.robertson.biz/app/search/listhome.htm,Sociology
950,c8bc2d5edb9307b5c420adc4eee3cf641a781b14,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",14-17,2007.0,http://www.elliott.net/blog/categoriesterms.jsp,Computer Science
951,43789305e5d2212da05f9c16b148e84aae5614b2,Citizen Science and Volunteered Geographic Information: Overview and Typology of Participation,,105-122,2013.0,https://www.carter-thompson.net/posts/postssearch.html,Geography
952,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",49-113,2016.0,https://king.com/list/postsprivacy.jsp,Computer Science
953,917943472ec4a00443d78bb696ed4d8f8d8c7f0a,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",1187-1218,2007.0,http://martin.com/explore/blogauthor.html,Sociology
954,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",75-119,2016.0,http://hanson-washington.com/tags/wp-contentregister.php,Physics
955,f567f5a4a57509c2288f510d6703212ce8499527,The Ames Stereo Pipeline: NASA's Open Source Software for Deriving and Processing Terrain Data,"The NASA Ames Stereo Pipeline is a suite of free and open source automated geodesy and stereogrammetry tools designed for processing stereo images captured from satellites (around Earth and other planets), robotic rovers, aerial cameras, and historical images, with and without accurate camera pose information. It produces cartographic products, including digital terrain models, ortho‐projected images, 3‐D models, and bundle‐adjusted networks of cameras. Ames Stereo Pipeline's data products are suitable for science analysis, mission planning, and public outreach.",537 - 548,2018.0,http://rodriguez.org/categories/postsprivacy.html,Computer Science
956,6d962e9f04c653f732da82073a3446f75a371055,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",27-34,1996.0,https://www.woods.biz/postslogin.php,Computer Science
957,62e0c6cf57bc345026d56fd654e80beaf9315c92,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",1 - 30,2011.0,https://walker.biz/tagslogin.html,Chemistry
958,a971f856fcf4a4a7589dbf711dd2544f51c5e9b2,Linked Data - A Paradigm Shift for Geographic Information Science,,173-186,2014.0,https://vasquez.com/posts/searchindex.htm,Computer Science
959,b7118fca8e7cd69d76090a5c145e89f303249eb8,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",291-297,2012.0,https://www.sharp.net/tag/listauthor.html,Political Science
960,b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,Handbook of theoretical computer science - Part A: Algorithms and complexity; Part B: Formal models and semantics,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",79-149,1990.0,https://ayala-stephenson.com/searchmain.php,Computer Science
961,90478017154dd6e4dbcb71895c64c9ddddebfb8c,Taxonomic bias in biodiversity data and societal preferences,,16-149,2017.0,https://www.adams.net/wp-contentfaq.html,Medicine
962,05859c8d47b16ce84c817c16d29ad6ec9d1d3a33,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",1-10,2013.0,http://ali.biz/poststerms.htm,Computer Science
963,97156d041b6cae2095dd29d76e24e0017a7ec799,Functional Data Analysis,,5822-5828,1997.0,https://www.kim.biz/wp-contentlogin.asp,Mathematics
964,85cd1c3c6346d8fe3b245cc41e2757631301bc27,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",400 - 414,2016.0,http://www.hall-thompson.biz/mainfaq.php,Sociology
965,41692ed07f393c1c3e335db99c7e3c5a0d265a78,Citation indexes for science; a new dimension in documentation through association of ideas.,"‘The uncritical citation of disputed data by a writer, whether it be deliberate or not, is a serious matter. Of course, knowingly propagandizing unsubstantiated claims is particularly abhorrent, but just as many naive students may be swayed by unfounded assertions presented by a writer who is unaware of the criticisms. Buried in scholarly journals, critical notes are increasingly likely to be overlooked with the passage of time, while the studies to which they pertain, having been reported more widely, are apt to be rediscovered.’ 1","
          108-11
        ",2006.0,http://harris-perez.com/app/categorysearch.php,Medicine
966,34ad09cda075101dc4ce3c04006ff804aca3ebf8,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",404-409,2013.0,https://harrison.com/blog/app/tagcategory.html,Computer Science
967,a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",1-5,2012.0,http://www.davis.com/searchpost.jsp,Engineering
968,5ae073986408c9931bf6887fafb85e253866f7cc,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",82-105,2001.0,http://www.sandoval.biz/tag/blogfaq.asp,Political Science
969,7a1b9cc42e6fc611970b451fbef795e72cbea46d,Ecoinformatics: supporting ecology as a data-intensive science.,,"
          85-93
        ",2012.0,http://massey.com/wp-contenthome.html,Medicine
970,9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",149-172,2010.0,https://leonard.com/list/tag/blogmain.htm,Geography
971,18a940ff6dce8bc140658da52d686291ca965979,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",292 - 326,1989.0,https://davis-rodriguez.com/explore/exploreauthor.php,Computer Science
972,a6e594b11bd8195e96a1826f591fcec9a20fdcf3,"Frascati manual 2015 : guidelines for collecting and reporting data in research and experimental development: the measurement of scientific, technological and innovation activities.","The Frascati Manual is firmly based on experience gained from collecting R&D 
statistics in both OECD and non-member countries. It is a result of the collective work 
of national experts in NESTI, the OECD Working Party of National Experts on Science 
and Technology Indicators. This group, with support from the OECD Secretariat, has 
worked over now more than 50 years as an effective community of practitioners to 
implement measurement approaches for the concepts of science, technology and 
innovation. This effort has resulted in a series of methodological manuals known as the 
“Frascati Family”, which in addition to this manual includes guidance documents on 
the measurement of innovation (the Oslo Manual), human resources devoted to science 
and technology, patents, and technological balance of payments, but most importantly, 
it has provided the basis for the main statistics and indicators on science and technology 
that are currently used.",73-118,2016.0,http://www.brown.com/tag/postshome.htm,Engineering
973,954f2a7b1c6f28c4a845ccda5761eb09da032a64,Data sharing,"The Science family of journals is committed to sharing data relevant to public health emergencies, and therefore we are signatories to, and wholeheartedly endorse, the following statement by funders and journals.*",1007 - 1007,2016.0,http://reed.com/tagpost.jsp,Medicine
974,edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",636 - 638,2015.0,https://finley.biz/categories/blogprivacy.php,Medicine
975,411f9ebe18a885e687788841f4b3a60a0c3df3bc,Computational Social Science,,721 - 723,2009.0,https://warren-alexander.com/tags/tagsregister.jsp,Computer Science
976,f6ce14f91b4641942947882062682125369847f7,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",55-126,2015.0,https://www.farmer.com/category/apphome.asp,Psychology
977,69732dcf45024f28e5c43de68d1208f6e737eada,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",D18 - D24,2016.0,https://www.davis.com/categories/blogregister.html,Medicine
978,9386590554c429e80402c082e9d6a2398bcc36b3,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",413-413,2005.0,http://bentley.com/category/appindex.htm,Computer Science
979,29196eb8c80a6fd6a159373f14ff323f081a8b7a,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",305 - 308,2013.0,http://www.santos.com/blogcategory.php,Medicine
980,c50dca78e97e335d362d6b991ae0e1448914e9a3,Reducing the Dimensionality of Data with Neural,"http://www.sciencemag.org/cgi/content/full/313/5786/504 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/313/5786/504/DC1 can be found at: Supporting Online Material found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/313/5786/504#related-content http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles , 6 of which can be accessed for free: cites 8 articles This article 15 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles 4 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining",71-149,2008.0,http://www.holland.org/blog/blog/wp-contentregister.php,Computer Science
981,5952a9f10ef65983042794369d376e23d2682d7e,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",19 - 42,2013.0,http://perkins.com/tag/exploremain.html,Political Science
982,a418d8fd1cc0abb34cf131d81723bc5da8817c93,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",167 - 187,2012.0,https://thomas.com/category/categoryabout.html,Political Science
983,1715fdc4df6774d95ed63f3feb58fa93a84dbed7,Data-intensive science applied to broad-scale citizen science.,,"
          130-7
        ",2012.0,http://garcia.com/categories/blog/tagsfaq.php,Medicine
984,8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",471-476,2012.0,http://www.oneill.biz/listauthor.html,Geography
985,3954e2d220d9a7b7a46f9561cafb6251524d8ee5,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",94-150,2007.0,http://higgins-curtis.net/searchregister.php,Geology
986,06d2a3fde80c5644f14f743b29a57f6b02e850d9,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",54-137,2016.0,https://mason.com/mainhome.html,Biology
987,04638c67b715b9d85ae5a44afd3730b83330fb66,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",51-102,2014.0,https://www.bishop.com/searchfaq.html,Medicine
988,687e00a5fec7d747d18866f60b7a21973e80b04f,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",26-127,2016.0,https://www.stuart-owens.com/tags/tagsregister.php,Sociology
989,c32b03c3b5bbc97b0ec30663da1ff555f30acd95,Principled Missing Data Treatments,,284-294,2018.0,http://www.roman.com/search/categoryprivacy.html,Computer Science
990,59b2796c176636a3222d7b129c6209fa6e979aa7,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",68-140,2018.0,http://www.henry.com/tagsearch.html,Sociology
991,f7d7f1eb559d8e2f410289fca37bb6cec7a3a907,Data politics,"The commentary raises political questions about the ways in which data has been constituted as an object vested with certain powers, influence, and rationalities. We place the emergence and transformation of professional practices such as ‘data science’, ‘data journalism’, ‘data brokerage’, ‘data mining’, ‘data storage’, and ‘data analysis’ as part of the reconfiguration of a series of fields of power and knowledge in the public and private accumulation of data. Data politics asks questions about the ways in which data has become such an object of power and explores how to critically intervene in its deployment as an object of knowledge. It is concerned with the conditions of possibility of data that involve things (infrastructures of servers, devices, and cables), language (code, programming, and algorithms), and people (scientists, entrepreneurs, engineers, information technologists, designers) that together create new worlds. We define ‘data politics’ as both the articulation of political questions about these worlds and the ways in which they provoke subjects to govern themselves and others by making rights claims. We contend that without understanding these conditions of possibility – of worlds, subjects and rights – it would be difficult to intervene in or shape data politics if by that it is meant the transformation of data subjects into data citizens.",60-138,2017.0,http://melendez.com/mainpost.asp,Computer Science
992,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",144-160,2017.0,https://carroll.info/taglogin.htm,Computer Science
993,832139bd87f51f0a173b5bd9255944748bc31a96,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",35-126,2011.0,http://anderson.com/category/wp-content/categorylogin.html,Geology
994,0d7a9a5233b1460941b51a50e032b3c5d3a711cc,The Interview: Data Collection in Descriptive Phenomenological Human Scientific Research*,"Abstract In this article, interviewing from a descriptive, phenomenological, human scientific perspective is examined. Methodological issues are raised in relation to evaluative criteria as well as reflective matters that concern the phenomenological researcher. The data collection issues covered are 1) the selection of participants, 2) the number of participants in a study, 3) the interviewer and the questions, and 4) data collection procedures. Certain conclusions were drawn indicating that phenomenological research methods cannot be evaluated on the basis of an empiricist theory of science, but must be critiqued from within a phenomenological theory of science. Some reflective matters, experienced by the phenomenological researcher, are also elaborated upon.",13-35,2012.0,http://www.hudson.org/wp-contentpost.asp,Psychology
995,951eab2b27c673e0ff1a20800f576d4792f60d5f,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",224 - 225,2016.0,https://baker-smith.com/explore/wp-content/tagterms.jsp,Sociology
996,e7f5ab8f486487dcefdf9b989d0eff2f0beff48c,A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research,"Despite the abundance of published material on conducting focus groups, scant specific information exists on how to analyze focus group data in social science research. Thus, the authors provide a new qualitative framework for collecting and analyzing focus group data. First, they identify types of data that can be collected during focus groups. Second, they identify the qualitative data analysis techniques best suited for analyzing these data. Third, they introduce what they term as a micro-interlocutor analysis, wherein meticulous information about which participant responds to each question, the order in which each participant responds, response characteristics, the nonverbal communication used, and the like is collected, analyzed, and interpreted. They conceptualize how conversation analysis offers great potential for analyzing focus group data. They believe that their framework goes far beyond analyzing only the verbal communication of focus group participants, thereby increasing the rigor of focus group analyses in social science research.",1 - 21,2009.0,https://smith.com/main/wp-contentterms.html,Psychology
997,052fcf10e96f282c0fb50f778150afeaf92bb65d,Inquiry-based science instruction—what is it and does it matter? Results from a research synthesis years 1984 to 2002,"The goal of the Inquiry Synthesis Project was to synthesize findings from research conducted between 1984 and 2002 to address the research question, What is the impact of inquiry science instruction on K-12 student outcomes? The timeframe of 1984 to 2002 was selected to continue a line of synthesis work last completed in 1983 by Bredderman (Bredderman (1983) Review of Educational Research 53: 499-518) and Shymansky, Kyle, and Alport (Shymansky et al. (1983) Journal of Research in Science Teaching 20: 387-404), and to accommodate a practicable cut- off date given the research project timeline, which ran from 2001 to 2006. The research question for the project was addressed by developing a conceptual framework that clarifies and specifies what is meant by ''inquiry-based science instruction,'' and by using a mixed-methodology approach to analyze both numerical and text data describing the impact of instruction on K-12 student science conceptual learning. Various findings across 138 analyzed studies indicate a clear, positive trend favoring inquiry-based instructional practices, particularly instruction that emphasizes student active thinking and drawing conclusions from data. Teaching strategies that actively engage students in the learning process through scientific investigations are more likely to increase conceptual understanding than are strategies that rely on more passive techniques, which are often necessary in the current standardized-assessment laden educational environment.",474-496,2010.0,http://banks.net/blogindex.html,Psychology
998,7e95c6f943b7c47af1b2ef1651b86022a001ce81,A Review of Microsoft Academic Services for Science of Science Studies,"Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.",79-149,2019.0,http://chavez.com/app/tagfaq.asp,Computer Science
999,9e7be12082f58cbf7ebdb84a8cbdc897a4e41683,The Deluge of Spurious Correlations in Big Data,,595 - 612,2016.0,http://www.bowen-tyler.com/mainauthor.html,Computer Science
1000,cd0557eef00be9a443bfda516e01f94d63601c77,Data Quality and Data Engineering,,90-143,2023.0,http://www.obrien.org/search/wp-contentregister.php,Technology
1001,6ac1749272c9d2db025a6e26b3166555382db5bc,Recent Advances in Data Engineering for Networking,"This tutorial paper examines recent advances in data engineering, focusing on aspects of network management and orchestration. We provide a comprehensive analysis of standardization efforts as well as platform development activities related to data engineering driven network design. We then focus on the integration aspects of the data engineering ecosystem and telecommunication networks. The results of our tutorial investigation show that despite various efforts towards standardization and network management and orchestration platforms, there is still a significant gap in applying recent developments in the evolving data engineering world to the telecommunication domain. New advanced functionalities in data engineering as well as clear separations between the building blocks of data engineering pipelines within the proposed standardized architectures have been overlooked or not explored in detail by the standardization or platform development bodies in the telecommunication domain. Therefore, at the end of the paper, we discuss these gaps and research challenges in the context of future development processes for data engineering-driven network design and applications of data engineering concepts in telecommunication networks. We also propose several recommendations for early adoption of these technologies and frameworks in telecommunication infrastructures and platforms.",1-1,2022.0,http://www.marshall.com/mainmain.htm,Computer Science
1002,bc9be735a6d18ff9d25a2cc6e43f70c0579c96f2,Towards a Data Engineering Process in Data-Driven Systems Engineering,"Highly Automated Driving (HAD) has become one of the leading trends in the automotive industry. Mandatory tasks like environment perception and scene understanding challenge existing rule-based methods. Thus, data-driven technologies and Artificial Intelligence (AI) have been introduced to automotive software development. Utilizing data in the development process has become essential as these systems are no longer developed with classical systems engineering methods, but rather by deriving requirements from and training the algorithms with recorded real-world data. This entails the introduction of data-driven workflows and data-management as new aspects of Automotive Systems Engineering (ASE). Tasks related to the development of Artificial Intelligence (AI) software differ from their classical engineering and programming counterparts. Thus, engineers require new tools and methods for developing safe and accurate AI-based software and handling data efficiently during ASE. Another important aspect of data-driven development is ensuring data quality throughout the systems engineering process. Hence, this paper aims to take a step towards the introduction of a data engineering process in data-driven automotive systems engineering. Putting a spotlight on developing well-designed data sets as the central element for training and validating AI-based software. Besides determining the quality of data sets, we present steps towards improving data and data set quality.",1-8,2022.0,https://woods.com/tags/tagshome.php,Technology
1003,18b18969b8688d01c124543f3956d4fd1b5ad5a7,Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems,,739 - 768,2021.0,https://www.campbell.org/apphome.jsp,Computer Science
1004,164fcdf754254d03a20806aece85aff8489e7e4f,Test Data Engineering,,49-101,2022.0,http://rodriguez.com/wp-content/searchterms.jsp,Technology
1005,b2ffbd5a4bebb3a67f64e82669b4255f3d1e5bee,Data engineering for fraud detection,,113492,2021.0,https://castro.com/blogindex.php,Computer Science
1006,fd5532f3da374f1081c15c8913eabb65c2dc582d,Economic Data Engineering,"Economic data engineering deliberately designs novel forms of data to solve fundamental identification problems associated with economic models of choice. I outline three diverse applications: to the economics of information; to life-cycle employment, earnings, and spending; and to public policy analysis. In all three cases one and the same fundamental identification problem is driving data innovation: that of separately identifying appropriately rich preferences and beliefs. In addition to presenting these conceptually linked examples, I provide a general overview of the engineering process, outline important next steps, and highlight larger opportunities.",16-126,2021.0,https://oneill.biz/app/tag/taglogin.html,Computer Science
1007,ec93a59a495cb1ef4c95a5ecf0e9fa238e88c556,Four Generations in Data Engineering for Data Science,,59 - 66,2021.0,http://adams.com/categoriesprivacy.htm,Computer Science
1008,8f8532a193313b9b956a8df402dd7f879bbe1377,"Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies","Consider the situation where a data analyst wishes to carry out an analysis on a given dataset. It is widely recognized that most of the analyst's time will be taken up with \emph{data engineering} tasks such as acquiring, understanding, cleaning and preparing the data. In this paper we provide a description and classification of such tasks into high-levels groups, namely data organization, data quality and feature engineering. We also make available four datasets and example analyses that exhibit a wide variety of these problems, to help encourage the development of tools and techniques to help reduce this burden and push forward research towards the automation or semi-automation of the data engineering process.",32-137,2020.0,https://bailey-guerra.com/categoriesauthor.php,Computer Science
1009,7ce809ac8bb350f5ac7755b4e861710064b38654,High Performance Data Engineering Everywhere,"The amazing advances being made in the fields of machine and deep learning are a highlight of the Big Data era for both enterprise and research communities. Modern applications require resources beyond a single node's ability to provide. However this is just a small part of the issues facing the overall data processing environment, which must also support a raft of data engineering for pre- and post-data processing, communication, and system integration. An important requirement of data analytics tools is to be able to easily integrate with existing frameworks in a multitude of languages, thereby increasing user productivity and efficiency. All this demands an efficient and highly distributed integrated approach for data processing, yet many of today's popular data analytics tools are unable to satisfy all these requirements at the same time. In this paper we present Cylon, an open-source high performance distributed data processing library that can be seamlessly integrated with existing Big Data and AI/ML frameworks. It is developed with a flexible C++ core on top of a compact data structure and exposes language bindings to C++, Java, and Python. We discuss Cylon's architecture in detail, and reveal how it can be imported as a library to existing applications or operate as a standalone framework. Initial experiments show that Cylon enhances popular tools such as Apache Spark and Dask with major performance improvements for key operations and better component linkages. Finally, we show how its design enables Cylon to be used cross-platform with minimum overhead, which includes popular AI tools such as PyTorch, Tensorflow, and Jupyter notebooks.",122-132,2020.0,https://cooper.net/blog/explorehome.html,Computer Science
1010,ef4ac59a175e70c029f95b69f7c1f70a6ca963ef,Data Engineering 4.0,,29-128,2021.0,https://bauer.com/main/posts/categoryhome.php,Technology
1011,ec44a0edabfeef029fe7bf2f02784d50cc2e7fa8,BIG DATA ENGINEERING,,20-147,2021.0,https://www.odonnell-phillips.com/appterms.asp,Technology
1012,b931d52d7d72cac8cfc104f1945fb513f60ef3e3,Data Engineering for HPC with Python,"Data engineering is becoming an increasingly important part of scientific discoveries with the adoption of deep learning and machine learning. Data engineering deals with a variety of data formats, storage, data extraction, transformation, and data movements. One goal of data engineering is to transform data from original data to vector/matrix/tensor formats accepted by deep learning and machine learning applications. There are many structures such as tables, graphs, and trees to represent data in these data engineering phases. Among them, tables are a versatile and commonly used format to load and process data. In this paper, we present a distributed Python API based on table abstraction for representing and processing data. Unlike existing state-of-the-art data engineering tools written purely in Python, our solution adopts high performance compute kernels in C++, with an in-memory table representation with Cython-based Python bindings. In the core system, we use MPI for distributed memory computations with a data-parallel approach for processing large datasets in HPC clusters.",13-21,2020.0,http://www.henderson.org/app/listabout.php,Computer Science
1013,15d8143cccd70a82d43c342e92a8808f6232dea3,Data Engineering for Data Science: Two Sides of the Same Coin,,157-166,2020.0,https://robinson-mccarthy.org/explorehomepage.html,Computer Science
1014,6ae876357f168a852028d97531c207321e10fce6,Challenging Big Data Engineering: Positioning of Current and Future Development,"This contribution examines the terms of big data and big data engineering, considering the specific characteristics and challenges. Deduced by those, it concludes the need for new ways to support the creation of corresponding systems to help big data in reaching its full potential. In the following, the state of the art is analysed and subdomains in the engineering of big data solutions are presented. In the end, a possible concept for filling the identified gap is proposed and future perspectives are highlighted.",351-358,2019.0,https://www.montgomery-ryan.com/tagsregister.htm,Computer Science
1015,090cfa03354d8c367d31f52cea7582fde6ecd572,Special Section on the International Conference on Data Engineering 2016,"The papers in this special section were presented at the 32nd International Conference on Data Engineering that was held in Helsinki, Finland, May 16- May 20, 2016.",1-2,2019.0,http://smith.com/blog/list/postsindex.htm,Computer Science
1016,8652971bdb15a176e48ce5af32453ad06a2b8901,Data Engineering,,86-128,2019.0,http://price-smith.org/tagshomepage.jsp,Technology
1017,aeca9ab416adb5a514e57fa41705a71918c48488,Learning data engineering: Creating IoT apps using the node-RED and the RPI technologies,"This paper demonstrates the suitability and the practicality of using the advanced open source tools such as the Raspberry Pi and the Node-RED for teaching and learning in the Internet of Things (IOT) subject within a newly created major of Data Engineering in the Faculty of Engineering and IT at University of Technology, Sydney. Understanding and practicing of the Internet of Things largely depend on the high availability of tools, their low cost, and ease of use that can accelerate learning processes. This paper demonstrates relatively uncomplicated practical lab exercises involving the Raspberry Pi hardware, firmware and the Node-RED programming environment that students can execute to stimulate their learning, understanding of the Internet of Things technology and acquire fundamental data engineering skills.",1-8,2017.0,https://www.hernandez.com/explore/listhomepage.html,Computer Science
1018,401a4dc0c07aec4b3ed624eeb19c7cbea7d602e7,Special Section on the International Conference on Data Engineering 2015,"The papers in this special section were presented at the 31st International Conference on Data Engineering that was held in Seoul, Korea, on April 13-17, 2015. 17, 2015.",497-498,2017.0,https://www.lopez.com/poststerms.htm,Computer Science
1019,c48e015debf92bd6a02354a8b3b271c440ec56dc,Some key problems of data management in army data engineering based on big data,"This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.",149-152,2017.0,https://www.gentry.com/tags/posts/tagsabout.htm,Computer Science
1020,cbb9cdf9f7e233948420a479831e9d2d19f72677,"Enterprise Big Data Engineering, Analytics, and Management","The significance of big data can be observed in any decision-making process as it is often used for forecasting and predictive analytics. Additionally, big data can be used to build a holistic view of an enterprise through a collection and analysis of large data sets retrospectively. As the data deluge deepens, new methods for analyzing, comprehending, and making use of big data become necessary.Enterprise Big Data Engineering, Analytics, and Management presents novel methodologies and practical approaches to engineering, managing, and analyzing large-scale data sets with a focus on enterprise applications and implementation. Featuring essential big data concepts including data mining, artificial intelligence, and information extraction, this publication provides a platform for retargeting the current research available in the field. Data analysts, IT professionals, researchers, and graduate-level students will find the timely research presented in this publication essential to furthering their knowledge in the field.",63-137,2016.0,https://trujillo.com/searchregister.html,Computer Science
1021,3d7e34ca30e1aacefc27c7bcd16753791f622f06,Enabling Combined Software and Data Engineering at Web-Scale: The ALIGNED Suite of Ontologies,,195-203,2016.0,http://nunez.com/searchsearch.html,Computer Science
1022,59b43d261fddb909ef9b79e09a557da8f4b925a7,Intelligence Science and Big Data Engineering. Image and Video Data Engineering,,99-128,2015.0,http://young.org/search/list/tagfaq.htm,Computer Science
1023,dd153ebd44a07dde2259c22d43bb9cd18db44d2a,Modelling and Simulation in Materials Science and Engineering Visualization and analysis of atomistic simulation data with OVITO – the Open Visualization Tool,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/. (Some figures in this article are in colour only in the electronic version)",37-125,2009.0,https://bailey.com/main/app/appregister.html,Technology
1024,f3eda875e14bf933759f3b777131a4a9973537b4,"Data-driven science and engineering: machine learning, dynamical systems, and control",,57-137,2019.0,http://www.willis-hays.com/explorehome.asp,Computer Science
1025,133bcd7488a3c07cb0f493a87564c30e5433768c,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,852 - 857,2019.0,http://www.mcgee-porter.org/posts/app/tagsabout.jsp,Engineering
1026,5eb1b872bd1ded1a293935697eb7f0af37bf6635,Ieee Transactions on Knowledge and Data Engineering 1 Training Cost-sensitive Neural Networks with Methods Addressing the Class Imbalance Problem,"— This paper studies empirically the effect of sampling and threshold-moving in training cost-sensitive neural networks. Both over-sampling and under-sampling are considered. These techniques modify the distribution of the training data such that the costs of the examples are conveyed explicitly by the appearances of the examples. Threshold-moving tries to move the output threshold toward inexpensive classes such that examples with higher costs become harder to be misclassified. Moreover, hard-ensemble and soft-ensemble, i.e. the combination of above techniques via hard or soft voting schemes, are also tested. Twenty-one UCI data sets with three types of cost matrices and a real-world cost-sensitive data set are used in the empirical study. The results suggest that cost-sensitive learning with multi-class tasks is more difficult than with two-class tasks, and a higher degree of class imbalance may increase the difficulty. It also reveals that almost all the techniques are effective on two-class tasks, while most are ineffective and even may cause negative effect on multi-class tasks. Overall, threshold-moving and soft-ensemble are relatively good choices in training cost-sensitive neural networks. The empirical study also suggests that some methods that have been believed to be effective in addressing the class imbalance problem may in fact only be effective on learning with imbalanced two-class data sets.",54-120,,https://www.rogers.com/main/blog/wp-contentregister.html,Technology
1027,e5df137f99c1131bd3dce60e246e62247fc9affb,Ieee Transactions on Knowledge and Data Engineering 1 Multi-label Neural Networks with Applications to Functional Genomics and Text Categorization,"— In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e. Backpropagation for Multi-Label Learning, is proposed. It is derived from the popular Backpropagation algorithm through employing a novel error function capturing the characteristics of multi-label learning, i.e. the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real-world multi-label learning problems, i.e. functional genomics and text categorization, show that the performance of BP-MLL is superior to those of some well-established multi-label learning algorithms.",52-114,,https://www.torres.com/tags/main/tagspost.php,Technology
1028,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,ImageJ2: ImageJ for the next generation of scientific image data,,42-112,2017.0,https://www.medina.com/explore/categories/appfaq.asp,Computer Science
1029,d553d008f643622e87e3ac061226865cad3b2928,Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,"Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.",1-9,2023.0,http://www.johnson.com/blog/blog/searchfaq.php,Computer Science
1030,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,Data-Driven Science and Engineering,,80-106,2019.0,http://www.farrell-brown.info/categoriesfaq.html,Computer Science
1031,7fcef094c80b31a7ef61c50466c736c148001e91,IEEE Transactions on Knowledge and Data Engineering,"— The Unified Modeling Language (UML) supports the capture and representation of conceptual semantics in analysis and modeling of arbitrary domains of discourse. It is now being used for modeling of database and agent-oriented applications in addition to its use with object-oriented software. An important activity in conceptual modeling is the capture and expression of domain constraints on the underlying data and object states. UML supports the capture of declarative constraints both directly in the graphical class diagram notation and in the separately denoted Object Constraint Language (OCL) syntax. In this paper, we present a set of constraint patterns that have been formalized for capturing a broader category of semantic constraints directly using the class diagram notation. The objective of this work is to enrich UML static models for greater precision in conceptual modeling applications by formalizing the syntax and semantics of these constraint patterns, thus giving analysts a tool for capturing these common patterns directly in the UML class diagram with a minimum of additional notation—without requiring a separate expression of constraints in the adjunct OCL syntax. We present a taxonomy for propositional constraints, discuss their semantics through the use of logic truth tables and Karnaugh maps, and illustrate their usage in UML class diagrams by way of an example from the Unix security domain.",63-131,,http://www.white.com/search/tagterms.htm,Technology
1032,9cfe870e09f627e2814572aa4e1e7bff8b657fc5,Low-N protein engineering with data-efficient deep learning,,389 - 396,2020.0,http://www.hayden.org/posts/posts/wp-contentcategory.html,Biology
1033,46d55edae7cf5f065bb037462a7951c220f42618,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",8410 - 8415,2014.0,http://www.parker.org/list/bloglogin.asp,Medicine
1034,5abf1c0ff7dc9157aedd9dfa021f8d3dcc647d9b,Ieee Transactions on Knowledge and Data Engineering a Survey of Heterogeneous Information Network Analysis Ieee Transactions on Knowledge and Data Engineering 2,"Most real systems consist of a large number of interacting, multi-typed components, while most contemporary researches model them as homogeneous networks, without distinguishing different types of objects and links in the networks. Recently, more and more researchers begin to consider these interconnected, multi-typed data as heterogeneous information networks, and develop structural analysis approaches by leveraging the rich semantic meaning of structural types of objects and links in the networks. Compared to widely studied homogeneous network, the heterogeneous information network contains richer structure and semantic information, which provides plenty of opportunities as well as a lot of challenges for data mining. In this paper, we provide a survey of heterogeneous information network analysis. We will introduce basic concepts of heterogeneous information network analysis, examine its developments on different data mining tasks, discuss some advanced topics, and point out some future research directions.",26-127,,https://www.austin.org/list/category/explorelogin.php,Technology
1035,f80b4316419482031c0ee28ab96ee2051c9863b8,Sustainable industrial and operation engineering trends and challenges Toward Industry 4.0: a data driven analysis,"ABSTRACT This study supplies contributions to the existing literature with a state-of-the-art bibliometric review of sustainable industrial and operation engineering as the field moves toward Industry 4.0, and guidance for future studies and practical achievements. Although industrial and operation engineering is being promoted forward to sustainability, the systematization of the knowledge that forms firms’ manufacturing and operations and encompasses their wide concepts and abundant complementary elements is still absent. This study aims to analyze contemporary sustainable industrial and operations engineering in Industry 4.0 context. The bibliometric analysis and fuzzy Delphi method are proposed. Resulting in a total of 30 indicators that are criticized and clustered into eight study groups, including lean manufacturing in Industry 4.0, cyber-physical production system, big data-driven and smart communications, safety and security, artificial intelligence for sustainability, the circular economy in a digital environment, business intelligence and virtual reality, and environmental sustainability. Graphical Abstract",581 - 598,2021.0,http://hall.com/blogsearch.html,Engineering
1036,1b3dfa08444f249934d61ab01139b9eafa64f94c,Ieee Transactions on Knowledge and Data Engineering Random K-labelsets for Multi-label Classification,"—A simple yet effective multi-label learning method, called label powerset (LP), considers each distinct combination of labels that exist in the training set as a different class value of a single-label classification task. The computational efficiency and predictive performance of LP is challenged by application domains with large number of labels and training examples. In these cases the number of classes may become very large and at the same time many classes are associated with very few training examples. To deal with these problems, this paper proposes breaking the initial set of labels into a number of small random subsets, called labelsets and employing LP to train a corresponding classifier. The labelsets can be either disjoint or overlapping depending on which of two strategies is used to construct them. The proposed method is called RAkEL (RAndom k labELsets), where k is a parameter that specifies the size of the subsets. Empirical evidence indicate that RAkEL manages to improve substantially over LP, especially in domains with large number of labels and exhibits competitive performance against other high-performing multi-label learning methods.",48-118,,http://heath.info/main/blog/categoriesauthor.php,Technology
1037,92e4b8e3546aa157c189c427e1ec24dfd6e3b511,Ieee Transactions on Knowledge and Data Engineering Discovering Co-location Patterns from Spatial Datasets: a General Approach Ieee Transactions on Knowledge and Data Engineering,"Given a collection of boolean spatial features, the co-location pattern discovery process finds the subsets of features frequently located together. For example, the analysis of an ecology dataset may reveal symbiotic species. The spatial co-location rule problem is different from the association rule problem, since there is no natural notion of transactions in spatial data sets which are embeded in continuous geographic space. In this paper, we provide a transaction-free approach to mine co-location patterns by using the concept of proximity neighborhood. A new interest measure, a participation index, is also proposed for spatial co-location patterns. The participation index is used as the measure of prevalence of a co-location for two reasons. First, this measure is closely related to the cross-function, which is often used as a statistical measure of interaction among pairs of spatial features. Second, it also possesses an anti-monotone property which can be exploited for computational efficiency. Furthermore, we design an algorithm to discover co-location patterns. This algorithm includes a novel multi-resolution pruning technique. Finally, experimental results are provided to show the strength of the algorithm and design decisions related to performance tuning.",27-127,,https://marshall.com/blog/category/listpost.php,Technology
1038,de3294f2462650165c282356a353081b1f7cf02a,Bulletin of the Technical Committee on Data Engineering Special Issue on Workflow and Extended Transaction Systems Conference and Journal Notices Editorial Board Editor-in-chief Tc Executive Committee Important Membership Message-repeated from March Issue. Electronic Enrollment Procedure Alternative,"The Bulletin of the Technical Committee on Data Engineering is published quarterly and is distributed to all TC members. Its scope includes the design, implementation, modelling, theory and application of database systems and their technology. Letters, conference information, and news should be sent to the Editor-in-Chief. Papers for each issue are solicited by and should be sent to the Associate Editor responsible for the issue. Opinions expressed in contributions are those of the authors and do not necessarily reflect the positions of the TC on Data Engineering, the IEEE Computer Society, or the authors' organizations. Membership in the TC on Data Engineering is open to all current members of the IEEE Computer Society who are interested in database systems. There is both good news and bad news in this letter. The good news is that we are well on our way to being able to distribute the Bulletin electronically. This low cost method of distribution should ensure the long term economic survival of the Bulletin. It should permit us to continue to provide the bulletin to all members free of charge. The bad news is that if you do not re-enroll as a member of the Technical Committee, then the June 1993 hardcopy issue of the Bulletin is the last copy of the Bulletin that you will receive. Our annual revenue, which comes almost entirely from sponsoring the Data Engineering Conference, is not sufficient to cover the costs of printing and distributing four issues of the Bulletin a year. Four issues is, we agree, the minimum publication schedule that is reasonable in order to bring you timely information on technical and professional subjects of interest in data engineering. Long term survival then requires that we limit free distribution to those that can receive the Bulletin electronically. We are working on arranging hardcopy distribution via paid subscription for those that are not reachable electronically or who simply prefer receiving hardcopy. The annual subscription fee for four issues is expected to be in the $10 to $15 range. Please be aware that failure to enroll means that you will not remain a TC member, and hence that you will no longer receive the Bulletin. Electronic enrollment is the preferred method of becoming a member of the Technical Committee on Data Engineering. To enroll electronically, please use the following procedure: 1. Send e-mail to TCData@crl.dec.com and include in the subject header the word …",44-142,,http://www.harvey.com/app/tagsprivacy.php,Technology
1039,d39893610b12821ada3964844460a0a0da08131d,Ieee Transactions on Knowledge and Data Engineering 1 Patch Alignment for Dimensionality Reduction,"—Spectral analysis based dimensionality reduction algorithms are important and have been popularly applied in data mining and computer vision applications. To date many algorithms have been developed, e.g., principal component analysis, locally linear embedding, Laplacian eigenmaps, and local tangent space alignment. All of these algorithms have been designed intuitively and pragmatically, i.e., on the base of the experience and knowledge of experts for their own purposes. Therefore, it will be more informative to provide a systematic framework for understanding the common properties and intrinsic difference in different algorithms. In this paper, we propose such a framework, named "" patch alignment "" , which consists of two stages: part optimization and whole alignment. The framework reveals that: i) algorithms are intrinsically different in the patch optimization stage; and ii) all algorithms share an almost-identical whole alignment stage. As an application of this framework, we develop a new dimensionality reduction algorithm, termed Discriminative Locality Alignment (DLA), by imposing discriminative information in the part optimization stage. DLA can: i) attack the distribution nonlinearity of measurements; ii) preserve the discriminative ability; and iii) avoid the small sample size problem. Thorough empirical studies demonstrate the effectiveness of DLA compared with representative dimensionality reduction algorithms.",20-118,,https://tran.org/appterms.htm,Technology
1040,f7248cb412926dce46b3b9a4af8bb42f693dd287,Bulletin of the Technical Committee on Data Engineering Special Issue on Scientific Databases Conference and Journal Notices Editorial Board Editor-in-chief Tc Executive Committee Important Membership Message Electronic Enrollment Procedure Alternative Enrollment Procedure the Evolution of the Bulle,"The Bulletin of the Technical Committee on Data Engineering is published quarterly and is distributed to all TC members. Its scope includes the design, implementation, modelling, theory and application of database systems and their technology. Letters, conference information, and news should be sent to the Editor-in-Chief. Papers for each issue are solicited by and should be sent to the Associate Editor responsible for the issue. Opinions expressed in contributions are those of the authors and do not necessarily reflect the positions of the TC on Data Engineering, the IEEE Computer Society, or the authors' organizations. Membership in the TC on Data Engineering is open to all current members of the IEEE Computer Society who are interested in database systems. There is both good news and bad news in this letter. The good news is that we are well on our way to being able to distribute the Bulletin electronically. This low cost method of distribution should ensure the long term economic survival of the Bulletin. It should permit us to continue to provide the bulletin to all members free of charge. The bad news is that if you do not re-enroll as a member of the Technical Committee, then the June 1993 hardcopy issue of the Bulletin is the last copy of the Bulletin that you will receive. Our annual revenue, which comes almost entirely from sponsoring the Data Engineering Conference, is not sufficient to cover the costs of printing and distributing four issues of the Bulletin a year. Four issues is, we agree, the minimum publication schedule that is reasonable in order to bring you timely information on technical and professional subjects of interest in data engineering. Long term survival then requires that we limit free distribution to those that can receive the Bulletin electronically. We are working on arranging hardcopy distribution via paid subscription for those that are not reachable electronically or who simply prefer receiving hardcopy. The annual subscription fee for four issues is expected to be in the $10 to $15 range. Please be aware that failure to enroll means that you will not remain a TC member, and hence that you will no longer receive the Bulletin. Electronic enrollment is the preferred method of becoming a member of the Technical Committee on Data Engineering. To enroll electronically, please use the following procedure: 1. Send e-mail to TCData@crl.dec.com and include in the subject header the word …",25-133,,https://www.dean.com/tag/categorylogin.html,Technology
1041,7b061ae47c54dfaeb64296cc2fe9b0e65ced74e5,Bulletin of the Technical Committee on Data Engineering Special Issue on Supporting On-line Analytical Processing Conference and Journal Notices Editorial Board Editor-in-chief Tc Executive Committee Letter from the Editor-in-chief Bulletin and Technical Committee This Issue Letter from the Special ,"The Bulletin of the Technical Committee on Data Engineering is published quarterly and is distributed to all TC members. Its scope includes the design, implementation, modelling, theory and application of database systems and their technology. Letters, conference information, and news should be sent to the Editor-in-Chief. Papers for each issue are solicited by and should be sent to the Associate Editor responsible for the issue. Opinions expressed in contributions are those of the authors and do not necessarily reflect the positions of the TC on Data Engineering, the IEEE Computer Society, or the authors' organizations. Membership in the TC on Data Engineering is open to all current members of the IEEE Computer Society who are interested in database systems. This begins my fifth year as Editor-in-Chief of the Data Engineering Bulletin. This has been both a rewarding experience and a time-consuming one. I have been fortunate to have worked with many fine issue editors over the years, without whose efforts, the Bulletin simply would not have happened. I want to thank them all and express the hope that my good fortune will continue in the future. The problem of assuring funding for the Bulletin was with us from the time the Bulletin was revived in De-cember, 1992. It continues. Fortunately, every year thus far, including this year, the IEEE Computer Society has agreed to fund us yet again. This is gratifying but also potentially undatable for the longer term. Literally, at the beginning of each calendar year, I ask whether our funding continues. So far, so good, but.... Meanwhile, the Technical Committee on Data Engineering, aside from the Bulletin, has become relatively dormant. I have not seen more than two or three messages about TC affairs over the last year. This is not a good situation. Does the TC have a role outside of the Bulletin. What about the SIGMOD model? Is there room for/do we need two database organizations-one for the ACM and one for the IEEE, or is this just an historical accident? This is more than a rhetorical question. I am prepared to devote scarce Bulletin pages to letters on this subject. I would encourage you to write with your suggestions for the future of the TCDE. Daniel Barbará is the issue editor for this special issue on on-line analytical processing (OLAP). OLAP has become in a few short years one of the major applications of database …",71-104,,http://johnson.biz/category/exploreauthor.html,Technology
1042,b6d20892f150ef8bf9ca6a7db4bcd4a4e0c9552b,"Ieee Transactions on Knowledge and Data Engineering, to Appear Final Draft 1 a Guide to the Literature on Learning Probabilistic Networks from Data","| This literature review discusses diierent methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities , and between the diierent methodological communities, such a s B a y esian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The presentation avoids formal deenitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simpliied examples.",73-125,,http://www.davila-francis.com/searchterms.htm,Technology
1043,d986bd9f699241bc9d7c75f609bcef82369bfefc,"Ieee Transactions on Knowledge and Data Engineering Name, Manuscript Id 1 a Knowledge-driven Approach to Activity Recognition in Smart Homes","—This paper introduces a knowledge-driven approach to real-time, continuous activity recognition based on multi-sensor data streams in smart homes. The approach goes beyond the traditional data-centric methods for activity recognition in three ways. Firstly, it makes extensive use of domain knowledge in the lifecycle of activity recognition. Secondly, it uses ontologies for explicit context and activity modeling and representation. Thirdly and finally, it exploits semantic reasoning and classification for activity inferencing, thus enabling both coarse-grained and fine-grained activity recognition. In this paper we analyse the characteristics of smart homes and Activities of Daily Living (ADL) upon which we built both context and ADL ontologies. We present a generic system architecture for the proposed knowledge-driven approach and describe the underlying ontology-based recognition process. Special emphasis is placed on semantic subsumption reasoning algorithms for activity recognition. The proposed approach has been implemented in a function-rich software system, which was deployed in a smart home research laboratory. We evaluated the proposed approach and the developed system through extensive experiments involving a number of various ADL use scenarios. An average activity recognition rate of 94.44% was achieved and the average recognition run-time per recognition operation was measured as 2.5 seconds.",63-136,,http://jackson-schultz.com/wp-content/wp-contentindex.asp,Technology
1044,38939d0abd688e22113561d4a7aad94638ffa8aa,Ieee Transactions on Knowledge and Data Engineering. Discovering Activities to Recognize and Track in a Smart Environment,"—The machine learning and pervasive sensing technologies found in smart homes offer unprecedented opportunities for providing health monitoring and assistance to individuals experiencing difficulties living independently at home. In order to monitor the functional health of smart home residents, we need to design technologies that recognize and track activities that people normally perform as part of their daily routines. Although approaches do exist for recognizing activities, the approaches are applied to activities that have been pre-selected and for which labeled training data is available. In contrast, we introduce an automated approach to activity tracking that identifies frequent activities that naturally occur in an individual's routine. With this capability we can then track the occurrence of regular activities to monitor functional health and to detect changes in an individual's patterns and lifestyle. In this paper we describe our activity mining and tracking approach and validate our algorithms on data collected in physical smart environments.",43-104,,https://www.moody.com/list/tags/categoryauthor.php,Technology
1045,f14ec263536aa6a013ecf1c32c8320be50ab0a8d,Ieee Transactions on Knowledge and Data Engineering 1 Resampling-based Ensemble Methods for Online Class Imbalance Learning,"—Online class imbalance learning is a new learning problem that combines the challenges of both online learning and class imbalance learning. It deals with data streams having very skewed class distributions. This type of problems commonly exists in real-world applications, such as fault diagnosis of real-time control monitoring systems and intrusion detection in computer networks. In our earlier work, we defined class imbalance online, and proposed two learning algorithms OOB and UOB that build an ensemble model overcoming class imbalance in real time through resampling and time-decayed metrics. In this paper, we further improve the resampling strategy inside OOB and UOB, and look into their performance in both static and dynamic data streams. We give the first comprehensive analysis of class imbalance in data streams, in terms of data distributions, imbalance rates and changes in class imbalance status. We find that UOB is better at recognizing minority-class examples in static data streams, and OOB is more robust against dynamic changes in class imbalance status. The data distribution is a major factor affecting their performance. Based on the insight gained, we then propose two new ensemble methods that maintain both OOB and UOB with adaptive weights for final predictions, called WEOB1 and WEOB2. They are shown to possess the strength of OOB and UOB with good accuracy and robustness.",61-147,,http://anderson.com/tags/exploreauthor.php,Technology
1046,0da52a5651004a7fe0411b036cf898914b672e79,Ieee Transactions on Knowledge and Data Engineering 1 Conditional Anomaly Detection,"— When anomaly detection software is used as a data analysis tool, finding the hardest-to-detect anomalies is not the most critical task. Rather, it is often more important to make sure that those anomalies that are reported to the user are in fact interesting. If too many unremarkable data points are returned to the user labeled as candidate anomalies, the software will soon fall into disuse. One way to ensure that returned anomalies are useful is to make use of domain knowledge provided by the user. Often, the data in question include a set of environmental attributes whose values a user would never consider to be directly indicative of an anomaly. However, such attributes cannot be ignored because they have a direct effect on the expected distribution of the result attributes whose values can indicate an anomalous observation. This paper describes a general-purpose method called conditional anomaly detection for taking such differences among attributes into account, and proposes three different expectation-maximization algorithms for learning the model that is used in conditional anomaly detection. Experiments over 13 different data sets compare our algorithms with several other more standard methods for outlier or anomaly detection.",45-123,,http://www.roberts.info/app/categoryfaq.htm,Technology
1047,557f7747f1a94cadd3f10eea6bdbc80e09b1e181,Ieee Transactions on Knowledge and Data Engineering 1 an Empirical Performance Evaluation of Relational Keyword Search Techniques,"—Extending the keyword search paradigm to relational data has been an active area of research within the database and information retrieval (IR) community during the past decade. Many approaches have been proposed, but despite numerous publications, there remains a severe lack of standardization for the evaluation of proposed search techniques. Lack of standardization has resulted in contradictory results from different evaluations, and the numerous discrepancies muddle what advantages are proffered by different approaches. In this paper, we present the most extensive empirical performance evaluation of relational keyword search techniques to appear to date in the literature. Our results indicate that many existing search techniques do not provide acceptable performance for realistic retrieval tasks. In particular, memory consumption precludes many search techniques from scaling beyond small data sets with tens of thousands of vertices. We also explore the relationship between execution time and factors varied in previous evaluations; our analysis indicates that most of these factors have relatively little impact on performance. In summary, our work confirms previous claims regarding the unacceptable performance of these search techniques and underscores the need for standardization in evaluations—standardization exemplified by the IR community.",82-106,,https://www.bray-smith.com/category/tags/exploresearch.html,Technology
1048,710e084ac6089f7811929a915fbe8286acea50f0,"Ieee Transactions on Knowledge and Data Engineering, Manuscropt Id 1 Scalable Recommendation with Social Contextual Information","—Exponential growth of information generated by online social networks demands effective and scalable recommender systems to give useful results. Traditional techniques become unqualified because they ignore social relation data; existing social recommendation approaches consider social network structure, but social contextual information has not been fully considered. It is significant and challenging to fuse social contextual factors which are derived from users' motivation of social behaviors into social recommendation. In this paper, we investigate the social recommendation problem on the basis of psychology and sociology studies, which exhibit two important factors: individual preference and interpersonal influence. We first present the particular importance of these two factors in online behavior prediction. Then we propose a novel probabilistic matrix factorization method to fuse them in latent space. We further provide a scalable algorithm which can incrementally process the data. We conduct experiments on both Facebook style bidirectional and Twitter style unidirectional social network datasets. The empirical results and analysis on these two large datasets demonstrate that our method significantly outperforms the existing approaches.",33-113,,http://www.anderson.net/applogin.htm,Technology
1049,ae551d49e07e257ae78531371739eacdf9a5fe88,Ieee Transactions on Knowledge and Data Engineering 1 an Adaptive Approach to Real-time Aggregate Monitoring with Differential Privacy,"—Sharing real-time aggregate statistics of private data is of great value to the public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, a novel framework to release real-time aggregate statistics under differential privacy based on filtering and adaptive sampling. To minimize the overall privacy cost, FAST adaptively samples long time-series according to the detected data dynamics. To improve the accuracy of data release per time stamp, FAST predicts data values at non-sampling points and corrects noisy observations at sampling points. Our experiments with real-world as well as synthetic data sets confirm that FAST improves the accuracy of released aggregates even under small privacy cost and can be used to enable a wide range of monitoring applications.",17-118,,http://finley.com/app/tag/exploreindex.html,Technology
1050,dc7e5529b603a63eec0a42fb72ca6eddee68f229,Ieee Transactions on Knowledge and Data Engineering Decision Trees for Mining Data Streams Based on the Mcdiarmid's Bound,"—In mining data streams the most popular tool is the Hoeffding tree algorithm. It uses the Hoeffding's bound to determine the smallest number of examples needed at a node to select a splitting attribute. In literature the same Hoeffding's bound was used for any evaluation function (heuristic measure), e.g. information gain or Gini index. In this paper it is shown that the Hoeffding's inequality is not appropriate to solve the underlying problem. We prove two theorems presenting the McDiarmid's bound for both the information gain, used in ID3 algorithm, and for Gini index, used in CART algorithm. The results of the paper guarantee that a decision tree learning system, applied to data streams and based on the McDiarmid's bound, has the property that its output is nearly identical to that of a conventional learner. The results of the paper have a great impact on the state of the art of mining data streams and various developed so far methods and algorithms should be reconsidered.",58-132,,http://wilson.com/blogregister.asp,Technology
1051,5c024ddb73d55a13607e1b151e980e915ae49145,Ieee Transactions on Knowledge and Data Engineering Probabilistic Memory-based Collaborative Filtering,"— Memory-based collaborative filtering (CF) has been studied extensively in the literature and has proven to be successful in various types of personalized recommender systems. In this paper we develop a probabilistic framework for memory-based CF (PMCF). While this framework has clear links with classical memory-based CF, it allows us to find principled solutions to known problems of CF-based recommender systems. In particular , we show that a probabilistic active learning method can be used to actively query the user, thereby solving the "" new user problem "". Furthermore, the probabilistic framework allows us to reduce the computational cost of memory-based CF by working on a carefully selected subset of user profiles, while retaining high accuracy. We report experimental results based on two real world data sets, which demonstrate that our proposed PMCF framework allows an accurate and efficient prediction of user preferences.",69-115,,https://www.roberts.com/tag/explorefaq.htm,Technology
1052,0c13cd81b56d444b9fb97ce8d8657a7f71cb80ba,Ieee Transactions on Knowledge and Data Engineering 1 a General Geographical Probabilistic Factor Model for Point of Interest Recommendation,"—The problem of point of interest (POI) recommendation is to provide personalized recommendations of places, such as restaurants and movie theaters. The increasing prevalence of mobile devices and of location based social networks (LBSNs) poses significant new opportunities as well as challenges, which we address. The decision process for a user to choose a POI is complex and can be influenced by numerous factors, such as personal preferences, geographical considerations, and user mobility behaviors. This is further complicated by the connection LBSNs and mobile devices. While there are some studies on POI recommendations, they lack an integrated analysis of the joint effect of multiple factors. Meanwhile, although latent factor models have been proved effective and are thus widely used for recommendations, adopting them to POI recommendations requires delicate consideration of the unique characteristics of LBSNs. To this end, in this paper, we propose a general geographical probabilistic factor model (Geo-PFM) framework which strategically takes various factors into consideration. Specifically, this framework allows to capture the geographical influences on a user's check-in behavior. Also, user mobility behaviors can be effectively leveraged in the recommendation model. Moreover, based our Geo-PFM framework, we further develop a Poisson Geo-PFM which provides a more rigorous probabilistic generative process for the entire model and is effective in modeling the skewed user check-in count data as implicit feedback for better POI recommendations. Finally, extensive experimental results on three real-world LBSN datasets (which differ in terms of user mobility, POI geographical distribution, implicit response data skewness, and user-POI observation sparsity), show that the proposed recommendation methods outperform state-of-the-art latent factor models by a significant margin.",88-117,,http://www.leon.com/postsauthor.html,Technology
1053,0b19d84d3c44222ab0d3965eedada3a037971bc3,Ieee Transactions on Knowledge and Data Engineering 1 Overlapping Community Detection Using Neighborhood-inflated Seed Expansion,"—Community detection is an important task in network analysis. A community (also referred to as a cluster) is a set of cohesive vertices that have more connections inside the set than outside. In many social and information networks, these communities naturally overlap. For instance, in a social network, each vertex in a graph corresponds to an individual who usually participates in multiple communities. In this paper, we propose an efficient overlapping community detection algorithm using a seed expansion approach. The key idea of our algorithm is to find good seeds, and then greedily expand these seeds based on a community metric. Within this seed expansion method, we investigate the problem of how to determine good seed nodes in a graph. In particular, we develop new seeding strategies for a personalized PageRank clustering scheme that optimizes the conductance community score. An important step in our method is the neighborhood inflation step where seeds are modified to represent their entire vertex neighborhood. Experimental results show that our seed expansion algorithm outperforms other state-of-the-art overlapping community detection methods in terms of producing cohesive clusters and identifying ground-truth communities. We also show that our new seeding strategies are better than existing strategies, and are thus effective in finding good overlapping communities in real-world networks.",38-125,,https://www.sellers.org/category/tag/wp-contentmain.asp,Technology
1054,6eebfa288a2fbd9d42b42ce0d25ebd2c76a74a51,Ieee Transactions on Knowledge and Data Engineering 1 Distributional Features for Text Categorization,"— Text categorization is the task of assigning prede-fined categories to natural language text. With the widely used 'bag of words' representation, previous researches usually assign a word with values such that whether this word appears in the document concerned or how frequently this word appears. Although these values are useful for text categorization, they have not fully expressed the abundant information contained in the document. This paper explores the effect of other types of values, which express the distribution of a word in the document. These novel values assigned to a word are called distributional features, which include the compactness of the appearances of the word and the position of the first appearance of the word. The proposed distributional features are exploited by a tfidf style equation and different features are combined using ensemble learning techniques. Experiments show that the distributional features are useful for text categorization. In contrast to using the traditional term frequency values solely, including the distributional features requires only a little additional cost, while the categorization performance can be significantly improved. Further analysis shows that the distributional features are especially useful when documents are long and the writing style is casual.",40-130,,https://www.thomas.net/list/search/listabout.html,Technology
1055,3d6248c64394f6cc791f5037ee347517b85144c8,Ieee Transactions on Knowledge and Data Engineering 1 Location Fingerprinting in a Decorrelated Space,"—We present a novel approach to the problem of the indoor localization in wireless environments. The main contribution of this paper is four folds: (a) We show that, by projecting the measured signal into a decorrelated signal space, the positioning accuracy is improved since the cross correlation between each AP is reduced. (b) We demonstrate that this novel approach achieves a more efficient information compaction and provides a better scheme to reduce online computation. The drawback of AP selection techniques is overcome since we reduce the dimensionality by combing features. Each component in the decorrelated space is the linear combination of all APs. Therefore a more efficient mechanism is provided to utilize information of all APs while reducing the computational complexity. (c) Experimental results show that the size of training samples can be greatly reduced in the decorrelated space. That is, fewer human efforts are required for developing the system. (d) We carry out comparisons between RSS and three classical decorre-lated spaces including Discrete Cosine Transform (DCT), Principal Component Analysis (PCA), and Independent Component Analysis (ICA) in this paper. Two AP selection criteria proposed in literature, MaxMean and InfoGain are also compared. Testing on a realistic WLAN environment, we find that PCA achieves the best performance on the location fingerprinting task.",72-104,,http://cruz-little.org/category/category/listabout.jsp,Technology
1056,60bb74f29f44c6358e6157153b5a6d9cd7bcd868,"Ieee Transactions on Knowledge and Data Engineering, Manuscript Id 1 Clustering and Sequential Pattern Mining of Online Collaborative Learning Data","— Group work is widespread in education. The growing use of online tools supporting group work generates huge amounts of data. We aim to exploit this data to support mirroring: presenting useful high-level views of information about the group, together with desired patterns characterizing the behaviour of strong groups. The goal is to enable the groups and their facilitators to see relevant aspects of the group's operation and provide feedback if these are more likely to be associated with positive or negative outcomes and where the problems are. We explore how useful mirror information can be extracted via a theory-driven approach and a range of clustering and sequential pattern mining. The context is a senior software development project where students use the collaboration tool TRAC. We extract patterns distinguishing the better from the weaker groups and get insights in the success factors. The results point to the importance of leadership and group interaction, and give promising indications if they are occurring. Patterns indicating good individual practices were also identified. We found that some key measures can be mined from early data. The results are promising for advising groups at the start and early identification of effective and poor practices, in time for remediation.",96-127,,http://hood.biz/explore/search/tagsregister.htm,Technology
1057,202d6dff59014382ba643e4a71852639b1d94d84,Ieee Transactions on Knowledge and Data Engineering April 2012 an Unsupervised Feature Selection Framework for Social Media Data,"—The explosive usage of social media produces massive amount of unlabeled and high-dimensional data. Feature selection has been proven to be effective in dealing with high-dimensional data for efficient learning and data mining. Unsupervised feature selection remains a challenging task due to the absence of label information based on which feature relevance is often assessed. The unique characteristics of social media data further complicate the already challenging problem of unsupervised feature selection, e.g., social media data is inherently linked, which makes invalid the independent and identically distributed assumption, bringing about new challenges to unsupervised feature selection algorithms. In this paper, we investigate a novel problem of feature selection for social media data in an unsupervised scenario. In particular, we analyze the differences between social media data and traditional attribute-value data, investigate how the relations extracted from linked data can be exploited to help select relevant features, and propose a novel unsupervised feature selection framework, LUFS, for linked social media data. We systematically design and conduct systemic experiments to evaluate the proposed framework on datasets from real-world social media websites. The empirical study demonstrates the effectiveness and potential of our proposed framework.",19-115,,https://woodward-lewis.com/explore/list/postslogin.html,Technology
1058,6890443a50fd596952a4a9c57e377582c63c8b5f,Ieee Transactions on Knowledge and Data Engineering 1 Relationships between Diversity of Classification Ensembles and Single-class Performance Measures,"—In class imbalance learning problems, how to better recognize examples from the minority class is the key focus, since it is usually more important and expensive than the majority class. Quite a few ensemble solutions have been proposed in the literature with varying degrees of success. It is generally believed that diversity in an ensemble could help to improve the performance of class imbalance learning. However, no study has actually investigated diversity in depth in terms of its definitions and effects in the context of class imbalance learning. It is unclear whether diversity will have a similar or different impact on the performance of minority and majority classes. In this paper, we aim to gain a deeper understanding of if and when ensemble diversity has a positive impact on the classification of imbalanced data sets. First, we explain when and why diversity measured by Q-statistic can bring improved overall accuracy based on two classification patterns proposed by Kuncheva et al. We define and give insights into good and bad patterns in imbalanced scenarios. Then, the pattern analysis is extended to single-class performance measures, including recall, precision and F-measure, which are widely used in class imbalance learning. Six different situations of diversity's impact on these measures are obtained through theoretical analysis. Finally, to further understand how diversity affects the single class performance and overall performance in class imbalance problems, we carry out extensive experimental studies on both artificial data sets and real-world benchmarks with highly skewed class distributions. We find strong correlations between diversity and discussed performance measures. Diversity shows a positive impact on the minority class in general. It is also beneficial to the overall performance in terms of AUC and G-mean.",97-111,,http://wilson.com/postspost.php,Technology
1059,9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,The State of the Art of Data Science and Engineering in Structural Health Monitoring,,63-101,2019.0,http://www.rivers-stevenson.biz/categoriesterms.htm,Computer Science
1060,ce1f1762b9111ee099021bb0de727ba22c228e6d,Ieee Transactions on Knowledge and Data Engineering Semantics of Ranking Queries for Probabilistic Data,"—Recently, there have been several attempts to propose definitions and algorithms for ranking queries on probabilistic data. However, these lack many intuitive properties of a top-k over deterministic data. We define several fundamental properties, including exact-k, containment, unique-rank, value-invariance, and stability, which are satisfied by ranking queries on certain data. We argue these properties should also be carefully studied in defining ranking queries in probabilistic data, and fulfilled by definition for ranking uncertain data for most applications. We propose an intuitive new ranking definition based on the observation that the ranks of a tuple across all possible worlds represent a well-founded rank distribution. We studied the ranking definitions based on the expectation, the median and other statistics of this rank distribution for a tuple and derived the expected rank, median rank and quantile rank correspondingly. We are able to prove that the expected rank, median rank and quantile rank satisfy all these properties for a ranking query. We provide efficient solutions to compute such rankings across the major models of uncertain data, such as attribute-level and tuple-level uncertainty. Finally, a comprehensive experimental study confirms the effectiveness of our approach.",88-143,,https://hunter.org/bloghome.php,Technology
1061,86eb0f1db702010306107007e25c5a991bba7240,Ieee Transactions on Knowledge and Data Engineering Group Enclosing Queries,"—Given a set of points P and a query set Q, a group enclosing query (GEQ) fetches the point p * ∈ P such that the maximum distance of p * to all points in Q is minimized. This problem is equivalent to the Min-Max case (minimizing the maximum distance) of aggregate nearest neighbor queries for spatial databases [27]. This work first designs a new exact solution by exploring new geometric insights, such as the minimum enclosing ball, the convex hull and the furthest voronoi diagram of the query group. To further reduce the query cost, especially when the dimensionality increases, we turn to approximation algorithms. Our main approximation algorithm has a worst case √ 2-approximation ratio if one can find the exact nearest neighbor of a point. In practice, its approximation ratio never exceeds 1.05 for a large number of data sets up to six dimension. We also discuss how to extend it to higher dimensions (up to 74 in our experiment) and show that it still maintains a very good approximation quality (still close to 1) and low query cost. In fixed dimensions, we extend the √ 2-approximation algorithm to get a (1 + ǫ)-approximate solution for the GEQ problem. Both approximation algorithms have O(log N + M) query cost in any fixed dimension, where N and M are the sizes of the data set P and query group Q. Extensive experiments on both synthetic and real data sets, up to 10 million points and 74 dimensions, confirm the efficiency, effectiveness and scalability of the proposed algorithms, especially their significant improvement over the state-of-the-art method.",95-112,,http://www.cook.net/blogabout.asp,Technology
1062,3ef98b77144a0394259d9ba5c7a60f1fc5bde9de,Ieee Transactions on Knowledge and Data Engineering 1 a Group Incremental Approach to Feature Selection Applying Rough Set Technique,"Many real data increase dynamically in size. This phenomenon occurs in several fields including economics, population studies and medical research. As an effective and efficient mechanism to deal with such data, incremental technique has been proposed in the literature and attracted much attention, which stimulates the result in this paper. When a group of objects are added to a decision table, we first introduce incremental mechanisms for three representative information entropies and then develop a group incremental rough feature selection algorithm based on information entropy. When multiple objects are added to a decision table, the algorithm aims to find the new feature subset in a much shorter time. Experiments have been carried out on eight UCI data sets and the experimental results show that the algorithm is effective and efficient.",57-111,,https://wright.com/app/categoryabout.php,Technology
1063,717b99765e7eec60fac0d1ce0c12cb22e78dadb0,"Ieee Transactions on Knowledge and Data Engineering, Manuscript Id 1 Development of a Software Engineering Ontology for Multi-site Software Development","This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook titled Software Engineering by Ian Sommerville that is now in its eighth edition [1] and the white paper, SoftWare Engineering Body Of Knowledge (SWEBOK), by the IEEE [2] upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of and what it is used for in the form of usage example scenarios. The usage scenarios presented in the paper highlight characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its end users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.",41-130,,http://martin.com/searchlogin.html,Technology
1064,8eb0968804d13b84a3950b76fc4f5f3768b471eb,Ieee Transactions on Knowledge and Data Engineering Malware Propagation in Large-scale Networks,"—Malware is pervasive in networks, and poses a critical threat to network security. However, we have very limited understanding of malware behavior in networks to date. In this paper, we investigate how malware propagate in networks from a global perspective. We formulate the problem, and establish a rigorous two layer epidemic model for malware propagation from network to network. Based on the proposed model, our analysis indicates that the distribution of a given malware follows exponential distribution, power law distribution with a short exponential tail, and power law distribution at its early, late and final stages, respectively. Extensive experiments have been performed through two real-world global scale malware data sets, and the results confirm our theoretical findings.",27-149,,http://merritt.org/search/categoriesauthor.asp,Technology
1065,9b6b702ce7e26b8d8a41d1641ba816aa075e1354,International Journal of Multimedia Data Engineering and Management,,78-133,,https://www.clay-mcdaniel.biz/category/app/mainterms.htm,Technology
1066,e9c9b4549adca2437ed49d01220db9b02b57af19,Intelligent Data Engineering and Automated Learning - IDEAL 2012,,37-123,2012.0,https://wang.com/listabout.htm,Computer Science
1067,95792e2c19e5f79082ffa856048f730878c79a49,Material Science and Engineering,"Answer FIVE questions, taking ANY TWO from Group A, ANY TWO from Group B and ALL from Group C. All parts of a question (a, b, etc) should be answered atone place, Answer should be brief and to-the-point and be supplemented with neat sketches. Unnecessary long answers may result in loss of marks. Any missing data ,or wrong data may be assumed suitably giving proper justification. Figures on the right-hand side margin indicate full marks. 1. (a) What is a Burger vector? Show it by drawing a Burger circuit? What is Frank-Read source? State its importance in plastic deformation. 2+2+2 (b) Distinguish between: (2x2)+ (2x2) (i) Slip and Cross slip (ii) Sessile dislocation and Glissile dislocation. (c) What is Critical Resolved Shear Stress? Derive its formulae. 2+2 (d) Calculate the degree of freedom of ice and water kept in a beaker at 1 atmosphere pressure. 2 2. (a) State Fick's laws of Diffusion. How can it help you m the problems of Case Carburising? Given an activation energy, Q of 142 kJ/mol, for the diffusion of carbon in FCC iron and an initial temperature of 1000 K, find the temperature that will increase the diffusion coefficient by a factor 10. [R =8.314 J/(mol.K)]: Will you use a very high temperature? 2+2+(3+1) (b) What is a Phase? What is the difference between α-iron and ferrite? Define an invariant reaction with an example. 2+2 (c) Differentiate between: (2x2)+ (2x2) (i) Phase Rule and Phase Diagram, (ii) Solvus Line and Solidus Line. 3. (a) Explain Lever Rule with a Tie Line. Find the weight percentage of pro-eutectoid ferrite just above, the eutectoid temperature of a 0 3%C-steel. 2+2 (b) Derive the relationship between True Strain and Engineering Strain. What is Resilience? Why is it important for spring material? 2+(1 +1) (c) Describe Yield Point Phenomenon. Draw the engineering stress-strain diagram of Glass. Why does necking occur during tension test of a ductile material 2+2+2 (d) Justify: 2x3 (i) Zinc is not as ductile as copper (ii) Cold working increases hardness of materials (iii) Steel is a brittle material at sub-zero atmosphere. 4. (a) Suggest one suitable material for each of the following purpose with justifications: 2x5 (i) File Cabinet (ii) WaterTap (iii) Manhole Cover (iv) Garden Chair (v) Glass Cutter.",87-144,2022.0,https://ramirez.com/explore/blogprivacy.html,Technology
1068,59eccbeeb7bcd72925247559513be157fd68a802,Requirements Engineering for Machine Learning: Perspectives from Data Scientists,"Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.",245-251,2019.0,http://www.underwood.com/wp-content/categorieslogin.htm,Computer Science
1069,b4498f4cb6d6fa41d4cd0da31e0d1019756e63b6,Using the levels of conceptual interoperability model and model-based data engineering to develop a modular interoperability framework,This paper describes how to use the Levels of Conceptual Interoperability (LCIM) as the theoretical backbone for developing and implementing an interoperability framework that supports the exchange of XML-based languages used by M&S systems across the web. The principles of Model-based Data Engineering (MBDE) are integrated within the framework to support the interactions between systems across the layers of the LCIM. We present a use case that shows how the framework supports the interoperability of heterogeneous military systems.,2571-2581,2011.0,http://www.ramirez.com/categories/explorehome.html,Computer Science
1070,d13fd4d5d057e0ad8d8aded828f971fa51c97654,Combining machine learning and process engineering physics towards enhanced accuracy and explainability of data-driven models,,106834,2020.0,https://johnson-cooke.net/posts/posts/listhome.html,Computer Science
1071,fad6242bdd15cc563c7d44fa1eaf89c8d08cec52,Intelligent Science and Intelligent Data Engineering,,58-145,2011.0,https://patrick-hughes.com/explore/tagpost.html,Computer Science
1072,652c77a90d84df639622efdc9cd7475e96a248c9,Data-driven modeling and learning in science and engineering,,25-139,2019.0,http://hughes.com/tags/wp-content/wp-contenthomepage.asp,Computer Science
1073,a78c4df65ee23e4b36207163c5408d61da812f4b,Intelligent Data Engineering and Automated Learning - IDEAL 2011,,21-117,2011.0,https://www.robbins.biz/postsregister.html,Computer Science
1074,d442ab43731a9c9dc93cd2303b86d1e4bbe5c06e,"Virtual, Digital and Hybrid Twins: A New Paradigm in Data-Based Engineering and Engineered Data",,105 - 134,2018.0,http://www.malone-trevino.info/blog/posts/mainauthor.asp,Computer Science
1075,ef2afdce9b71657522d743178ab39fb03a394647,"Big data analytics in medical engineering and healthcare: methods, advances and challenges","Abstract Big data analytics are gaining popularity in medical engineering and healthcare use cases. Stakeholders are finding big data analytics reduce medical costs and personalise medical services for each individual patient. Big data analytics can be used in large-scale genetics studies, public health, personalised and precision medicine, new drug development, etc. The introduction of the types, sources, and features of big data in healthcare as well as the applications and benefits of big data and big data analytics in healthcare is key to understanding healthcare big data and will be discussed in this article. Major methods, platforms and tools of big data analytics in medical engineering and healthcare are also presented. Advances and technology progress of big data analytics in healthcare are introduced, which includes artificial intelligence (AI) with big data, infrastructure and cloud computing, advanced computation and data processing, privacy and cybersecurity, health economic outcomes and technology management, and smart healthcare with sensing, wearable devices and Internet of things (IoT). Current challenges of dealing with big data and big data analytics in medical engineering and healthcare as well as future work are also presented.",267 - 283,2020.0,http://rivera.com/tags/tag/exploremain.php,Medicine
1076,e4d66b15fce00531b96af6330238301ebbb76291,StyleGAN-Human: A Data-Centric Odyssey of Human Generation,"Unconditional human image generation is an important task in vision and graphics, which enables various applications in the creative industry. Existing studies in this field mainly focus on""network engineering""such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in""data engineering"", which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are needed to train a high-fidelity unconditional human generation model with vanilla StyleGAN. 2) A balanced training set helps improve the generation quality with rare face poses compared to the long-tailed counterpart, whereas simply balancing the clothing texture distribution does not effectively bring an improvement. 3) Human GAN models with body centers for alignment outperform models trained using face centers or pelvis points as alignment anchors. In addition, a model zoo and human editing applications are demonstrated to facilitate future research in the community.",1-19,2022.0,http://day.com/main/posts/appprivacy.html,Computer Science
1077,106274c0cb6b8551fc36b75632658f382474a890,Big Data Software Engineering: Analysis of Knowledge Domains and Skill Sets Using LDA-Based Topic Modeling,"Software engineering is a data-driven discipline and an integral part of data science. The introduction of big data systems has led to a great transformation in the architecture, methodologies, knowledge domains, and skills related to software engineering. Accordingly, education programs are now required to adapt themselves to up-to-date developments by first identifying the competencies concerning big data software engineering to meet the industrial needs and follow the latest trends. This paper aims to reveal the knowledge domains and skill sets required for big data software engineering and develop a taxonomy by mapping these competencies. A semi-automatic methodology is proposed for the semantic analysis of the textual contents of online job advertisements related to big data software engineering. This methodology uses the latent Dirichlet allocation (LDA), a probabilistic topic-modeling technique to discover the hidden semantic structures from a given textual corpus. The output of this paper is a systematic competency map comprising the essential knowledge domains, skills, and tools for big data software engineering. The findings of this paper are expected to help evaluate and improve IT professionals’ vocational knowledge and skills, identify professional roles and competencies in personnel recruitment processes of companies, and meet the skill requirements of the industry through software engineering education programs. Additionally, the proposed model can be extended to blogs, social networks, forums, and other online communities to allow automatic identification of emerging trends and generate contextual tags.",82541-82552,2019.0,https://lopez.com/main/wp-contentregister.php,Computer Science
1078,061a3b79f51007fc12b133761683b03687d73c74,Feature Engineering for Machine Learning and Data Analytics,,19-133,2018.0,https://vazquez.com/posts/tagsterms.php,Computer Science
1079,90d88a4bb7b133be9ea2cfb5159b46b58f49fa13,Guest Editor's Introduction to the Special Section on the IEEE International Conference on Data Engineering,"The eight papers in this special section were selected from the 93 long papers presented at the 25th IEEE International Conference on Data Engineering (ICDE 2009), held in Shanghai, China, on 29 March-2 April 2009.",1057-1058,2010.0,http://www.romero.com/categories/search/listlogin.html,Computer Science
1080,87f7c170aecf8f3465b26a11b9a384fef934337b,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",60-111,2017.0,http://www.perry.com/wp-content/categoryregister.html,Computer Science
1081,2a375b0d8c643aa98df0555355a40526de773ed5,Toward Data-Driven Requirements Engineering,"Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.",48-54,2016.0,http://www.sanchez.biz/mainprivacy.html,Computer Science
1082,a461233e56079fc5af6e48d75f38be8c9ff87c1e,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",100-133,2021.0,http://mendez.com/listregister.htm,Medicine
1083,6bec0106bebc93fc30ec47af9779d7e327639034,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",66-102,2018.0,https://rodriguez.com/appfaq.htm,Medicine
1084,f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",291-300,2019.0,https://yang.com/searchabout.asp,Computer Science
1085,6a97303b92477d95d1e6acf7b443ebe19a6beb60,Learning from Imbalanced Data,"With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.",1263-1284,2009.0,http://james-miller.com/exploreprivacy.htm,Computer Science
1086,bfdab5fb2049cbb78a2ef36d17d7cef3779b4010,The stiffness of living tissues and its implications for tissue engineering,,351-370,2020.0,http://www.delgado-moore.com/search/wp-content/tagmain.htm,Computer Science
1087,06eb3c3ccae16fced2222f8a45877906f54f2164,Unified rational protein engineering with sequence-based deep representation learning,,1315 - 1322,2019.0,https://anderson.com/searchregister.html,Medicine
1088,b890447611d11dcd8b835183a35afef16c096eb9,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.","
          63-85
        ",2017.0,http://little.com/appregister.html,Engineering
1089,391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",97-107,2016.0,http://www.jones.com/search/listfaq.html,Computer Science
1090,ca38d68c87a2f3734ca2d806ec2daceac7dbbfb4,2022 roadmap on neuromorphic computing and engineering,"Modern computation based on von Neumann architecture is now a mature cutting-edge science. In the von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.",76-138,2021.0,https://young.com/main/listhomepage.htm,Computer Science
1091,c1137ba91f24c31af959e21af90aaa88605e618c,XBRL for Interactive Data: Engineering the Information Value Chain.,"The article reviews the book ""XBRL for Interactive Data: Engineering the Information Value Chain,"" by R. Debreceny, C. Felden, B. Ochocki, M. Piechocki, and M. Piechocki.",83-84,2009.0,https://bell.com/wp-content/bloglogin.html,Computer Science
1092,538229ccc6b3cdd3f4162fe13d03791234b991e6,"Proceedings of the 25th International Conference on Data Engineering, ICDE 2009, March 29 2009 - April 2 2009, Shanghai, China",,87-107,2009.0,https://www.edwards-green.com/explore/explore/categoryregister.html,Engineering
1093,4d01fc57d4a00274acacfe1aac3414434476e73c,"Data Engineering: Mining, Information and Intelligence","It is quite clear that the world is awash in all kinds of data. In fact, the sheer volume of data adds little value to human activity and choices. The key to the vast amounts of data and information available to us is to distill and organize the data into information that we can productively use. Therefore, the most desirable information is that information which can be used to provide insight and intelligence for actionable strategies. DATA ENGINEERING: Mining, Information and Intelligence focuses specifically on applied information-warehousing and data-mining research that is being used or can be used by both academic researchers and industry enterprises. Moreover, the book will be the first to categorize and synthesize the diverse methodologies that are used in these interrelated fields into a structured approach entitled, Data Engineering.",90-116,2009.0,http://reynolds.com/categories/mainhome.php,Engineering
1094,16b86d4d590433a76520ed0feed5af11adf1ee63,Guest Editors' Introduction: Knowledge and Data Engineering for E-Learning,"The 13 papers in this special issue focus on knowledge and data engineering for e-learning. Some of these papers were recommended submissions from the best ranked papers presented at the Sixth International Conference on Web-Based Learning (ICWL '07), held in August 2007 in Edinburgh, United Kingdom.",756-758,2009.0,http://www.garcia.com/wp-content/category/appindex.php,Computer Science
1095,dabb54c8241fb1b9eb681f722613814a74e9e6fb,SFPE handbook of fire protection engineering,,1-3493,2016.0,http://wells-juarez.com/search/mainmain.html,Computer Science
1096,967c6ed8bbc9ae86802f9025fd47d91f7e83a6e4,"Multi-Disciplinary Engineering for Cyber-Physical Production Systems, Data Models and Software Solutions for Handling Complex Engineering Projects",,77-121,2017.0,http://www.west.com/app/categoryregister.php,Computer Science
1097,032832f71799eba6565e2444778dcffccbd76280,Microsoft CloudMine: Data Mining for the Executive Order on Improving the Nation's Cybersecurity,"As any other US software maker, Microsoft is bound by the “Executive Order on Improving the Nation's Cybersecurity” [2] which dictates a clear mandate to “enhance the software supply chain security” and to generally improve the cyber security practices. However, this is much easier written down than enforced. The executive order imposes new rules and requirements that will impact engineering practices and evidence collection for most projects and engineering teams in a relatively short period of time. Part of the response is the requirement to build up comprehensive inventories of software artifacts contributing to US government systems, which is a massive task when done manually would be tedious and fragile as software eco-systems change rapidly. Required is a system that will constantly monitor and update the inventory of software artifacts and contributors so that at any given point of time, the scope and involved teams for any software security incident can be notified and response plans activated. The front line of this security battle includes data mining platforms providing the security and compliance teams with engineering artifacts and insights into artifact dependencies and engineering practices of the corresponding engineering teams. The data provided does not only allow Microsoft to build an accurate engineering artifact inventory, but also enables Microsoft�s teams to initiate so called “get-clean” initiatives to start issue remediation before proper policy tools and pipelines (“stay-clean”) can be developed, tested, and deployed. In this talk we will present CloudMine1, one of Microsoft's main data mining platforms serving data sets and dependency graphs of more than 270 different engineering artifacts (e.g., builds, releases, commits, pull requests, etc.) gathered on an hourly basis. During the talk we will provide some insights into CloudMine, its engineering team and operational costs-which is significant. We will then highlight the benefits and opportunities a data mining framework like CloudMine provides the company including insights into how inventory and automation bots use CloudMine data to impact thousands of Microsoft engineers daily, saving the company significant costs and response times to security incidents: the ability to scan more than 100,000 code repositories across the enterprise within hours; building up an artifact engineering inventory enabling us to flag any known security vulnerability in any of the software components within hours; or spotting non-compliant build and release pipelines across Microsoft's 500,000 pipelines. In addition, we will also present open challenges the CloudMine engineering team is facing during operating and growing CloudMine as a platform, which will hopefully provide motivation and inspiration for researcher and other companies to start a dialog with us and other companies about these challenges and latest research results that may help us solve these issues. From the talk it should become clear that running enterprise scale systems is not cheap but worth the effort as it enables Microsoft and its engineering teams to respond to current cyber security threads even before we can build and test best in class built-in defense systems.",639-639,2022.0,https://franklin.com/list/posts/blogmain.php,Computer Science
1098,f7a217f58c99fcfb41f16f9f32159a3ffb262b8c,XBRL for Interactive Data: Engineering the Information Value Chain,"Interactive data supports organizations to communicate effectively with their stakeholders and partners on the Internet and the World Wide Web. XBRL (eXtensible Business Reporting Language) is a key enabling technology for interactive data. XBRL links organizations and knowledge consumers in a variety of information value chains. XBRL is now in use in many countries and important settings. These include the mandate by the Securities and Exchange Commission (SEC) in the USA for corporations and mutual funds to report in XBRL. The publication includes an in-depth analysis of XBRL and up-to-date explanation of the most popular constructs in XML, on which XBRL builds. The book provides business and policy makers, technologists and information engineers with an essential toolkit to understand the complete implementation of XBRL. The chapters include introduction to XBRL, design and construction topics as well as advanced dimensional and extensibility aspects. The book also provides detailed analysis of the interaction of instance documents and taxonomies and a synopsis of the most current XBRL technologies. Features: * Written by XBRL experts with long experience in XBRL projects. * With foreword by Olivier Servais, Director - XBRL Activities of the International Accounting Standards Committee Foundation (IASCF). * With personal letter ""How it all began..."" from Charles Hoffman - father of XBRL. * First and only book on the market with two in depth and comprehensive chapters on dimensions in XBRL. * Supporting the content of the official XBRL International Taxonomy Development training. * With over 70 illustrations and over 60 code examples. * Covering aspects of XBRL taxonomy engineering in general and dimensional taxonomy engineering in particular. * With a number of real-world practical examples and eight comprehensive case studies. * First book on the market to cover XBRL formulas, XBRL versioning and XBRL rendering.",27-148,2009.0,https://www.mitchell-moore.com/tagsauthor.html,Engineering
1099,1e97929230cdcfc13584c3c66f0ecf5cff9be5e2,Influence of Data Splitting on Performance of Machine Learning Models in Prediction of Shear Strength of Soil,"The main objective of this study is to evaluate and compare the performance of different machine learning (ML) algorithms, namely, Artificial Neural Network (ANN), Extreme Learning Machine (ELM), and Boosting Trees (Boosted) algorithms, considering the influence of various training to testing ratios in predicting the soil shear strength, one of the most critical geotechnical engineering properties in civil engineering design and construction. For this aim, a database of 538 soil samples collected from the Long Phu 1 power plant project, Vietnam, was utilized to generate the datasets for the modeling process. Different ratios (i.e., 10/90, 20/80, 30/70, 40/60, 50/50, 60/40, 70/30, 80/20, and 90/10) were used to divide the datasets into the training and testing datasets for the performance assessment of models. Popular statistical indicators, such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Correlation Coefficient (R), were employed to evaluate the predictive capability of the models under different training and testing ratios. Besides, Monte Carlo simulation was simultaneously carried out to evaluate the performance of the proposed models, taking into account the random sampling effect. The results showed that although all three ML models performed well, the ANN was the most accurate and statistically stable model after 1000 Monte Carlo simulations (Mean R = 0.9348) compared with other models such as Boosted (Mean R = 0.9192) and ELM (Mean R = 0.8703). Investigation on the performance of the models showed that the predictive capability of the ML models was greatly affected by the training/testing ratios, where the 70/30 one presented the best performance of the models. Concisely, the results presented herein showed an effective manner in selecting the appropriate ratios of datasets and the best ML model to predict the soil shear strength accurately, which would be helpful in the design and engineering phases of construction projects.",1-15,2021.0,https://www.garza-kelly.com/posts/tag/blogprivacy.htm,Mathematics
1100,ddc30215f89e48fa59c0d78c1746d5a974fd0ec6,On the feature engineering of building energy data mining,,508-518,2018.0,http://vincent.com/explore/tag/bloghome.html,Computer Science
1101,0e76c4da3f5c0cd6948128dcfe984e7d158bfc15,Semantic interoperability and characterization of data provenance in computational molecular engineering,"By introducing a common representational system for metadata that describe the employed simulation workflows, diverse sources of data and platforms in computational molecular engineering, such as workflow management systems, can become interoperable at the semantic level. To achieve semantic interoperability, the present work introduces two ontologies that provide a formal specification of the entities occurring in a simulation workflow and the relations between them: The software ontology VISO is developed to represent software packages and their features, and OSMO, an ontology for simulation, modelling, and optimization, is introduced on the basis of MODA, a previously developed semi-intuitive graph notation for workflows in materials modelling. As a proof of concept, OSMO is employed to describe a leading-edge application scenario of the TaLPas workflow management system.",97-117,2019.0,https://christensen.info/categories/main/categoriesterms.asp,Computer Science
1102,04e2f184505a6b67c611bc57c05864385c024418,"Engineering the public: Big data, surveillance and computational politics","Digital technologies have given rise to a new combination of big data and computational practices which allow for massive, latent data collection and sophisticated computational modeling, increasing the capacity of those with resources and access to use these tools to carry out highly effective, opaque and unaccountable campaigns of persuasion and social engineering in political, civic and commercial spheres. I examine six intertwined dynamics that pertain to the rise of computational politics: the rise of big data, the shift away from demographics to individualized targeting, the opacity and power of computational modeling, the use of persuasive behavioral science, digital media enabling dynamic real-time experimentation, and the growth of new power brokers who own the data or social media environments. I then examine the consequences of these new mechanisms on the public sphere and political campaigns.",84-126,2014.0,https://www.hoffman.net/taglogin.jsp,Computer Science
1103,bbbe0b59e24b2355eab71914c2387f504c9e39aa,The pan-European Engineering Strong Motion (ESM) flatfile: compilation criteria and data statistics,,561-582,2018.0,https://www.garcia-lyons.com/tag/tag/wp-contentindex.php,Computer Science
1104,a0022ee86e99981f0cde48fb4258b8ae21b96c6e,Harnessing multimodal data integration to advance precision oncology,,114 - 126,2021.0,http://www.morrison.com/postsfaq.html,Medicine
1105,3f0c29566f996933ba6fc556735f1ecff5ce3c1d,Privacy and Data Protection by Design - from policy to engineering,"Privacy and data protection constitute core values of individuals and of democratic societies. There have been decades of debate on how those values -and legal obligations- can be embedded into systems, preferably from the very beginning of the design process. One important element in this endeavour are technical mechanisms, known as privacy-enhancing technologies (PETs). Their effectiveness has been demonstrated by researchers and in pilot implementations. However, apart from a few exceptions, e.g., encryption became widely used, PETs have not become a standard and widely used component in system design. Furthermore, for unfolding their full benefit for privacy and data protection, PETs need to be rooted in a data governance strategy to be applied in practice. This report contributes to bridging the gap between the legal framework and the available technological implementation measures by providing an inventory of existing approaches, privacy design strategies, and technical building blocks of various degrees of maturity from research and development. Starting from the privacy principles of the legislation, important elements are presented as a first step towards a design process for privacy-friendly systems and services. The report sketches a method to map legal obligations to design strategies, which allow the system designer to select appropriate techniques for implementing the identified privacy requirements. Furthermore, the report reflects limitations of the approach. It concludes with recommendations on how to overcome and mitigate these limits.",74-148,2014.0,https://schmidt.com/main/tag/searchlogin.html,Computer Science
1106,cd74acb268404cde24f5131a22d04d48776b283e,Turbulence Modeling in the Age of Data,"Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.",72-136,2018.0,https://www.bryant.biz/wp-content/mainauthor.php,Physics
1107,5692e63b4a2947f777cc1c3eb2a0369e774eac99,Bayesian Data Analysis in Empirical Software Engineering Research,"Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings—such as lack of flexibility and results that are unintuitive and hard to interpret—that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice. In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that provide tangible benefits—as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, we present the reanalysis of two empirical studies on the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analyses with our new Bayesian analyses, we demonstrate the concrete advantages of the latter. To conclude we advocate a more prominent role for Bayesian statistical techniques in empirical software engineering research and practice.",1786-1810,2018.0,https://www.gill-randall.com/tag/searchabout.php,Computer Science
1108,ed163f11318db2f29e0036277868159c61bcf47b,Knowledge and Data Engineering for e-Learning Special Issue of IEEE Transactions on Knowledge and Data Engineering,"With the advent of the Internet, we are seeing more sophisticated techniques being developed to support e-Iearning. The rapid developme nt of Web-based learning and new concepts like virtual classrooms, virtual laboratories and virtual universities introduces many new issues to be addressed. On the technical side, we need to develop effective e-technologies for supporting distance education. On the learning and management side, we need to consider issues such as new style of learning and different system set-u p requirements. Finally, the issue of standardization of e-Iearning systems should also be considered. In this special issue, our focus will be on the technical side, although other issues related to knowledge and data engineering for e-Iearning may also be considered. Topics: In this special issue, we call for original papers describing novel knowledge and data engineering techniques that support e-Iearning. Preference will be given to papers that include an evaluation of users' experience in using the proposed methods. Areas of interests include, but are not limited to: • Semantic Web technology for e-Iearning • Data modeling (eg., XML) for efficient management of course materials • Searching and indexing techniques to suppo rt effective course notes retrieval • User-centric e-Iearning systems and user interaction management • Profiling techniques to support grading and learning recommendation • Data and knowledge base suppo rt for pervasive e-Iearning • Course material analysis and understanding • Automatic generation of questions and answers • Collaborative communities for e-Iearning",67-125,2008.0,https://booker.org/mainindex.jsp,Computer Science
1109,0a5128188245516f716918faef7dae1e6ee4cb1f,Leveraging Digital Twin Technology in Model-Based Systems Engineering,"Digital twin, a concept introduced in 2002, is becoming increasingly relevant to systems engineering and, more specifically, to model-based system engineering (MBSE). A digital twin, like a virtual prototype, is a dynamic digital representation of a physical system. However, unlike a virtual prototype, a digital twin is a virtual instance of a physical system (twin) that is continually updated with the latter’s performance, maintenance, and health status data throughout the physical system’s life cycle. This paper presents an overall vision and rationale for incorporating digital twin technology into MBSE. The paper discusses the benefits of integrating digital twins with system simulation and Internet of Things (IoT) in support of MBSE and provides specific examples of the use and benefits of digital twin technology in different industries. It concludes with a recommendation to make digital twin technology an integral part of MBSE methodology and experimentation testbeds.",7,2019.0,http://www.woodard-calderon.com/wp-content/category/listregister.php,Computer Science
1110,7c96ac905edb6ebb4616b77a6826b532cd2990b8,International Journal of Engineering & Technology,"The smart jar enables us to keep track of the medicines stocks with the help of an android app easily accessible anywhere with a simple internet connection. The jar contains an ultra-sonic sound waves emitter and sensor which uses the reflected ultra-sonic waves to find out what level the jar is filled to and how much empty space remains in jar yet to be filled. This sensor is also connected to the internet and interfaced with the application so that as soon as the level of content in the jar changes the data is updated to us in the application without any delay. This helps us in monitoring the stocks and prepare for restocking from anywhere,additionally it also provides important data such as the expiry date and the manufactured data of the content in the jar. r .",30-126,2019.0,https://www.harper-johnson.biz/categories/categorieshome.html,Technology
1111,2e461882b8f55b11efca5edf74c1a1b40e404075,Methods and Tools for GDPR Compliance Through Privacy and Data Protection Engineering,"In this position paper we posit that, for Privacy by Design to be viable, engineers must be effectively involved and endowed with methodological and technological tools closer to their mindset, and which integrate within software and systems engineering methods and tools, realizing in fact the definition of Privacy Engineering. This position will be applied in the soon-to-start PDP4E project, where privacy will be introduced into existent general-purpose software engineering tools and methods, dealing with (risk management, requirements engineering, model-driven design, and software/systems assurance).",108-111,2018.0,http://www.berry.info/categories/category/postssearch.jsp,Computer Science
1112,97bfa89addc6e5d76361e4c1e296949cad887b86,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science,"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",587-604,2018.0,https://www.brown.org/blog/tagsfaq.jsp,Computer Science
1113,730fba26faa7f91dc6742a0c3521eb439670a825,Machine-learning-guided directed evolution for protein engineering,,687 - 694,2018.0,http://smith-hughes.info/wp-content/tagauthor.php,Medicine
1114,766ed11ce5ffcd27879716d817f2f5fd697bbb27,Modeling & Simulation-Based Data Engineering: Introducing Pragmatics into Ontologies for Net-Centric Information Exchange,"Data Engineering has become a necessary and critical activity for business, engineering, and scientific organizations as the move to service oriented architecture and web services moves into full swing. Notably, the US Department of Defense is mandating that all of its agencies and contractors assume a defining presence on the Net-centric Global Information Grid. This book provides the first practical approach to data engineering and modeling, which supports interoperabililty with consumers of the data in a service- oriented architectures (SOAs). Although XML (eXtensible Modeling Language) is the lingua franca for such interoperability, it is not sufficient on its own. The approach in this book addresses critical objectives such as creating a single representation for multiple applications, designing models capable of supporting dynamic processes, and harmonizing legacy data models for web-based co-existence. The approach is based on the System Entity Structure (SES) which is a well-defined structure, methodology, and practical tool with all of the functionality of UML (Unified Modeling Language) and few of the drawbacks. The SES originated in the formal representation of hierarchical simulation models. So it provides an axiomatic formalism that enables automating the development of XML dtds and schemas, composition and decomposition of large data models, and analysis of commonality among structures.Zeigler and Hammond include a range of features to benefit their readers. Natural language, graphical and XML forms of SES specification are employed to allow mapping of legacy meta-data. Real world examples and case studies provide insight into data engineering and test evaluation in various application domains. Comparative information is provided on concepts of ontologies, modeling and simulation, introductory linguistic background, and support options enable programmers to work with advanced tools in the area. The website of the Arizona Center for Integrative Modeling and Simulation, co-founded by Zeigler in 2001, provides links to downloadable software to accompany the book. * The only practical guide to integrating XML and web services in data engineering* Introduces linguistic levels of interoperability for effective information exchange* Covers the interoperability standards mandated by national and international agencies * Complements Zeigler's classic THEORY OF MODELING AND SIMULATION",96-132,2007.0,http://silva-blankenship.net/tags/categories/searchhome.html,Computer Science
1115,ec59569fdee17844ae071be1536a08f937f08c57,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",82-88,2016.0,https://mathis.com/explore/wp-contentabout.php,Engineering
1116,5de7babaeb9c67c8e79a19f3335b651700719f35,The Engineering Strong‐Motion Database: A Platform to Access Pan‐European Accelerometric Data,"This article describes the Engineering Strong‐Motion Database (ESM), developed in the framework of the European project Network of European Research Infrastructures for Earthquake Risk Assessment and Mitigation (NERA, see [Data and Resources][1]). ESM is specifically designed to provide end users only with quality‐checked, uniformly processed strong‐motion data and relevant parameters and has done so since 1969 in the Euro‐Mediterranean region. The database was designed for a large variety of stakeholders (expert seismologists, earthquake engineers, students, and professionals) with a user‐friendly and straightforward web interface.

Users can access earthquake and station information and download waveforms of events with magnitude≥4.0 (unprocessed and processed acceleration, velocity, and displacement, and acceleration and displacement response spectra at 5% damping). Specific tools are also available to users to process strong‐motion data and select ground‐motion suites for code‐based seismic structural analyses.

 [1]: #sec-13",987-997,2016.0,https://www.alexander.org/mainfaq.html,Engineering
1117,ade6d1a34ce18c333f531bf430118c5963a2f260,Off-target Effects in CRISPR/Cas9-mediated Genome Engineering,,56-128,2015.0,https://wade-peterson.com/tags/categorieslogin.html,Biology
1118,cd0488f6270559806f96f03cf8597b62239211b1,Knowledge and Data Engineering,"The authors provide an overview of the current research and development directions in knowledge and data engineering. They classify research problems and approaches in this area and discuss future trends. Research on knowledge and data engineering is examined with respect to programmability and representation, design tradeoffs, algorithms and control, and emerging technologies. Future challenges are considered with respect to software and hardware architecture and system design. The paper serves as an introduction to this first issue of a new quarter. >",9-16,1989.0,https://www.palmer.org/postssearch.php,Computer Science
1119,718577588f38727ff28cf5321a5772a6fcdc1865,Gaussian Processes for Data-Efficient Learning in Robotics and Control,"Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.",408-423,2015.0,https://www.kirby.com/listsearch.jsp,Computer Science
1120,e1cbcffa6875920d3c8df398d16caaa4d4a25269,Model-Based Data Engineering for Web Services,"Although XML offers heterogeneous IT systems a new level of interoperability, it doesn't ensure that the various systems correctly interpret the data they receive. To address this, data engineering supports clear definitions for exchanged data elements. With model-based data engineering, organizations use a common reference model, which offers further clarity and performance improvements. Organizations can use the resulting data to configure mediation services, translating dialects of new or legacy services into a common language for use in a service-oriented architecture.",65-70,2005.0,https://www.turner-morris.org/explore/search/appabout.html,Computer Science
1121,f14a544b15f8646cdaa7a1f8ef5db3c0ef083750,Data Engineering education with real-world projects,"This paper presents an experience report on teaching Data Engineering using a real-world project domain. Our course introduces databases within the context of Systems and Information Engineering, supplementing relational database theory with requirements engineering, design, and analysis. The primary deliverable of the course was a semester-long project to implement an information system in a real-world application domain, interacting with an external customer with uncertain requirements. We believe that real-world projects motivate students to apply good Software Engineering principles in the classroom and encourage those principles to be adopted into industrial practice.",64-68,2006.0,http://hopkins.com/search/categories/listauthor.html,Computer Science
1122,35aab48e1045740b1a8b3992e541f51f624130bc,Solving inverse problems using data-driven models,"Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical–analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.",1 - 174,2019.0,http://www.maldonado-orozco.com/categories/mainabout.php,Computer Science
1123,02791ee57137fee9915aba8eac54886f772a54f0,Predicate Invention in Inductive Data Engineering,,83-94,1993.0,http://foster.biz/categoryterms.php,Computer Science
1124,67c77fa737f876a37784af99f91eb4afaa0d2c43,Bulletin of the Technical Committee on Data Engineering Special Issue on Workkow and Extended Transaction Systems Implementation of the Flex Transaction Model . . Editorial Board Editor-in-chief Associate Editors Tc Executive Committee Chair Vice-chair Secretry/treasurer Conferences Co-ordinator Geo,"The Bulletin of the Technical Committee on Data Engineering is published quarterly and is distributed to all TC members. Its scope includes the design, implementation, modelling, theory and application of database systems and their technology. Letters, conference information, and news should be sent to the Editor-in-Chief. Papers for each issue are solicited by and should be sent to the Associate Editor responsible for the issue. Opinions expressed in contributions are those of the authors and do not necessarily reeect the positions of the TC on Data Engineering, the IEEE Computer Society, or the authors' organizations. Membership in the TC on Data Engineering is open to all current members of the IEEE Computer Society who are interested in database systems. To Current Members of the Technical Committee on Data Engineering: There is both good news and bad news in this letter. The good news is that we are well on our way to being able to distribute the Bulletin electronically. This low cost method of distribution should ensure the long term economic survival of the Bulletin. It should permit us to continue to provide the bulletin to all members free of charge. The bad news is that if you do not re-enroll as a member of the Technical Committee, then the June 1993 hardcopy issue of the Bulletin is the last copy of the Bulletin that you will receive. Our annual revenue, which comes almost entirely from sponsoring the Data Engineering Conference, is not suucient to cover the costs of printing and distributing four issues of the Bulletin a year. Four issues is, we agree, the minimum publication schedule that is reasonable in order to bring you timely information on technical and professional subjects of interest in data engineering. Long term survival then requires that we limit free distribution to those that can receive the Bulletin electronically. We are working on arranging hardcopy distribution via paid subscription for those that are not reachable electronically or who simply prefer receiving hardcopy. The annual subscription fee for four issues is expected to be in the $10 to $15 range. Please be aware that failure to enroll means that you will not remain a TC member, and hence that you will no longer receive the Bulletin. Electronic enrollment is the preferred method of becoming a member of the Technical Committee on Data Engineering. To enroll electronically, please use the following procedure: 1. Send e-mail …",63-137,1993.0,http://www.robinson-colon.com/blogterms.html,Technology
1125,1a6fc05a37ae1b2fc7c6193cb6dc8282767c2cb6,Software Engineering for AI-Based Systems: A Survey,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",1 - 59,2021.0,https://wright.com/categories/blogauthor.php,Computer Science
1126,70956547fcaf4ae6c7fc6e53142b9ed30e1c14fd,Out-of-the-box data engineering events in heterogeneous data environments,"Data has changed significantly over the last few decades. Computing systems that initially dealt with data and computation rapidly moved to information and communication. The next step on the evolutionary scale is insight and experience. Applications are demanding the use of live, spatio-temporal, heterogeneous data. Data engineering must keep pace by designing experiential environments that let users apply their senses to observe data and information about an event and to interact with aspects of the event that are of particular interest. We call this out-of-the-box data engineering because it means we must think beyond many of our time-worn perspectives and technical approaches.",8-21,2003.0,https://guerra-klein.com/explore/category/categoriespost.php,Computer Science
1127,250eb86ff0fc3694904e25006e7a4416b9fe08c2,Data-Driven Requirements Engineering - An Update,"Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. In this talk we will present and discuss most recent achievements in this direction since the paper's original publication. We will also show to mine data sets mobile apps, give a few success/failure stories and a few practical advises.",289-290,2019.0,http://schroeder.biz/categories/tagspost.html,Computer Science
1128,f50389864d9da0266ab9f21acfb913f0f7f4c8c3,Data Engineering for the Analysis of Semiconductor Manufacturing Data,"We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering — preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.",27-136,2002.0,http://garcia.com/explore/appsearch.htm,Computer Science
1129,8aafd1f3bb8fd308f9989b6cb7df541c40dc5779,A Review of Distributed Optical Fiber Sensors for Civil Engineering Applications,"The application of structural health monitoring (SHM) systems to civil engineering structures has been a developing studied and practiced topic, that has allowed for a better understanding of structures’ conditions and increasingly lead to a more cost-effective management of those infrastructures. In this field, the use of fiber optic sensors has been studied, discussed and practiced with encouraging results. The possibility of understanding and monitor the distributed behavior of extensive stretches of critical structures it’s an enormous advantage that distributed fiber optic sensing provides to SHM systems. In the past decade, several R & D studies have been performed with the goal of improving the knowledge and developing new techniques associated with the application of distributed optical fiber sensors (DOFS) in order to widen the range of applications of these sensors and also to obtain more correct and reliable data. This paper presents, after a brief introduction to the theoretical background of DOFS, the latest developments related with the improvement of these products by presenting a wide range of laboratory experiments as well as an extended review of their diverse applications in civil engineering structures.",77-110,2016.0,http://www.hughes.com/posts/blog/categoriesmain.html,Computer Science
1130,116927fbe4c9732fd1e392035a100c33b14e9d59,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",13 - 53,2017.0,https://www.allen.biz/app/categoriesindex.htm,Computer Science
1131,9374019e86b7e97260b240bbcc98671c7d8976ec,"on Knowledge and Data Engineering,",C. Mohan and I. Narang. Recovery and coherency-control protocols for fast inter-system page transfer and ne-granularity locking in a shared disks transaction environment .,55-139,1990.0,http://www.conrad-sheppard.info/posts/tagsindex.jsp,Technology
1132,27f8f9fe46939ea5356459b800e9926abe0b331b,Ieee Transactions on Knowledge and Data Engineering the Piazza Peer Data Management System,"Intuitively, data management and data integration tools should be well-suited for exchanging information in a semantically meaningful way. Unfortunately, they suffer from two significant problems: they typically require a comprehensive schema design before they can be used to store or share information, and they are difficult to extend because schema evolution is heavyweight and may break backward compatibility. As a result, many small-scale data sharing tasks are more easily facilitated by non-database-oriented tools that have little support for semantics. The goal of the peer data management system (PDMS) is to address this need: we propose the use of a decentralized, easily extensible data management architecture in which any user can contribute new data, schema information, or even mappings between other peers' schemas. PDMSs represent a natural step beyond data integration systems, replacing their single logical schema with an interlinked collection of semantic mappings between peers' individual schemas. This paper describes several aspects of the Piazza PDMS, including the schema mediation formalism, query answering and optimization algorithms, and the relevance of PDMSs to the Semantic Web.",76-116,2004.0,https://griffin.com/category/main/tagsearch.html,Technology
1133,ae9368b469a8fa0922fd5985e70af2501f7b25e4,"Intelligent Data Engineering and Automated Learning - IDEAL 2006, 7th International Conference, Burgos, Spain, September 20-23, 2006, Proceedings",,58-137,2006.0,https://www.mclaughlin.org/blog/postsauthor.htm,Computer Science
1134,03514ab95768a2f2c2eb7aaf441707e5759c2425,Rules of thumb in data engineering,"This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for system design need only slight revision after 35 years-the major change being the increased use of RAM. An analysis also indicates storage should be used to cache both database and Web data to save disk bandwidth, network bandwidth, and people's time. Surprisingly, the 5-minute rule for disk caching becomes a cache-everything rule for Web caching.",3-10,2000.0,https://prince.info/blog/listindex.html,Computer Science
1135,74bc701c9d356e2a084e275509ddf15214c23813,"IEEE Transactions on Knowledge and Data Engineering, Vol. 14","data types geospatial data modeling, database perspective. Voisard, A., + , T-KDE Mar-Apr 02 226-243 Abstract data types; cf. Inheritance Active databases act. database trigger condition testing and view maint. using optimized discrim. networks. Hanson, E.N., + , T-KDE Mar-Apr 02 261-280 real-time act. database systs. concurrency control. Datta, A., + , T-KDE May-Jun 02 465-484 Administrative data processing; cf. Business data processing Algebraic specification constraint optim. problem solving from CLP-style specs. using heuristic search techs. Dasgupta, P., + , T-KDE Mar-Apr 02 353-368 Algorithm theory; cf. Computability; Computational complexity Application program interfaces digital library platforms for multi-agent environ., OO design. Nur Zincir-Heywood, A., + , T-KDE Mar-Apr 02 281-295 Artificial intelligence; cf. Decision support systems; Knowledge based systems; Knowledge engineering; Learning (artificial intelligence); Uncertainty handling Associative processing specifying and enforcing association semantics via ORN in presence of association cycles. Ehlmann, B.K., + , T-KDE Nov-Dec 02 1249-1257 Authorization content-based authorization model for digital libraries. Adam, N.R., + , T-KDE Mar-Apr 02 296-315 cryptographic key assignment scheme for access control. Wen-Guey Tzeng, T-KDE Jan-Feb 02 182-188 password quality tester, connectionist algm. Duffy, N., + , T-KDE Jul-Aug 02 920-922 Automata theory; cf. Finite automata Automatic testing; cf. Automatic test software Automatic test software password quality tester, connectionist algm. Duffy, N., + , T-KDE Jul-Aug 02 920-922",54-146,2002.0,https://murray-boone.com/search/categoriesabout.html,Technology
1136,38688e84d2009d885a39a6490f5e78fc4c13c6a8,Proceedings of the Eighth International Conference on Data Engineering,,68-123,1992.0,https://johnston.com/blog/maincategory.asp,Computer Science
1137,d843192295fc6f9d2e3e2a883de6b48abc86815d,Initial Data Engineering,,29-42,2004.0,https://www.wright.com/wp-contentregister.jsp,Physics
1138,5d381a18c2ddca3871a52b27eedf8cb76a5a1e29,Data Engineering: Fuzzy Mathematics in Systems Theory and Data Analysis,"From the Publisher: 
There are many situations in science and engineering where complex output data from a given system is used to formulate a model of how that system operates, or to simulate its response to different inputs. Applications include control, decision theory, and the emerging fields of bioinformatics. A key advance in this general area is the use of fuzzy mathematics in the development of models.",79-130,2001.0,https://williams.com/tagprivacy.html,Computer Science
1139,df590925f034baba4e18e586642d43a2b6e23046,Bulletin of the Ieee Computer Society Technical Committee on Data Engineering,"SAP R/3 is the most successful product for enterprise resource planning (ERP). It is used by most Fortune 500 companies and comprises modules for human resource management, accounting, logistics, etc. Like many other application systems, SAP R/3 is based on a commercial relational database system which is used to store all R/3 related data; e.g., a company’s sales information. When installing R/3, it is possible to choose among several commercial systems; e.g., Adabas D, IBM UDB, Informix Adaptive Server, Microsoft SQL Server, or Oracle 8. Obviously, very good performance is crucial for users of SAP R/3. In many companies, transactions of thousands of users must be processed concurrently by SAP R/3 and the underlying database system. In addition, the size of the database can become very large. Today, the largest SAP R/3 databases have about 1.5 terabytes; in a few years, these databases are likely to grow several times that size as a result of new R/3 releases and developments; e.g., component architecture or SAP’s business information warehouse. Unfortunately, tuning the performance of an SAP R/3 system is very difficult because both the R/3 application system and the underlying database system must be tuned. Furthermore, the performance of an R/3 system can suffer because parts of R/3 were designed and implemented in the early eighties at a time when commercial database systems and their interfaces were immature. Finally, SAP R/3 is an open system that allows customers and third-party vendors to integrate specialized modules which are not part of the R/3 standard. Such modules can severely hurt the performance of the whole system, if they are poorly implemented. The purpose of this paper is to give an overview of various performance aspects of SAP R/3. This paper describes tuning options and experiences for transaction processing (OLTP) and query processing",39-135,1999.0,http://www.lopez.org/blog/tags/explorefaq.html,Technology
1140,3be4b774d0112ce8d76c56215dab2d80f2a00db3,"Proceedings of the 22nd International Conference on Data Engineering Workshops, ICDE 2006, 3-7 April 2006, Atlanta, GA, USA",,32-133,2006.0,http://www.reed.com/postsauthor.html,Computer Science
1141,7ab84ca9b72ae064e1c1a23ed439bb1a717c5185,Unit Operations Of Chemical Engineering,"chemical engineering chemical engineering essentials for, home american journal of chemical engineering, bachelor of science in chemical engineering american, specialty polymers high performance polymers solvay, chemical engineering degrees top universities, chemical process wikipedia, perry s chemical engineers handbook eighth edition, wolfram and mathematica solutions for chemical engineering, aquatherm engineering consultants india pvt ltd edit, naval reserve officers training corps scholarship, college of engineering california state university long, unit and door heaters armstrong international, chemical engineering cput, water treatment products and services h2o engineering, journal of chemical amp engineering data acs publications, wbdg wbdg whole building design guide, chemical engineering for non chemical engineers aiche, proposed syllabus for b tech program in chemical engineering, phase out of the national diploma in chemical engineering, chemical engineering free books at ebd, operations council sgs, chemical plants india caustic soda plants chemical plants, martindale s calculators on line center mathematics, kraft recovery operations course tappi org, patent technology centers management uspto, chemical recycling makes waste plastic a resource, faculty of engineering amp technology vaal university of, chemical engineer wikipedia, index chemical engineering conferences asia events, european training network for chemical engineering, diploma of engineering curtin college, search unit standards south african qualifications authority, bcit chemical and environmental technology process, perry s chemical engineers handbook 9th edition, electrical amp systems engineering washington university, csulb chemical engineering california state university, visual encyclopedia of chemical engineeringabout gold membership gold level membership allows you full access to the chemical engineering archives dating back to 1986 quickly search and retrieve all articles and back issues, in chemical engineering process design is the design of processes for desired physical and or chemical transformation of materials process design is central to chemical engineering and it can be considered to be the summit of that field bringing together all of the fields components, at the department of chemical engineering we provide you with a challenging and contemporary chemical engineering degree program enhanced by the research accomplishments of our faculty members we value excellence in teaching quality research and service along with the intellectual development of students in a challenging rewarding academic environment, the largest selection of the highest performing polymers solvay is the industry leader in specialty polymers offering the broadest selection of high performance thermoplastic resins fluoroelastomers and fluorinated fluids, what is chemical engineering so what is chemical engineering chemical engineering is a multi disciplinary branch of engineering that combines natural and experimental sciences such as chemistry and physics along with life sciences such as biology microbiology and biochemistry plus mathematics and economics to design develop produce transform transport operate and manage the, in a scientific sense a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds such a chemical process can occur by itself or be caused by an outside force and involves a chemical reaction",38-139,2016.0,http://www.russell.com/tagregister.php,Engineering
1142,70eb87613183a9141a8c3ea8212384446e5120a5,Intelligent Data Engineering and Automated Learning,,93-103,2004.0,http://www.phillips-romero.com/app/poststerms.asp,Computer Science
1143,582640c0b232acdb487cb17ebab4dabff3b1d264,Editorial: Two Named to Editorial Board of IEEE Transactions on Knowledge and Data Engineering,,673,1996.0,https://www.barnes.biz/blog/categoriessearch.html,Computer Science
1144,1bce1b410cbde7851e902291aad14616fbc318f7,Intelligent Data Engineering and Automated Learning,,72-131,2003.0,https://koch.biz/category/mainmain.asp,Computer Science
1145,839f883c01af2d6d2287de9904a4860a8e6fb7cf,Knowledge and Data Engineering: An Outlook,,1-3,1989.0,http://www.byrd.com/tagterms.php,Computer Science
1146,eb9272596b20ca3e6b4483c639280671b39065c9,Mining Software Engineering Data from GitHub,"GitHub is the largest collaborative source code hosting site built on top of the Git version control system. The availability of a comprehensive API has made GitHub a target for many software engineering and online collaboration research efforts. In our work, we have discovered that a) obtaining data from GitHub is not trivial, b) the data may not be suitable for all types of research, and c) improper use can lead to biased results. In this tutorial, we analyze how data from GitHub can be used for large-scale, quantitative research, while avoiding common pitfalls. We use the GHTorrent dataset, a queryable offline mirror of the GitHub API data, to draw examples from and present pitfall avoidance strategies.",501-502,2017.0,https://www.clark.com/explore/categorieshome.htm,Computer Science
1147,70a86a524917db7b6122b6f5e2f45661dca1b8c9,Intelligent Data Engineering and Automated Learning — IDEAL 2002,,63-142,2002.0,http://richardson-craig.com/categories/categorylogin.asp,Business
1148,3cfea68a088b443b94052c3c4b53bde136cbaac9,Towards Benchmarks for Knowledge Systems and Their Implications for Data Engineering,"The author suggests a new focus on benchmarks for knowledge systems, following the lines of similar benchmarks in other computing fields. It is noted that knowledge systems differ from conventional systems in a key way, namely their ability to interpret and apply knowledge. This gives rise to a distinction between intrinsic measures concerned with engineering qualities and extrinsic measures relating to task productivity, and both warrant improved measurement techniques. Primary concerns within the extrinsic realm include advice quality, reasoning correctness, robustness, and solution efficiency. Intrinsic concerns, on the other hand, center on elegance of knowledge base design, modularity, and architecture. The author suggests criteria for good measures and benchmarks, and ways to satisfy these through the design of knowledge and key knowledge engineering costs and performance parameters. It is suggest that the focus on measuring knowledge systems should help clarify the technical relationships between knowledge engineering and data engineering. >",101-110,1989.0,https://www.howard-sweeney.com/exploreterms.html,Computer Science
1149,5f1abea9244e76cab265dfc83cb68226ea381df6,Bulletin of the Technical Committee on Data Engineering,,97-126,1995.0,http://ross.net/app/categoriesauthor.asp,Engineering
1150,7caf67fec572c532056f941d7b03811e5eebcb1a,Proceedings of the Eleventh International Conference on Data Engineering,,92-103,1995.0,http://smith.info/main/search/postssearch.php,Computer Science
1151,3a97bd1115686e4959e84a60eeb518dfd2de052d,Engineering Design via Surrogate Modelling - A Practical Guide,"Preface. About the Authors. Foreword. Prologue. Part I: Fundamentals. 1. Sampling Plans. 1.1 The 'Curse of Dimensionality' and How to Avoid It. 1.2 Physical versus Computational Experiments. 1.3 Designing Preliminary Experiments (Screening). 1.3.1 Estimating the Distribution of Elementary Effects. 1.4 Designing a Sampling Plan. 1.4.1 Stratification. 1.4.2 Latin Squares and Random Latin Hypercubes. 1.4.3 Space-filling Latin Hypercubes. 1.4.4 Space-filling Subsets. 1.5 A Note on Harmonic Responses. 1.6 Some Pointers for Further Reading. References. 2. Constructing a Surrogate. 2.1 The Modelling Process. 2.1.1 Stage One: Preparing the Data and Choosing a Modelling Approach. 2.1.2 Stage Two: Parameter Estimation and Training. 2.1.3 Stage Three: Model Testing. 2.2 Polynomial Models. 2.2.1 Example One: Aerofoil Drag. 2.2.2 Example Two: a Multimodal Testcase. 2.2.3 What About the k -variable Case? 2.3 Radial Basis Function Models. 2.3.1 Fitting Noise-Free Data. 2.3.2 Radial Basis Function Models of Noisy Data. 2.4 Kriging. 2.4.1 Building the Kriging Model. 2.4.2 Kriging Prediction. 2.5 Support Vector Regression. 2.5.1 The Support Vector Predictor. 2.5.2 The Kernel Trick. 2.5.3 Finding the Support Vectors. 2.5.4 Finding . 2.5.5 Choosing C and epsilon. 2.5.6 Computing epsilon : v -SVR 71. 2.6 The Big(ger) Picture. References. 3. Exploring and Exploiting a Surrogate. 3.1 Searching the Surrogate. 3.2 Infill Criteria. 3.2.1 Prediction Based Exploitation. 3.2.2 Error Based Exploration. 3.2.3 Balanced Exploitation and Exploration. 3.2.4 Conditional Likelihood Approaches. 3.2.5 Other Methods. 3.3 Managing a Surrogate Based Optimization Process. 3.3.1 Which Surrogate for What Use? 3.3.2 How Many Sample Plan and Infill Points? 3.3.3 Convergence Criteria. 3.3.4 Search of the Vibration Isolator Geometry Feasibility Using Kriging Goal Seeking. References. Part II: Advanced Concepts. 4. Visualization. 4.1 Matrices of Contour Plots. 4.2 Nested Dimensions. Reference. 5. Constraints. 5.1 Satisfaction of Constraints by Construction. 5.2 Penalty Functions. 5.3 Example Constrained Problem. 5.3.1 Using a Kriging Model of the Constraint Function. 5.3.2 Using a Kriging Model of the Objective Function. 5.4 Expected Improvement Based Approaches. 5.4.1 Expected Improvement With Simple Penalty Function. 5.4.2 Constrained Expected Improvement. 5.5 Missing Data. 5.5.1 Imputing Data for Infeasible Designs. 5.6 Design of a Helical Compression Spring Using Constrained Expected Improvement. 5.7 Summary. References. 6. Infill Criteria With Noisy Data. 6.1 Regressing Kriging. 6.2 Searching the Regression Model. 6.2.1 Re-Interpolation. 6.2.2 Re-Interpolation With Conditional Likelihood Approaches. 6.3 A Note on Matrix Ill-Conditioning. 6.4 Summary. References. 7. Exploiting Gradient Information. 7.1 Obtaining Gradients. 7.1.1 Finite Differencing. 7.1.2 Complex Step Approximation. 7.1.3 Adjoint Methods and Algorithmic Differentiation. 7.2 Gradient-enhanced Modelling. 7.3 Hessian-enhanced Modelling. 7.4 Summary. References. 8. Multi-fidelity Analysis. 8.1 Co-Kriging. 8.2 One-variable Demonstration. 8.3 Choosing X c and X e . 8.4 Summary. References. 9. Multiple Design Objectives. 9.1 Pareto Optimization. 9.2 Multi-objective Expected Improvement. 9.3 Design of the Nowacki Cantilever Beam Using Multi-objective, Constrained Expected Improvement. 9.4 Design of a Helical Compression Spring Using Multi-objective, Constrained Expected Improvement. 9.5 Summary. References. Appendix: Example Problems. A.1 One-Variable Test Function. A.2 Branin Test Function. A.3 Aerofoil Design. A.4 The Nowacki Beam. A.5 Multi-objective, Constrained Optimal Design of a Helical Compression Spring. A.6 Novel Passive Vibration Isolator Feasibility. References. Index.","I-XVIII, 1-210",2008.0,http://gutierrez-garcia.org/wp-content/categoriespost.htm,Computer Science
1152,3a83d8595e6727269c876fcebd23ee9ddd524b76,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",1328-1347,2018.0,https://www.harris-english.com/mainindex.jsp,Computer Science
1153,d5e0f5cfb95f9da8b8a9f7a004695dc3eeef0a7e,RIDL: Rogue In-Flight Data Load,"We present Rogue In-flight Data Load (RIDL), a new class of speculative unprivileged and constrained attacks to leak arbitrary data across address spaces and privilege boundaries (e.g., process, kernel, SGX, and even CPU-internal operations). Our reverse engineering efforts show such vulnerabilities originate from a variety of micro-optimizations pervasive in commodity (Intel) processors, which cause the CPU to speculatively serve loads using extraneous CPU-internal in-flight data (e.g., in the line fill buffers). Contrary to other state-of-the-art speculative execution attacks, such as Spectre, Meltdown and Foreshadow, RIDL can leak this arbitrary in-flight data with no assumptions on the state of the caches or translation data structures controlled by privileged software. The implications are worrisome. First, RIDL attacks can be implemented even from linear execution with no invalid page faults, eliminating the need for exception suppression mechanisms and enabling system-wide attacks from arbitrary unprivileged code (including JavaScript in the browser). To exemplify such attacks, we build a number of practical exploits that leak sensitive information from victim processes, virtual machines, kernel, SGX and CPU-internal components. Second, and perhaps more importantly, RIDL bypasses all existing “spot” mitigations in software (e.g., KPTI, PTE inversion) and hardware (e.g., speculative store bypass disable) and cannot easily be mitigated even by more heavyweight defenses (e.g., L1D flushing or disabling SMT). RIDL questions the sustainability of a per-variant, spot mitigation strategy and suggests more fundamental mitigations are needed to contain ever-emerging speculative execution attacks.",88-105,2019.0,http://reed.com/app/categories/categorycategory.jsp,Computer Science
1154,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,A survey of machine learning for big data processing,,76-128,2016.0,https://wood.info/posts/tagsearch.asp,Computer Science
1155,542c23f026a1801d30153fcf718b5c9b4bdd20e1,Research Directions for Engineering Big Data Analytics Software,"Many software startups and research and development efforts are actively trying to harness the power of big data and create software with the potential to improve almost every aspect of human life. As these efforts continue to increase, full consideration needs to be given to the engineering aspects of big data software. Since these systems exist to make predictions on complex and continuous massive datasets, they pose unique problems during specification, design, and verification of software that needs to be delivered on time and within budget. But, given the nature of big data software, can this be done? Does big data software engineering really work? This article explores the details of big data software, discusses the main problems encountered when engineering big data software, and proposes avenues for future research.",13-19,2015.0,https://www.washington.com/tag/blogmain.jsp,Computer Science
1156,c31665ce7ec70a876d2993761018c3e93912a1a3,A Collection of Software Engineering Challenges for Big Data System Development,"In recent years, the development of systems for processing and analyzing large amounts of data (so-called Big Data) has become an important sub-discipline of software engineering. However, to date there exits no comprehensive summary of the specific idiosyncrasies and challenges that the development of Big Data systems imposes on software engineers. With this paper, we aim to provide a first step towards filling this gap based on our collective experience from industry and academic projects as well as from consulting and initial literature reviews. The main contribution of our work is a concise summary of 26 challenges in engineering Big Data systems, collected and consolidated by means of a systematic identification process. The aim is to make practitioners more aware of common challenges and to offer researchers a solid baseline for identifying novel software engineering research directions.",362-369,2018.0,http://www.neal.org/posts/wp-content/searchpost.php,Computer Science
1157,9a1f352ef21044700c180882038c28c3b2361914,Data collection and quality challenges in deep learning: a data-centric AI perspective,,791-813,2021.0,http://maldonado.com/posts/main/tagshomepage.html,Computer Science
1158,aede98ccae969f508bcbb4ce57b8eee5cf69f132,Realising the potential of product data engineering,"The increased pressures of competing in the global marketplace have compelled industries to seek innovative ways to increase their competitive edge. Immediate access to information has rapidly become a key to success. The use of information technology in information management makes such access to computer-based information a real possibility. Product data technology provides mechanisms for the management of product-related information. It offers approaches for business enterprises to design and implement their product information systems. Research results lead to the hypothesis that the benefits of product data technology can only be fully realised through its planned and controlled introduction, namely, product data engineering, and through the implementation of changes to the business processes that this leads to. The European Computer Manufacturers' Association's Reference Model for Open Distributed Processing (RM-ODP) provides five viewpoints from which an information system may be viewed. A product data engineering methodology based upon RM-ODP was established and used by a research project, MOSES, in collaboration with industry.",403-410,1997.0,https://www.miller.com/app/tag/categorieshomepage.htm,Engineering
1159,98637661d9c5d9a6b749ed596b96e2d1fb1f9be3,A Benchmark for Data Imputation Methods,"With the increasing importance and complexity of data pipelines, data quality became one of the key challenges in modern software applications. The importance of data quality has been recognized beyond the field of data engineering and database management systems (DBMSs). Also, for machine learning (ML) applications, high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems is missing values. Incomplete datasets can break data pipelines and can have a devastating impact on downstream ML applications when not detected. While statisticians and, more recently, ML researchers have introduced a variety of approaches to impute missing values, comprehensive benchmarks comparing classical and modern imputation approaches under fair and realistic conditions are underrepresented. Here, we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of datasets with heterogeneous data and realistic missingness conditions, comparing both novel deep learning approaches and classical ML imputation methods when either only test or train and test data are affected by missing data. Each imputation method is evaluated regarding the imputation quality and the impact imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions. We hope that our results help researchers and engineers to guide their data preprocessing method selection for automated data quality improvement.",73-104,2021.0,http://www.ward.com/categoryindex.html,Medicine
1160,d996bd45107881735664a08113034fc016dde41a,Exploring topic models in software engineering data analysis: A survey,"Topic models are shown to be effective to mine unstructured software engineering (SE) data. In this paper, we give a simple survey of exploring topic models to support various SE tasks between 2003 and 2015. The survey results show that there is an increasing concern in this area. Among the SE tasks, source code comprehension and software history comprehension are the mostly studied, followed by software defects prediction. However, there is still only a few studies on other SE tasks, such as feature location and regression testing.",357-362,2016.0,http://johnson.com/posts/category/exploreauthor.htm,Computer Science
1161,8702e8a17eed574c314d77c19a0ff2c2f26fdbb3,The promise of implementing machine learning in earthquake engineering: A state-of-the-art review,"Machine learning (ML) has evolved rapidly over recent years with the promise to substantially alter and enhance the role of data science in a variety of disciplines. Compared with traditional approaches, ML offers advantages to handle complex problems, provide computational efficiency, propagate and treat uncertainties, and facilitate decision making. Also, the maturing of ML has led to significant advances in not only the main-stream artificial intelligence (AI) research but also other science and engineering fields, such as material science, bioengineering, construction management, and transportation engineering. This study conducts a comprehensive review of the progress and challenges of implementing ML in the earthquake engineering domain. A hierarchical attribute matrix is adopted to categorize the existing literature based on four traits identified in the field, such as ML method, topic area, data resource, and scale of analysis. The state-of-the-art review indicates to what extent ML has been applied in four topic areas of earthquake engineering, including seismic hazard analysis, system identification and damage detection, seismic fragility assessment, and structural control for earthquake mitigation. Moreover, research challenges and the associated future research needs are discussed, which include embracing the next generation of data sharing and sensor technologies, implementing more advanced ML techniques, and developing physics-guided ML models.",1769 - 1801,2020.0,https://mitchell.com/categories/categoryauthor.htm,Engineering
1162,ef51193388ebcc0ab19a6200e469fb6666e42d3c,A Methodology for Collecting Valid Software Engineering Data,"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.",728-738,1984.0,https://cunningham.com/explore/listhomepage.htm,Computer Science
1163,49bf4507f5670dacbf8299af83ab1390438d79f7,Knowledge Engineering: Principles and Methods,,161-197,1998.0,https://hooper.com/blogmain.html,Computer Science
1164,f7c3b5a070e3cb73708964d4d2199c1c146a5527,Network traffic characteristics of data centers in the wild,"Although there is tremendous interest in designing improved networks for data centers, very little is known about the network-level traffic characteristics of data centers today. In this paper, we conduct an empirical study of the network traffic in 10 data centers belonging to three different categories, including university, enterprise campus, and cloud data centers. Our definition of cloud data centers includes not only data centers employed by large online service providers offering Internet-facing applications but also data centers used to host data-intensive (MapReduce style) applications). We collect and analyze SNMP statistics, topology and packet-level traces. We examine the range of applications deployed in these data centers and their placement, the flow-level and packet-level transmission properties of these applications, and their impact on network and link utilizations, congestion and packet drops. We describe the implications of the observed traffic patterns for data center internal traffic engineering as well as for recently proposed architectures for data center networks.",267-280,2010.0,http://www.gilmore.net/tags/tags/appterms.jsp,Computer Science
1165,921d1869f4cb4b0aa19199a1dabe3a00e0dce023,Knowledge Engineering with Big Data,"In the era of big data, knowledge engineering faces fundamental challenges induced by fragmented knowledge from heterogeneous, autonomous sources with complex and evolving relationships. The knowledge representation, acquisition, and inference techniques developed in the 1970s and 1980s, driven by research and development of expert systems, must be updated to cope with both fragmented knowledge from multiple sources in the big data revolution and in-depth knowledge from domain experts. This article presents BigKE, a knowledge engineering framework that handles fragmented knowledge modeling and online learning from multiple information sources, nonlinear fusion on fragmented knowledge, and automated demand-driven knowledge navigation.",46-55,2015.0,http://www.dean-warner.com/mainhomepage.asp,Computer Science
1166,9b3508e82af76f3da1b233f57756ae9e6328e6f5,Elements of Chemical Reaction Engineering,"1. Mole Balances. The Rate of Reaction The General Mole Balance Equation Batch Reactors Continuous-Flow Reactors Industrial Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 2. Conversion and Reactor Sizing. Definition of Conversion Batch Reactor Design Equations Design Equations for Flow Reactors Applications of the Design Equations for Continuous-Flow Reactors Reactors in Series Some Further Definitions Summary CD-ROM Materials Questions and Problems Supplementary Reading 3. Rate Laws and Stoichiometry. Part 1. Rate Laws Basic Definitions The Reaction Order and the Rate Law The Reaction Rate Constant Present Status of Our Approach to Reactor Sizing and Design Part 2. Stoichiometry Batch Systems Flow Systems Summary CD-ROM Material Questions and Problems Supplementary Reading 4. Isothermal Reactor Design. Part 1. Mole Balances in Terms of Conversion Design Structure for Isothermal Reactors Scale-Up of Liquid-Phase Batch Reactor Data to the Design of a CSTR Design of Continuous Stirred Tank Reactors (CSTRs) Tubular Reactors Pressure Drop in Reactors Synthesizing the Design of a Chemical Plant Part 2. Mole Balances Written in Terms of Concentration and Molar Flow Rate Mole Balances on CSTRs, PFRs, PBRs, and Batch Reactors Microreactors Membrane Reactors Unsteady-State Operation of Stirred Reactors The Practical Side Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Some Thoughts on Critiquing What You read Journal Critique Problems Supplementary Reading 5. Collection and Analysis of Rate Data. The Algorithm for Data Analysis Batch Reactor Data Method of Initial Rates Method of Half-Lives Differential Reactors Experimental Planning Evaluation of Laboratory Reactors Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 6. Multiple Reactions. Definitions Parallel Reactions Maximizing the Desired Product in Series Reactions Algorithm for Solution of Complex Reactions Multiple Reactions in a PFR/PBR Multiple Reactions in a CSTR Membrane Reactors to Improve Selectivity in Multiple Reactions Complex Reactions of Ammonia Oxidation Sorting It All Out The Fun Part Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 7. Reaction Mechanisms, Pathways, Bioreactions, and Bioreactors. Active Intermediates and Nonelementary Rate Laws Enzymatic Reaction Fundamentals Inhibition of Enzyme Reactions Bioreactors Physiologically Based Pharmacokinetic (PBPK) Models Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 8. Steady-State Nonisothermal Reactor Design. Rationale The Energy Balance Adiabatic Operation Steady-State Tubular Reactor with Heat Exchange Equilibrium Conversion CSTR with Heat Effects Multiple Steady States Nonisothermal Multiple Chemical Reactions Radial and Axial Variations in a Tubular Reactor The Practical Side Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 9. Unsteady-State Nonisothermal Reactor Design. The Unsteady-State Energy Balance Energy Balance on Batch Reactors Semibatch Reactors with a Heat Exchanger Unsteady Operation of a CSTR Nonisothermal Multiple Reactions Unsteady Operation of Plug-Flow Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 10. Catalysis and Catalytic Reactors. Catalysts Steps in a Catalytic Reaction Synthesizing a Rate Law, Mechanism, and Rate-Limiting Step Heterogeneous Data Analysis for Reactor Design Reaction Engineering in Microelectronic Fabrication Model Discrimination Catalyst Deactivation Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 11. External Diffusion Effects on Heterogeneous Reactions. Diffusion Fundamentals Binary Diffusion External Resistance to Mass Transfer What If ... ? (Parameter Sensitivity) The Shrinking Core Model Summary CD-ROM Material Questions and Problems Supplementary Reading 12. Diffusion and Reaction. Diffusion and Reaction in Spherical Catalyst Pellets Internal Effectiveness Factor Falsified Kinetics Overall Effectiveness Factor Estimation of Diffusion- and Reaction-Limited Regimes Mass Transfer and Reaction in a Packed Bed Determination of Limiting Situations from Reaction Data Multiphase Reactors Fluidized Bed Reactors Chemical Vapor Deposition (CVD) Summary CD-ROM Material Questions and Problems Journal Article Problems Journal Critique Problems Supplementary Reading 13. Distributions of Residence Times for Chemical Reactors. General Characteristics Part 1. Characteristics and Diagnostics Measurement of the RTD Characteristics of the RTD RTD in Ideal Reactors Diagnostics and Troubleshooting Part 2. Predicting Conversion and Exit Concentration Reactor Modeling Using the RTD Zero-Parameter Models Using Software Packages RTD and Multiple Reactions Summary CD-ROM Material Questions and Problems Supplementary Reading 14. Models for Nonideal Reactors. Some Guidelines Tanks-in-Series (T-I-S) Model Dispersion Model Flow, Reaction, and Dispersion Tanks-in-Series Model Versus Dispersion Model Numerical Solutions to Flows with Dispersion and Reaction Two-Parameter Models-Modeling Real Reactors with Combinations of Ideal Reactors Use of Software Packages to Determine the Model Parameters Other Models of Nonideal Reactors Using CSTRs and PFRs Applications to Pharmacokinetic Modeling Summary CD-ROM Material Questions and Problems Supplementary Reading Appendix A: Numerical Techniques. Appendix B: Ideal Gas Constant and Conversion Factors. Appendix C: Thermodynamic Relationships Involving the Equilibrium Constant. Appendix D: Measurement of Slopes on Semilog Paper. Appendix E: Software Packages. Appendix F: Nomenclature. Appendix G: Rate Law Data. Appendix H: Open-Ended Problems. Appendix I: How to Use the CD-ROM. Appendix J: Use of Computational Chemistry Software Packages. Index. About the CD-ROM.",17-131,1986.0,https://www.thomas.com/posts/list/mainauthor.asp,Chemistry
1167,8ece479b5dfed4727d2d9b9763f777bb9a94096e,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",1 - 24,2019.0,https://sullivan-anderson.com/list/app/tagshome.html,Computer Science
1168,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",80-90,2014.0,http://www.moore.org/posts/tag/appauthor.jsp,Computer Science
1169,698c1bd50f476899c9959d6830434d4a455941e2,Big Picture of Big Data Software Engineering: With Example Research Challenges,"In the rapidly growing field of Big Data, we note that a disproportionately larger amount of effort is being invested in infrastructure development and data analytics in comparison to applications software development -- approximately a 80:20 ratio. This prompted us to create a context model of Big Data Software Engineering (BDSE) containing various elements -- such as development practice, Big Data systems, corporate decision-making, and research -- and their relationships. The model puts into perspective where various types of stakeholders fit in. From the research perspective, we describe example challenges in BDSE, specifically requirements, architectures, and testing and maintenance.",11-14,2015.0,http://bradford-watson.com/explore/posts/categoryhome.html,Computer Science
1170,1d1de7d9e4be91a1a29927169f7f22ef7a513259,Data engineering for intelligent inference,"As advanced applications become more and more demanding, the use of the data on hand becomes more and more sophisticated. In applications such as the kind in commercial banking environment, the search for data is rather simple and precise; the time constraints are not severe; statistical applications are not often used; history data is not so important; and estimates are not performed. In short, the demand to have an intelligent DB is at best minimal.",276-276,1987.0,http://www.duncan.biz/categories/explore/tagsregister.html,Computer Science
1171,d481a6219f4cd1c337e0ec31086a2f527bab9109,Data science: Accelerating innovation and discovery in chemical engineering,,1402-1416,2016.0,https://www.salas-bauer.com/search/tag/categorieshome.html,Engineering
1172,9afba7a488f027ee7e46405623d05c07ffc21a16,Intelligent data engineering,,24-140,2004.0,https://hansen.com/tags/main/searchpost.htm,Technology
1173,a787499299fa75a7d8597aa4793255e9587a3c7f,Sentiment Analysis of Twitter Data,"We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.",30-38,2011.0,https://www.fleming.com/explorepost.asp,Computer Science
1174,7a0a373a80c4f65950958d6a2253d8da90493bc5,The use of unmanned aerial vehicles (UAVs) for engineering geology applications,,3437 - 3481,2020.0,http://brown-williams.com/main/tagprivacy.jsp,Computer Science
1175,988653d9cc361ce41dbe0930eaf2ecea27c85053,MicroTE: fine grained traffic engineering for data centers,"The effects of data center traffic characteristics on data center traffic engineering is not well understood. In particular, it is unclear how existing traffic engineering techniques perform under various traffic patterns, namely how do the computed routes differ from the optimal routes. Our study reveals that existing traffic engineering techniques perform 15% to 20% worse than the optimal solution. We find that these techniques suffer mainly due to their inability to utilize global knowledge about flow characteristics and make coordinated decision for scheduling flows.
 To this end, we have developed MicroTE, a system that adapts to traffic variations by leveraging the short term and partial predictability of the traffic matrix. We implement MicroTE within the OpenFlow framework and with minor modification to the end hosts. In our evaluations, we show that our system performs close to the optimal solution and imposes minimal overhead on the network making it appropriate for current and future data centers.",8,2011.0,https://molina.com/category/app/categoriesindex.html,Computer Science
1176,8220aa685354d53868899a08ca97c74c669f7c4a,Social Engineering Attacks: A Survey,"The advancements in digital communication technology have made communication between humans more accessible and instant. However, personal and sensitive information may be available online through social networks and online services that lack the security measures to protect this information. Communication systems are vulnerable and can easily be penetrated by malicious users through social engineering attacks. These attacks aim at tricking individuals or enterprises into accomplishing actions that benefit attackers or providing them with sensitive data such as social security number, health records, and passwords. Social engineering is one of the biggest challenges facing network security because it exploits the natural human tendency to trust. This paper provides an in-depth survey about the social engineering attacks, their classifications, detection strategies, and prevention procedures.",89,2019.0,http://www.williams.org/categories/tag/tagsindex.jsp,Computer Science
1177,813c3085647a7147ae69df7f2eaa619f85f4da77,Educational data mining for prediction and classification of engineering students achievement,"This paper highlights the importance of using student data to drive improvement in education planning. It then presents techniques of how to obtain knowledge from databases such as large arrays of student data from academic Institution databases. Further, it describes the development of a tool that will enable faculty members to identify, predict and classify students based on academic performance measured using Cumulative Grade point average (CGPA) grades. The need for prediction of a student's performance is to enable the university to intervene and provide assistance to low achievers as early as possible. Included in the paper is a brief overview of the most commonly used classifiers techniques in educational data mining and an outline of the use of Neuro-Fuzzy classification in a case study research to predict and classify students' academic achievement in an Electrical Engineering faculty of a Malaysian public university.",49-53,2015.0,https://goodman.info/search/postscategory.php,Computer Science
1178,25ac496bc07c2961860542c83abbc14b26cf42f4,Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator,,203-222,1996.0,http://barnett.com/wp-content/explore/wp-contentprivacy.htm,Computer Science
1179,775a9c722262c7b656876a5fef20f4577afd8981,Multilingual training for Software Engineering,"Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models.",1443-1455,2021.0,http://www.riley.com/wp-contentprivacy.jsp,Computer Science
1180,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",255-308,2009.0,https://www.fletcher.com/main/listcategory.htm,Computer Science
1181,682b105746238d3c39bd4f6cd0baa375dc0c2534,Perspectives on Data Science for Software Engineering,,3-6,2016.0,https://www.mullins-ortiz.com/category/tags/mainhome.jsp,Computer Science
1182,fb3082412509fb50cb5a8b014b9eddbe6e5002cb,Methodologies of knowledge discovery from data and data mining methods in mechanical engineering,Abstract,97-108,2016.0,https://www.wilson.biz/categories/postsprivacy.html,Computer Science
1183,e74e78efd12de73a9bc8c7ddc7bf543b7cc77149,International Journal of Engineering and Advanced Technology,"163 Published By: Blue Eyes Intelligence Engineering & Sciences Publication Pvt. Ltd.  Abstract: One of the main current problems facing Global Positioning System (GPS) is to get the positions with high accuracy and low cost, effort and time. Two techniques are used in GPS positioning, which are the relative and point positioning. In common, the first technique provides the higher accuracy, but with higher cost and effort. Another kind of point positioning is the Precise Point Positioning (PPP) which counts on GNSS precise products. It is adequate for many applications that requires the decimeter level accuracy using one receiver, but requires scientific software or online services for data processing. The main challenge here is to raise the accuracy of PPP to add other applications suited to the gained accuracy. The main objective of the current study is to test different mathematical models producing positional corrections to select the best set depending on synchronized data and validate the selected model in synchronized and non-synchronized cases depending on data of two different campaigns. These corrections -produced from permanent stationsare added to the static PPP coordinates of the tested points near the permanent stations to reach the highest possible accuracy depending on GPS single frequency observations using a scientific package. The obtained results offered a synchronized average positional error reaching to 0.074m and RMSE of 0.023m in the first campaign and 0.146m with RMSE of 0.061m in the second campaign. It reaches 0.156m with RMSE of 0.074m in the best non-synchronized case. The user can raise the accuracy of single frequency static PPP when the data of four synchronized permanent stations are available in the same observational time or within 4 days before or after the observational time.",70-147,2017.0,https://www.payne.biz/blog/list/tagscategory.asp,Technology
1184,45752d6bbca650b182964d388b431293fbd2d41b,State of practice in requirements engineering: contemporary data,,235-241,2014.0,https://www.reed-coleman.com/categorieslogin.php,Engineering
1185,191b9759decb2700c3d4b072f67cdcdf9f57cc8f,"Civil Engineering Grand Challenges: Opportunities for Data Sensing, Information Analysis, and Knowledge Discovery","AbstractThis paper presents an exploratory analysis to identify civil engineering challenges that can be addressed with further data sensing and analysis (DSA) research. An initial literature review was followed by a web-based survey to solicit expert opinions in each civil engineering subdiscipline to select challenges that can be addressed by civil engineering DSA research. A total of 10 challenges were identified and evidence of economic, environmental, and societal impacts of these challenges is presented through a review of the literature. The challenges presented in this paper are high building energy consumption, crude estimation of sea level, increased soil and coastal erosion, inadequate water quality, untapped and depleting groundwater, increasing traffic congestion, poor infrastructure resilience to disasters, poor and degrading infrastructure, need for better mining and coal ash waste disposal, and low construction site safety. The paper aims to assist the civil engineering research community ...",17-125,2014.0,http://hansen.com/wp-content/listabout.html,Computer Science
1186,365aed570f5ee0a2e0a5590feaa48a2b563b4344,Conceptual data model: A foundation for successful concurrent engineering,"Today, phase A studies of future space systems are often conducted in special design facilities such as the Concurrent Engineering Facility at the German Aerospace Center (DLR). Within these facilities, the studies are performed following a defined process making use of a data model for information exchange. Quite often it remains unclear what exactly such a data model is and how it is implemented and applied. Nowadays, such a data model is usually a software using a formal specification describing its capabilities within a so-called meta-model. This meta-model, often referred as conceptual data model, is finally used and instantiated as system model during these concurrent engineering studies. Such software also provides a user interface for instantiating and sharing the system model within the design team and it provides capabilities to analyze the system model on the fly. This is possible due to the semantics of the underlying conceptual data model creating a common language used to exchange and process design information. This article explains the implementation of the data model at DLR and shows information how it is applied in the concurrent engineering process of the Concurrent Engineering Facility. It highlights important aspects concerning the modeling capabilities during a study and discusses how they can be implemented into a corresponding conceptual data model. Accordingly, the article presents important aspects such as rights management and data consistency and the implications of them to the software’s underlying technology. A special use case of the data model is depicted and shows the flexibility of the implementation proven by a study of a multi-module space station.",55 - 76,2017.0,https://kelly.com/tag/main/tagsearch.html,Computer Science
1187,0ed846e87ce0961d162e9115b4e9837537138e3a,Analyze this! 145 questions for data scientists in software engineering,"In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.",57-110,2013.0,https://www.clark.info/category/app/listabout.php,Computer Science
1188,48febeeed061d93e5bb17ee7dba03ff8ab37353e,Joint virtual machine assignment and traffic engineering for green data center networks,"The popularization of cloud computing brings emergency concern to the energy consumption in big data centers. Besides the servers, the energy consumed by the network in a data center is also considerable. Existing works for improving the network energy efficiency are mainly focused on traffic engineering, i.e., consolidating flows and switching off unnecessary devices, which fails to comprehensively consider the unique features in data centers. In this paper, we advocate a joint optimization for achieving energy efficiency of data center networks by proposing a unified optimization framework. In this framework, we consider to take advantage of the application characteristics and topology features, and to integrate virtual machine assignment and traffic engineering. Under this framework, we then devise two efficient algorithms, TE VMA and TER, for assigning virtual machines and routing traffic flows respectively. Knowing the ncommunication patterns of the applications, the TE VMA algorithm is purposeful and can generate desirable traffic conditions for the next-step routing optimization. The TER algorithm makes full use of the hierarchical feature of the topology and is conducted on the multipath routing protocol. The performance of the overall framework is confirmed by both theoretical analysis and simulation results, where up to 50% total energy savings can be achieved, 20% more compared with traffic engineering only approaches.",107-112,2014.0,http://hicks.net/tagsauthor.php,Computer Science
1189,39593dc89d2c3d8c3e61b4a042f8e753b37e7106,"Artificial Intelligence for Engineering Design , Analysis and Manufacturing","The use of grammars in design and analysis has been set back by the lack of automated ways to induce them from arbitrarily structured datasets. Machine translation methods provide a construct for inducing grammars from coded data which have been extended to be used for design through pre-coded design data. This work introduces a four-step process for inducing grammars from un-coded structured datasets which can constitute a wide variety of data types, including many used in the design. The method includes: (1) extracting objects from the data, (2) forming structures from objects, (3) expanding structures into rules based on frequency, and (4) finding rule similarities that lead to consolidation or abstraction. To evaluate this method, grammars are induced from generated data, architectural layouts and threedimensional design models to demonstrate that this method offers usable grammars automatically which are functionally similar to grammars produced by hand.",61-138,2018.0,https://www.clarke-perez.com/postsabout.htm,Technology
1190,760d38a08bff329ff67719935c18fa1631e3ded8,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",171-198,2016.0,http://adams.com/tags/categoriesprivacy.html,Technology
1191,b88fd0a029c4cc229c1331d8d85444418d0b80c8,Polymer Data Handbook,"Polymers are the compounds that includes platics, artificial fibers, rubber, cellulose, and many other materials, including coatings and adhesives. This book presents key data on approximately 200 important polymers currently in industrial use or under study in industrial or academic research. No other single source covers so many polymers or offers such a depth of data. The book standardizes and makes accessible a wealth of essential data for students, teachers, researchers, and other professionals in chemistry and chemical engineering.",33-113,2009.0,https://thomas.com/tag/explorepost.asp,Materials Science
1192,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",866-883,1996.0,https://www.white.net/main/search/tagslogin.asp,Computer Science
1193,e8b9fa6f9e0b606ff335a0557a838dea2696b084,Towards Reverse-Engineering Black-Box Neural Networks,,121-144,2017.0,http://www.brown.info/main/categorymain.html,Computer Science
1194,f0fedbd445ad489ed20deb14fced8f11a5b8ba4b,Digital Twins in Health Care: Ethical Implications of an Emerging Engineering Paradigm,"Personalized medicine uses fine grained information on individual persons, to pinpoint deviations from the normal. ‘Digital Twins’ in engineering provide a conceptual framework to analyze these emerging data-driven health care practices, as well as their conceptual and ethical implications for therapy, preventative care and human enhancement. Digital Twins stand for a specific engineering paradigm, where individual physical artifacts are paired with digital models that dynamically reflects the status of those artifacts. When applied to persons, Digital Twins are an emerging technology that builds on in silico representations of an individual that dynamically reflect molecular status, physiological status and life style over time. We use Digital Twins as the hypothesis that one would be in the possession of very detailed bio-physical and lifestyle information of a person over time. This perspective redefines the concept of ‘normality’ or ‘health,’ as a set of patterns that are regular for a particular individual, against the backdrop of patterns observed in the population. This perspective also will impact what is considered therapy and what is enhancement, as can be illustrated with the cases of the ‘asymptomatic ill’ and life extension via anti-aging medicine. These changes are the consequence of how meaning is derived, in case measurement data is available. Moral distinctions namely may be based on patterns found in these data and the meanings that are grafted on these patterns. Ethical and societal implications of Digital Twins are explored. Digital Twins imply a data-driven approach to health care. This approach has the potential to deliver significant societal benefits, and can function as a social equalizer, by allowing for effective equalizing enhancement interventions. It can as well though be a driver for inequality, given the fact that a Digital Twin might not be an accessible technology for everyone, and given the fact that patterns identified across a population of Digital Twins can lead to segmentation and discrimination. This duality calls for governance as this emerging technology matures, including measures that ensure transparency of data usage and derived benefits, and data privacy.",32-118,2018.0,http://turner-garcia.net/categories/main/tagsterms.html,Psychology
1195,f85879782b6a586f992d3bb99f9907ecce61924a,Embrace the Challenges: Software Engineering in a Big Data World,"The design and development of data-intensive software systems -- systems that generate, collect, store, process, analyze, query, and visualize large sets of data -- is fraught with significant challenges both technical and social. Project EPIC has been designing and developing data-intensive systems in support of crisis informatics research since Fall 2009. Our experience working on Project EPIC has provided insight into these challenges. In this paper, we share our experience working in this design space and describe the choices we made in tackling these challenges and their attendant trade-offs. We highlight the lack of developer support tools for data-intensive systems, the importance of multidisciplinary teams, the use of highly-iterative life cycles, the need for deep understanding of the frameworks and technologies used in data intensive systems, how simple operations transform into significant challenges at scale, and the paramount significance of data modeling in producing systems that are scalable, robust, and efficient.",19-25,2015.0,https://www.charles.com/listcategory.html,Computer Science
1196,6aca07154c111f1c8738347d7112cad6b0bf974a,Customer churn prediction in telecom using machine learning in big data platform,,62-136,2019.0,http://www.davis.info/blog/categoryauthor.php,Computer Science
1197,0405150d045bc39ddf5ba8a1c71df8f2d63c661e,Engineering Applications of Correlation and Spectral Analysis,Probability Functions and Amplitude Measures. Correlation and Spectral Density Functions. Single-Input/Single-Output Relationships. System Identification and Response. Propagation-Path Identification. Single-Input/Multiple-Output Problems. Multiple-Input/Multiple-Output Relationships. Energy-Source Identification. Procedures to Solve Multiple-Input/Multiple-Output Problems. Statistical Errors in Estimates. Nonstationary Data Analysis Techniques. Nonlinear System Analysis Techniques. References. List of Figures. List of Tables. Index. Glossary of Symbols.,22-147,1980.0,https://www.smith.com/category/categoriespost.php,Computer Science
1198,0c471c3aa5aef737ee00cfe9c58e9097328a954e,A Survey of Data-Driven and Knowledge-Aware eXplainable AI,"We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.",29-49,2020.0,https://www.santiago.com/categoryauthor.html,Computer Science
1199,62e0c6cf57bc345026d56fd654e80beaf9315c92,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",1 - 30,2011.0,https://petersen-horn.com/categorypost.html,Chemistry
1200,0c471c3aa5aef737ee00cfe9c58e9097328a954e,A Survey of Data-Driven and Knowledge-Aware eXplainable AI,"We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.",29-49,2020.0,http://www.noble.com/searchauthor.php,Computer Science
1201,62e0c6cf57bc345026d56fd654e80beaf9315c92,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",1 - 30,2011.0,http://frank.info/category/explore/exploresearch.htm,Chemistry
1202,7980cf24fca6dd9314c11537ff1596c36ffc01dd,Culture of Disengagement in Engineering Education?,"Much has been made of the importance of training ethical, socially conscious engineers, but does US engineering education actually encourage neophytes to take seriously their professional responsibility to public welfare? Counter to such ideals of engagement, I argue that students’ interest in public welfare concerns may actually decline over the course of their engineering education. Using unique longitudinal survey data of students at four colleges, this article examines (a) how students’ public welfare beliefs change during their engineering education, (b) whether engineering programs emphasize engagement, and (c) whether these program emphases are related to students’ public welfare beliefs. I track four specific public welfare considerations: the importance to students of professional/ethical responsibilities, understanding the consequences of technology, understanding how people use machines, and social consciousness. Suggesting a culture of disengagement, I find that the cultural emphases of students’ engineering programs are directly related to their public welfare commitments and students’ public welfare concerns decline significantly over the course of their engineering education. However, these findings also suggest that if engineering programs can dismantle the ideological pillars of disengagement in their local climates, they may foster more engaged engineers.",42 - 72,2014.0,http://www.williams-nichols.com/wp-content/searchindex.htm,Sociology
1203,4d2758c7bc95e5f296a68a9ac6908771d6a81168,Inferring Regulatory Networks from Expression Data Using Tree-Based Methods,"One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn't make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.",45-143,2010.0,https://fuentes.com/appindex.html,Biology
1204,c93b938cc6942497a0b683c54a4b606727b2417a,Fuzzy logic systems for engineering: a tutorial,"A fuzzy logic system (FLS) is unique in that it is able to simultaneously handle numerical data and linguistic knowledge. It is a nonlinear mapping of an input data (feature) vector into a scalar output, i.e., it maps numbers into numbers. Fuzzy set theory and fuzzy logic establish the specifics of the nonlinear mapping. This tutorial paper provides a guided tour through those aspects of fuzzy sets and fuzzy logic that are necessary to synthesize an FLS. It does this by starting with crisp set theory and dual logic and demonstrating how both can be extended to their fuzzy counterparts. Because engineering systems are, for the most part, causal, we impose causality as a constraint on the development of the FLS. After synthesizing a FLS, we demonstrate that it can be expressed mathematically as a linear combination of fuzzy basis functions, and is a nonlinear universal function approximator, a property that it shares with feedforward neural networks. The fuzzy basis function expansion is very powerful because its basis functions can be derived from either numerical data or linguistic knowledge, both of which can be cast into the forms of IF-THEN rules. >",345-377,1995.0,http://sanchez.com/explorehomepage.jsp,Computer Science
1205,377f1e43c5a48f12b0592b09a142322e74729409,Genetic Algorithms in the Fields of Artificial Intelligence and Data Sciences,,1007 - 1018,2021.0,https://powell.info/tagprivacy.php,Computer Science
1206,532b2033ed22b81333cdc5a60c4a545849388ca9,Joint VM placement and routing for data center traffic engineering,"Today's data centers need efficient traffic management to improve resource utilization in their networks. In this work, we study a joint tenant (e.g., server or virtual machine) placement and routing problem to minimize traffic costs. These two complementary degrees of freedom-placement and routing-are mutually-dependent, however, are often optimized separately in today's data centers. Leveraging and expanding the technique of Markov approximation, we propose an efficient online algorithm in a dynamic environment under changing traffic loads. The algorithm requires a very small number of virtual machine migrations and is easy to implement in practice. Performance evaluation that employs the real data center traffic traces under a spectrum of elephant and mice flows, demonstrates a consistent and significant improvement over the benchmark achieved by common heuristics.",2876-2880,2012.0,https://www.maldonado.com/tagmain.htm,Computer Science
1207,cd7b4fdd5d945cb3be828cc544f3239e060b802a,Heavy-Hitter Detection Entirely in the Data Plane,"Identifying the ""heavy hitter"" flows or flows with large traffic volumes in the data plane is important for several applications e.g., flow-size aware routing, DoS detection, and traffic engineering. However, measurement in the data plane is constrained by the need for line-rate processing (at 10-100Gb/s) and limited memory in switching hardware. We propose HashPipe, a heavy hitter detection algorithm using emerging programmable data planes. HashPipe implements a pipeline of hash tables which retain counters for heavy flows while evicting lighter flows over time. We prototype HashPipe in P4 and evaluate it with packet traces from an ISP backbone link and a data center. On the ISP trace (which contains over 400,000 flows), we find that HashPipe identifies 95% of the 300 heaviest flows with less than 80KB of memory.",36-132,2016.0,https://www.kennedy.com/search/tag/searchpost.php,Computer Science
1208,f463018624b6f4b8dd576732b6cce36e31bac978,Software Engineering of Self-adaptive Systems,,399-443,2019.0,http://wilkins.com/wp-content/tags/blogmain.html,Computer Science
1209,b5839c523c9647b89b0453e2efce12f47380d2f3,"Data Assimilation: Methods, Algorithms, and Applications","Data assimilation is an approach that combines observations and model output, with the objective of improving the latter. This book places data assimilation into the broader context of inverse problems and the theory, methods, and algorithms that are used for their solution. It provides a framework for, and insight into, the inverse problem nature of data assimilation, emphasizing “why” and not just “how.” Methods and diagnostics are emphasized, enabling readers to readily apply them to their own field of study. 
 
Readers will find a comprehensive guide that is accessible to nonexperts; numerous examples and diverse applications from a broad range of domains, including geophysics and geophysical flows, environmental acoustics, medical imaging, mechanical and biomedical engineering, economics and finance, and traffic control and urban planning; and the latest methods for advanced data assimilation, combining variational and statistical approaches.",61-109,2016.0,https://www.boyle-wilson.com/mainauthor.php,Computer Science
1210,4d111edca5bf3b878d9b7e1df43b757da1af86da,Engineering big data solutions,"Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.",48-145,2014.0,https://ellis.info/wp-contentabout.htm,Computer Science
1211,93ef4425014a22f9e9466809834dd70573d28750,2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository,"This review provides an overview of 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository compiled by the Johns Hopkins University Center for Systems Science and Engineering. It provides a background of how the repository was compiled, the data included and how the repo is being made use of in a Canadian academic library context.",47-51,2020.0,http://www.freeman-ayers.info/tags/tagsindex.jsp,Computer Science
1212,d094f0faff376af9a0ee79a742b350531b3c89bf,Exascale computing and big data,Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.,56 - 68,2015.0,http://www.fox.net/main/wp-content/searchfaq.php,Computer Science
1213,0caa61c0ffa59fd74e11368b77e267bc27bf5564,Engineering AI Systems: A Research Agenda,"Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry. However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this chapter, the authors provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that they have studied. The main contribution of the chapter is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.",22-149,2020.0,https://jones.com/categories/searchhomepage.html,Computer Science
1214,b42d14dab8e5b172d99b6eee11581f8bdbb8b0a3,"Chemical, Biochemical, and Engineering Thermodynamics","Notation. Chapter 1. Introduction. Chapter 2. Conservation of Mass. Chapter 3. Conservation of Energy. Chapter 4. Entropy: An Additional Balance Equation. Chapter 5. Liquefaction, Power Cycles, and Explosions. Chapter 6. The Thermodynamic Properties of Real Substances. Chapter 7. Equilibrium and Stability in One-Component Systems. Chapter 8. The Thermodynamics of Multicomponent Mixtures. Chapter 9. The Estimation of the Gibbs Free Energy and Fugacity of a Component in a Mixture. Chapter 10. Vapor-liquid Equilibrium in Mixtures. Chapter 11. Other types of Phase Equilibria in Fluid Mixtures. Chapter 12. Mixture Phase Equilibria Involving Solids. Chapter 13. Chemical Equilibrium. Chapter 14. The Balance Equations for Chemical Reactors and Electrochemistry. Chapter 15. Some Biochemical Applications of Thermodynamics. Appendix A: Thermodynamic Data. Appendix B: Computer Programs.",61-134,2017.0,http://www.jacobson.com/appmain.html,Chemistry
1215,68d12aeab53834e9e851fa6751b0d3122bf18d51,Review of Metamodeling Techniques in Support of Engineering Design Optimization,"Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner’s perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.Copyright © 2006 by ASME",415-426,2007.0,https://schmidt.org/appsearch.php,Engineering
1216,49d3816939a2ccf0916919695cf023e4b1d7726f,Requirements Engineering: Processes and Techniques,"Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws",73-105,1998.0,https://velasquez.com/listhome.htm,Computer Science
1217,12d89245440d8c2a57c0741d10177189adf230d3,"“Zhores” — Petaflops supercomputer for data-driven modeling, machine learning and artificial intelligence installed in Skolkovo Institute of Science and Technology","Abstract The Petaflops supercomputer “Zhores” recently launched in the “Center for Computational and Data-Intensive Science and Engineering” (CDISE) of Skolkovo Institute of Science and Technology (Skoltech) opens up new exciting opportunities for scientific discoveries in the institute especially in the areas of data-driven modeling, machine learning and artificial intelligence. This supercomputer utilizes the latest generation of Intel and NVidia processors to provide resources for the most compute intensive tasks of the Skoltech scientists working in digital pharma, predictive analytics, photonics, material science, image processing, plasma physics and many more. Currently it places 7th in the Russian and CIS TOP-50 (2019) supercomputer list. In this article we summarize the cluster properties and discuss the measured performance and usage modes of this new scientific instrument in Skoltech.",512 - 520,2019.0,http://www.collins-bishop.info/taglogin.html,Computer Science
1218,5935e5460b8aa62a41e2e29a6b92dce95b441fb1,Big data challenges in railway engineering,"As Big Data becomes part of railroad data analysis, there are many challenges which need to be addressed by the railway industry. This extended abstract highlights some of the challenges from specific examples in railway engineering. This work does not present the challenges of dealing with Big Data in general which is beyond the scope of this paper. The examples provided in this extended abstract cover both the engineering and the management of railroad applications.",7-9,2014.0,https://hughes.com/exploreprivacy.jsp,Computer Science
1219,4644afca80b98dc39ee8ad4cf1ce1e940214c366,A practical guide on conducting eye tracking studies in software engineering,,3128 - 3174,2020.0,https://www.martinez.org/categories/search/postspost.asp,Computer Science
1220,51a66bf3e76cd0457f7533a9449c3410fc72bf4f,Foundations for microwave engineering,Chapter 1: Introduction Chapter 2: Electromagnetic Theory Chapter 3: Transmission Line and Waveguides Chapter 4: Circuit Theory for Waveguiding Systems Chapter 5: Impedence Transformations and Matching Chapter 6: Passive Microwave Devices Chapter 7: Electromagnetic Resonators Chapter 8: Periodic Structures and Filters Chapter 9: Microwave Tubes Chapter 10: Solid State Amplifiers Chapter 11: Parametric Amplifiers Chapter 12: Oscillators and Mixers Appendix One: Useful Relations from Vector Analysis Appendix Two: Bessel Functions Appendix Three: Conformal Mapping Techniques Appendix Four: Physical Constants and Other Data,51-118,1966.0,http://www.walton-nicholson.com/explore/appsearch.php,Mathematics
1221,f74d664f0c3451bd4c31cf973a549a6dc00897c6,\{PROMISE\} Repository of empirical software engineering data,,55-141,2007.0,https://wilkerson.com/explore/app/tagsmain.htm,Computer Science
1222,13aff185e35503491bf40d7e2c6219552f462031,Data Mining: A Prediction for Performance Improvement of Engineering Students using Classification,"Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational data mining is used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.",50-105,2012.0,http://www.nelson.biz/tags/tags/appauthor.php,Computer Science
1223,d77934caf9d8195099ba2188059ecba7a9c324a7,Microbial engineering for the production of advanced biofuels,,320-328,2012.0,http://www.holloway.info/category/categoriespost.html,Business
1224,ee75981d55610ea82c73ec73c3ba7fb9e5f4d9cc,Additive manufacturing techniques for the production of tissue engineering constructs,"‘Additive manufacturing’ (AM) refers to a class of manufacturing processes based on the building of a solid object from three‐dimensional (3D) model data by joining materials, usually layer upon layer. Among the vast array of techniques developed for the production of tissue‐engineering (TE) scaffolds, AM techniques are gaining great interest for their suitability in achieving complex shapes and microstructures with a high degree of automation, good accuracy and reproducibility. In addition, the possibility of rapidly producing tissue‐engineered constructs meeting patient's specific requirements, in terms of tissue defect size and geometry as well as autologous biological features, makes them a powerful way of enhancing clinical routine procedures. This paper gives an extensive overview of different AM techniques classes (i.e. stereolithography, selective laser sintering, 3D printing, melt–extrusion‐based techniques, solution/slurry extrusion‐based techniques, and tissue and organ printing) employed for the development of tissue‐engineered constructs made of different materials (i.e. polymeric, ceramic and composite, alone or in combination with bioactive agents), by highlighting their principles and technological solutions. Copyright © 2012 John Wiley & Sons, Ltd.",174 - 190,2015.0,http://www.allen.com/category/posts/categorieshome.html,Engineering
1225,92b5986906dc286d7008323bd49833eaa2dd5e5f,Sharing Data and Models in Software Engineering,,33-143,2014.0,http://horn.com/wp-content/wp-contentcategory.htm,Computer Science
1226,6704d01bb7788b67bec3194962a57693766bd417,A Survey of Available Corpora for Building Data-Driven Dialogue Systems,"During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets and how they can be used to learn diverse dialogue strategies. We also describe other potential uses of these datasets, such as methods for transfer learning between datasets and the use of external knowledge, and discuss appropriate choice of evaluation metrics for the learning objective.",49-114,2015.0,http://matthews.com/main/mainregister.jsp,Computer Science
1227,bcc7041e0fb7717a7d67a9c00e08b7fb81384cbf,Robust Statistical Methods for Empirical Software Engineering,,579-630,2017.0,http://www.rios.net/categoryterms.php,Computer Science
1228,b579ffbd6bf615be06d97f954710ad7be34abb53,State of the Art Manufacturing and Engineering of Nanocellulose: A Review of Available Data and Industrial Applications,"This review provides a critical overview 
of the recent methods and processes developed for the production of cellulose nanoparticles 
with controlled morphology, structure and properties, and also sums up (1) the processes for the chemical modifications 
of these particles in order to prevent their re-aggregation during spray-drying 
procedures and to increase their reactivity, (2) the recent processes involved in the 
production of nanostructured biomaterials and composites. The structural and physical 
properties of those nanocelluloses, combined with their biodegradability, make them 
materials of choice in the very promising area of nanotechnology, likely subject 
to major commercial successes in the context of green chemistry. With a prospective 
and pioneering approach to the subject matter, various laboratories involved in 
this domain have developed bio-products now almost suitable to industrial applications; 
although some important steps remain to be overcome, those are worth been reviewed and supplemented. At this stage, several pilot 
units and demonstration 
plants have been built to improve, optimize and scale-up the processes developed 
at laboratory scale. Industrial reactors with suitable environment and modern control 
equipment are to be expected within that context. This review shall bring the suitable 
processing dimension that may be needed now, given the numerous reviews outlining 
the product potential attributes. 
An abundant literature database, close to 250 publications and patents, is provided, 
consolidating the various research 
and more practical angles.",165-188,2013.0,https://riddle-garner.com/wp-content/app/tagshome.php,Materials Science
1229,92ca9f57ebb0cfdbfa07e5d5956e11509f902f0b,A practical guide for using statistical tests to assess randomized algorithms in software engineering,"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering",1-10,2011.0,http://www.phelps.net/postssearch.htm,Computer Science
1230,78706624cf201635f30bf93b10ae23806ca3c204,Semantic Matching of Engineering Data Structures,,137-157,2016.0,https://brown.com/tagregister.jsp,Computer Science
1231,a8d3dc7d66a242973dcd9ff0426c6b4369d7a90b,An Introduction to Reliability and Maintainability Engineering,Part 1 Basic reliability models: the failure distribution constant failure rate model time-dependent failure models reliability of systems state dependent systems physical reliability models design for reliability maintainability design for maintainability availability. Part 2 The analysis of failure data: data collection and empirical methods reliabilty testing reliability growth testing identifying failure and repair distribution goodness-of-fit tests. Part 3 Application: reliability estimation and application implementation.,98-105,1996.0,http://www.robinson.biz/tag/searchfaq.php,Engineering
1232,a6e695ddd07aad719001c0fc1129328452385949,The New Data and New Challenges in Multimedia Research,"We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released. The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development. We further present several new challenges in multimedia research that can now be expanded upon with our dataset.",44-136,2015.0,https://robinson-clark.com/mainindex.html,Computer Science
1233,92f160c2ccf58c8098a062abae5f3c99c6951aa0,Benchmarking Attribute Selection Techniques for Discrete Class Data Mining,"Data engineering is generally considered to be a central issue in the development of data mining applications. The success of many learning schemes, in their attempts to construct models of data, hinges on the reliable identification of a small set of highly predictive attributes. The inclusion of irrelevant, redundant, and noisy attributes in the model building process phase can result in poor predictive performance and increased computation. Attribute selection generally involves a combination of search and attribute utility estimation plus evaluation with respect to specific learning schemes. This leads to a large number of possible permutations and has led to a situation where very few benchmark studies have been conducted. This paper presents a benchmark comparison of several attribute selection methods for supervised classification. All the methods produce an attribute ranking, a useful devise for isolating the individual merit of an attribute. Attribute selection is achieved by cross-validating the attribute rankings with respect to a classification learner to find the best attributes. Results are reported for a selection of standard data sets and two diverse learning schemes C4.5 and naive Bayes.",1437-1447,2003.0,https://www.owen-gardner.com/main/search/searchpost.html,Computer Science
1234,37a0fcb9ce00b16b47c7030a50075d149a614762,Fundamentals of Software Engineering,,29-147,2017.0,http://www.ball-hall.biz/list/explorehomepage.htm,Computer Science
1235,918b10db517a3fde10e2b5291186998a33530397,Data analytics and optimization for smart industry,,157 - 171,2020.0,https://www.cox-king.com/postsprivacy.html,Computer Science
1236,f6e0aeaaaae8736007657b627c714261f7638c96,Brainwash: A Data System for Feature Engineering,"A new generation of data processing systems, including web search, Google’s Knowledge Graph, IBM’s Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, finance, energy, and general business. But building them can be challenging, even for computer scientists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier. We explore one crucial pain point in the construction of trained systems: feature engineering. Given the sheer size of modern datasets, feature developers must (1) write code with few effective clues about how their code will interact with the data and (2) repeatedly endure long system waits even though their code typically changes little from run to run. We propose brainwash, a vision for a feature engineering data system that could dramatically ease the ExploreExtract-Evaluate interaction loop that characterizes many trained system projects.",54-132,2013.0,http://www.goodwin.com/categories/taghomepage.html,Computer Science
1237,f77331fc287e4bc76c4b3c464121ec6453fd448b,When big data meets software-defined networking: SDN for big data and big data for SDN,"Both big data and software-defined networking (SDN) have attracted great interests from both academia and industry. These two important areas have traditionally been addressed separately in the most of previous works. However, on the one hand, the good features of SDN can greatly facilitate big data acquisition, transmission, storage, and processing. On the other hand, big data will have profound impacts on the design and operation of SDN. In this paper, we present the good features of SDN in solving several issues prevailing with big data applications, including big data processing in cloud data centers, data delivery, joint optimization, scientific big data architectures and scheduling issues. We show that SDN can manage the network efficiently for improving the performance of big data applications. In addition, we show that big data can benefit SDN as well, including traffic engineering, cross-layer design, defeating security attacks, and SDN-based intra and inter data center networks. Moreover, we discuss a number of open issues that need to be addressed to jointly consider big data and SDN in future research.",58-65,2016.0,http://www.kramer.com/postsmain.php,Computer Science
1238,1077bce87735472e03ac3685cf5a8edfea1710c4,"Female peers in small work groups enhance women's motivation, verbal participation, and career aspirations in engineering","Significance Advances in science, technology, engineering, and mathematics are critical to the American economy and require a robust workforce. The scarcity of women in this workforce is a well-recognized problem, but data-driven solutions to this problem are less common. We provide experimental evidence showing that gender composition of small groups in engineering has a substantial impact on undergraduate women’s persistence. Women participate more actively in engineering groups when members are mostly female vs. mostly male or in equal gender proportions. Women feel less anxious in female-majority groups vs. minority groups, especially as first-year students. Gender-parity groups are less effective than female-majority groups in promoting verbal participation. Female peers protect women’s confidence and engineering career aspirations despite masculine stereotypes about engineering. For years, public discourse in science education, technology, and policy-making has focused on the “leaky pipeline” problem: the observation that fewer women than men enter science, technology, engineering, and mathematics fields and more women than men leave. Less attention has focused on experimentally testing solutions to this problem. We report an experiment investigating one solution: we created “microenvironments” (small groups) in engineering with varying proportions of women to identify which environment increases motivation and participation, and whether outcomes depend on students’ academic stage. Female engineering students were randomly assigned to one of three engineering groups of varying sex composition: 75% women, 50% women, or 25% women. For first-years, group composition had a large effect: women in female-majority and sex-parity groups felt less anxious than women in female-minority groups. However, among advanced students, sex composition had no effect on anxiety. Importantly, group composition significantly affected verbal participation, regardless of women’s academic seniority: women participated more in female-majority groups than sex-parity or female-minority groups. Additionally, when assigned to female-minority groups, women who harbored implicit masculine stereotypes about engineering reported less confidence and engineering career aspirations. However, in sex-parity and female-majority groups, confidence and career aspirations remained high regardless of implicit stereotypes. These data suggest that creating small groups with high proportions of women in otherwise male-dominated fields is one way to keep women engaged and aspiring toward engineering careers. Although sex parity works sometimes, it is insufficient to boost women’s verbal participation in group work, which often affects learning and mastery.",4988 - 4993,2015.0,http://www.carter.org/explore/tags/taghomepage.html,Medicine
1239,b10829b87072a6c3faf971bd94c1f0dc71053194,"A cloud approach to unified lifecycle data management in architecture, engineering, construction and facilities management: Integrating BIMs and SNS",,173-188,2013.0,http://www.ortiz.net/list/main/tagsprivacy.php,Computer Science
1240,ac6d8eaffe0481e51955eff242acc5b3faea925e,Data Analysis WorkbeNch (DAWN),DAWN is a generic data analysis software platform that has been developed for use at synchrotron beamlines for data visualization and analysis. Its generic design makes it suitable for use in a range of scientific and engineering applications.,853 - 858,2015.0,https://lee-cox.com/blog/app/appregister.html,Medicine
1241,29196eb8c80a6fd6a159373f14ff323f081a8b7a,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",305 - 308,2013.0,http://www.scott.biz/mainprivacy.php,Medicine
1242,a7797aed6d23d7b599e71ad129211c2834925c0d,EMG Pattern Recognition in the Era of Big Data and Deep Learning,"The increasing amount of data in electromyographic (EMG) signal research has greatly increased the importance of developing advanced data analysis and machine learning techniques which are better able to handle “big data”. Consequently, more advanced applications of EMG pattern recognition have been developed. This paper begins with a brief introduction to the main factors that expand EMG data resources into the era of big data, followed by the recent progress of existing shared EMG data sets. Next, we provide a review of recent research and development in EMG pattern recognition methods that can be applied to big data analytics. These modern EMG signal analysis methods can be divided into two main categories: (1) methods based on feature engineering involving a promising big data exploration tool called topological data analysis; and (2) methods based on feature learning with a special emphasis on “deep learning”. Finally, directions for future research in EMG pattern recognition are outlined and discussed.",21,2018.0,http://greene.com/wp-contentregister.html,Computer Science
1243,845afdf05ac75fedb65532487aadd0538bc4c6da,Qualitative Methods in Empirical Studies of Software Engineering,"While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.",557-572,1999.0,https://park-holmes.com/posts/main/tagsindex.htm,Computer Science
1244,0dde092a85ce6451fc28685e7816a0df9dbaa21b,Reverse engineering of regulatory networks in human B cells,,382-390,2005.0,http://mitchell.com/blog/tag/apphomepage.jsp,Biology
1245,e13a2f9914112e1099e54a1d85d0f95da4bbb3f3,Computer Engineering,"Choose any 3 CPE Core Courses Below: 2, 4 9 ECE 501 Contemporary Digital Systems ECE 505 Digital Signal Processing ECE 530 Digital Integrated Circuit Design ECE 532 Embedded Systems ECE 533 Computer Design ECE 586 Computer Networks CPS 510 System Analysis CPS 536 Operating Systems CPS 570 Data Communications Specialization Area: 4 9 Choose any 3 graduate courses from ECE 501:ECE 694 or CPS 501:CPS 694, excluding thesis courses.",24-133,2018.0,http://nelson-smith.com/search/taglogin.html,Technology
1246,678731fe120f4faf05de4ee6b5def8305c8155cb,Data Processing of Point Clouds for Object Detection for Structural Engineering Applications,"This research investigates the use of high‐resolution three‐dimensional terrestrial laser scanners as tools to capture geometric range data of complex scenes for structural engineering applications. Laser scanning technology is continuously improving, with commonly available scanners now able to capture over 1,000,000 points per second with an accuracy of ∼0.1 mm. This research focuses on developing the foundation toward the use of laser scanning to structural engineering applications, including structural health monitoring, collapse assessment, and post‐hazard response assessment. One of the keys to this work is to establish a process for extracting important information from raw laser‐scanned data sets such as the location, orientation, and size of objects in a scene, and location of damaged regions on a structure. A methodology for processing range data to identify objects in the scene is presented. Previous work in this area has created an initial foundation of basic data processing steps. Existing algorithms, including sharp feature detection and segmentation are implemented and extended in this work. Additional steps to remove extraneous and outlying points are added. Object detection based on a predefined library is developed allowing generic description of objects. The algorithms are demonstrated on synthetic scenes as well as validated on range data collected from an experimental test specimen and a collapsed bridge. The accuracy of the object detection is presented, demonstrating the applicability of the methodology. These additional steps and modifications to existing algorithms are presented to advance the performance of data processing on laser scan range data sets for future application in structural engineering applications such as robust determination of damage location and finite element modeling.",77-131,2013.0,http://gonzalez.org/posts/wp-contentsearch.php,Computer Science
1247,acdc89274e34d2f905a055aad41e7f815aef52eb,Selecting Empirical Methods for Software Engineering Research,,285-311,2008.0,https://www.lewis.com/category/wp-content/maincategory.asp,Computer Science
1248,1f53b2c6428ca6b4484b201e14b56d41db5c5ce1,Prognostics and Health Management: A Review on Data Driven Approaches,"Prognostics and health management (PHM) is a framework that offers comprehensive yet individualized solutions for managing system health. In recent years, PHM has emerged as an essential approach for achieving competitive advantages in the global market by improving reliability, maintainability, safety, and affordability. Concepts and components in PHM have been developed separately in many areas such as mechanical engineering, electrical engineering, and statistical science, under varied names. In this paper, we provide a concise review of mainstream methods in major aspects of the PHM framework, including the updated research from both statistical science and engineering, with a focus on data-driven approaches. Real world examples have been provided to illustrate the implementation of PHM in practice.",793161,2015.0,https://perry.biz/categories/app/searchsearch.html,Engineering
1249,ee25620eea19f793d598dbbf6241cf7520a60ccd,Patching as Translation: the Data and the Metaphor,"Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that software patching is like language translation. We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as “proof-of-concept” tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation.",275-286,2020.0,https://www.smith.biz/blog/categories/explorehome.html,Computer Science
1250,f759d01c4cc9633a3732c353ed0ce8f3009f52af,Naming the pain in requirements engineering,,2298 - 2338,2016.0,https://stephens-clark.org/posts/tag/categoryprivacy.asp,Computer Science
1251,9e1e273ed7b6f3210f70576a36312670441d35f6,"A Review of Trimming in Isogeometric Analysis: Challenges, Data Exchange and Simulation Aspects",,1059 - 1127,2017.0,http://www.taylor.com/explorefaq.php,Medicine
1252,4f9b916ae1c38b991f39b6ebd6e0a92f88ece698,Building detection in very high resolution multispectral data with deep learning features,"The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach.",1873-1876,2015.0,https://griffin.com/searchregister.html,Computer Science
1253,cc3137164f2d292a5255d5322abcd6eb95c087e3,Data-Driven Modeling: Using MATLAB® in Water Resources and Environmental Engineering,,61-150,2013.0,https://www.anderson.com/category/tags/tagsfaq.php,Environmental Science
1254,a4a582c6739c8f6d88f3ad01671b5b6733eb464c,Bridges: a uniquely flexible HPC resource for new communities and data analytics,"In this paper, we describe Bridges, a new HPC resource that will integrate advanced memory technologies with a uniquely flexible, user-focused, data-centric environment to empower new research communities, bring desktop convenience to HPC, connect to campuses, and drive complex workflows. Bridges will differ from traditional HPC systems and support new communities through extensive interactivity, gateways (convenient web interfaces that hide complex functionality and ease access to HPC resources) and tools for gateway building, persistent databases and web servers, high-productivity programming languages, and virtualization. Bridges will feature three tiers of processing nodes having 128GB, 3TB, and 12TB of hardware-enabled coherent shared memory per node to support memory-intensive applications and ease of use, together with persistent database and web nodes and nodes for logins, data transfer, and system management. State-of-the-art Intel® Xeon® CPUs and NVIDIA Tesla GPUs will power Bridges' compute nodes. Multiple filesystems will provide optimal handling for different data needs: a high-performance, parallel, shared filesystem, node-local filesystems, and memory filesystems. Bridges' nodes and parallel filesystem will be interconnected by the Intel Omni-Path Fabric, configured in a topology developed by PSC to be optimal for the anticipated data-centric workload. Bridges will be a resource on XSEDE, the NSF Extreme Science and Engineering Discovery Environment, and will interoperate with other advanced cyberinfrastructure resources. Through a pilot project with Temple University, Bridges will develop infrastructure and processes for campus bridging, consisting of offloading jobs at periods of unusually high load to the other site and facilitating cross-site data management. Education, training, and outreach activities will raise awareness of Bridges and data-intensive science across K-12 and university communities, industry, and the general public.",96-107,2015.0,http://www.wells.biz/category/categorieshomepage.asp,Computer Science
1255,d1cc35e2a547ba79f1b07fdd81ee0da264c0d6b6,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",108-119,2016.0,http://martinez-summers.com/app/app/explorefaq.php,Computer Science
1256,c575eb25feb0f06d2702fcf7751f6b4f61b892ee,The Emerging Role of Data Scientists on Software Development Teams,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",96-107,2016.0,https://www.thomas.com/categorymain.html,Computer Science
1257,b83a7f29f7ac72b7b604b9f94e47d07539e7f1b3,Handbook of Accelerator Physics and Engineering,"Concerned with the design and operation of modern accelerators including linacs, synchrotrons and storage rings, this text includes both theoretical and practical matters. Chapters on beam dynamics and electromagnetic and nuclear interactions deals with linear and nonlinear single particle and collective effects including spin motion, beam-environment, beam-beam and intrabeam interactions. The impedance concept and calculations are covered along with the instabilities associated with the various interactions mentioned. A chapter on operational considerations deals with orbit error assessment and correction. Chapters on mechanical and electrical considerations present material data and aspects of component design including heat transfer and refrigeration. Hardware systems for particle sources, feedback systems, confinement and acceleration (both normal conduction and superconducting) receive detailed treatment in a subsystems chapter, which also covers beam measurement techniques and apparatus. The closing chapter gives data and methods for radiation protection computations as well as much data on radiation damage to various materials and devices.",38-128,2013.0,http://www.hernandez.com/tags/searchindex.htm,Physics
1258,050d2be0b031878282e2bc626ffe31e064679188,Big Social Data Analytics in Journalism and Mass Communication,"This article presents an empirical study that investigated and compared two “big data” text analysis methods: dictionary-based analysis, perhaps the most popular automated analysis approach in social science research, and unsupervised topic modeling (i.e., Latent Dirichlet Allocation [LDA] analysis), one of the most widely used algorithms in the field of computer science and engineering. By applying two “big data” methods to make sense of the same dataset—77 million tweets about the 2012 U.S. presidential election—the study provides a starting point for scholars to evaluate the efficacy and validity of different computer-assisted methods for conducting journalism and mass communication research, especially in the area of political communication.",332 - 359,2016.0,https://www.bryan-khan.info/wp-content/listterms.html,Computer Science
1259,1f2a86ab0d88b9bb31a7cafd1c1c3fcb4e193547,[Journal First] Data Scientists in Software Teams: State of the Art and Challenges,"The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams. For example, Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",585-585,2018.0,https://www.long.org/main/tags/tagsfaq.jsp,Computer Science
1260,f7d3f3a23c1b284a17adc93a922c56be38d221df,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.","
          246-255
        ",2016.0,http://jefferson.org/searchabout.html,Medicine
1261,314ebb14030eb6a321dd86440ffe56887541b07c,Identifying 21st Century STEM Competencies Using Workplace Data,,284-301,2015.0,https://www.velez-hernandez.com/posts/appauthor.php,Engineering
1262,a9911c08c685815f4dbce2a09eacfc5bdc00e6ca,An Overview of the Tissue Engineering Market in the United States from 2011 to 2018.,"IMPACT STATEMENT
This report seeks to provide an update of the current landscape of the tissue engineering market in the United States from an unbiased point of view by analyzing the financial reports provided by tissue engineering companies, as well as data from publicly available clinical trials with relevant tissue engineering applications.","
          1-8
        ",2019.0,http://www.miller-patterson.com/category/blogabout.htm,Medicine
1263,267fd0014fb61d46f764dc5d0c720507a3a0a583,Mining Social Media Data for Understanding Students’ Learning Experiences,"Students' informal conversations on social media (e.g., Twitter, Facebook) shed light into their educational experiences-opinions, feelings, and concerns about the learning process. Data from such uninstrumented environments can provide valuable knowledge to inform student learning. Analyzing such data, however, can be challenging. The complexity of students' experiences reflected from social media content requires human interpretation. However, the growing scale of data demands automatic data analysis techniques. In this paper, we developed a workflow to integrate both qualitative analysis and large-scale data mining techniques. We focused on engineering students' Twitter posts to understand issues and problems in their educational experiences. We first conducted a qualitative analysis on samples taken from about 25,000 tweets related to engineering students' college life. We found engineering students encounter problems such as heavy study load, lack of social engagement, and sleep deprivation. Based on these results, we implemented a multi-label classification algorithm to classify tweets reflecting students' problems. We then used the algorithm to train a detector of student problems from about 35,000 tweets streamed at the geo-location of Purdue University. This work, for the first time, presents a methodology and results that show how informal social media data can provide insights into students' experiences.",246-259,2014.0,http://washington-mason.com/explorehome.htm,Computer Science
1264,e473b85d6fff3c667dec45fa8ee233b6c0db7378,"Biomedical Engineering Online Review of ""data Mining: Practical Machine Learning Tools and Techniques"" by Witten and Frank Book Details","In the early 1990s some sectors of the computer science community were developing the idea of data understanding as a discovery-driven, systematic and iterative process. This ""data mining"" research and development area was expected to take advantage of the expansion and consolidation of machine learning methodologies together with the integration of traditional statistical analysis and database management strategies. The main goal was to identify relevant, interesting and potentially novel informational patterns and relationships in large data sets to support decision making and knowledge discovery. In the mid 1990s developers and users of decision-making support systems in areas such as finance (e.g. credit approval and fraud detection applications), marketing and sales analysis (e.g. shopping patterns and sales prediction) were showing a great deal of enthusiasm about the business value of data mining applications. During the next few years international conferences, journals and books were more frequently reporting advances, tools and applications in other areas such as biomedical informat-ics, engineering, physics, law enforcement and agriculture. Today data mining is seen as a discipline or paradigm that actively aids in the development of these and other scientific areas (e.g. Web-based computing and systems biology). Data mining has become a fundamental research topic in the progression of computing applications in health care and biomedicine. Advances in data mining have applications and implications in areas ranging from information management in healthcare organisations, consumer health informatics, public health and epidemiology, patient care and monitoring systems, large-scale image analysis to information extraction and classification of scientific literature [1]. Approaches, techniques and applications associated with data mining has also significantly supported different data understanding and decision support tasks in bio-signal processing, such as the classification , visualisation and identification of complex relationships between diagnostic variables or groups of patients [2,3]. In ""Data Mining: Practical Machine Learning Tools and Tech-niques"" Witten and Frank offer users, students and researchers alike a balanced, clear introduction to concepts , techniques and tools for designing, implementing and evaluating data mining applications. Although it puts emphasis on machine learning techniques, it also introduces basic statistical and information representation methods. This book provides a variety of simple yet elegant explanations to guide the reader to understand essential concepts and approaches. The book can also be seen as a well-structured, intensive tutorial, which excels in explaining how to implement solutions to different problems .",22-115,,https://www.parrish.com/blogprivacy.html,Technology
1265,ba69dc1fe7da073efcadcde60ba22f085f440b61,A Study on Big Knowledge and Its Engineering Issues,"After entering the big data era, a new term of ‘big knowledge’ has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed.",1630-1644,2019.0,http://www.smith.com/category/categories/mainabout.html,Computer Science
1266,f04a9db024afd57bf3cba3e1c9028e8d2f6f3f13,Statistics and Data Analysis for Financial Engineering,,84-136,2010.0,https://thomas.com/wp-contentabout.jsp,Economics
1267,402fac84b796104a63b7f917307347040fa616bf,A robust data mining approach for formulation of geotechnical engineering systems,"Purpose – The complexity of analysis of geotechnical behavior is due to multivariable dependencies of soil and rock responses. In order to cope with this complex behavior, traditional forms of engineering design solutions are reasonably simplified. Incorporating simplifying assumptions into the development of the traditional models may lead to very large errors. The purpose of this paper is to illustrate capabilities of promising variants of genetic programming (GP), namely linear genetic programming (LGP), gene expression programming (GEP), and multi‐expression programming (MEP) by applying them to the formulation of several complex geotechnical engineering problems.Design/methodology/approach – LGP, GEP, and MEP are new variants of GP that make a clear distinction between the genotype and the phenotype of an individual. Compared with the traditional GP, the LGP, GEP, and MEP techniques are more compatible with computer architectures. This results in a significant speedup in their execution. These method...",242-274,2011.0,http://beasley-rice.com/categoriesindex.htm,Computer Science
1268,49f9a1caabcf5a24ee0dbdc829562b81b7867b72,The anatomy of big data computing,"Advances in information technology and its widespread growth in several areas of business, engineering, medical, and scientific studies are resulting in information/data explosion. Knowledge discovery and decision‐making from such rapidly growing voluminous data are a challenging task in terms of data organization and processing, which is an emerging trend known as big data computing, a new paradigm that combines large‐scale compute, new data‐intensive techniques, and mathematical models to build data analytics. Big data computing demands a huge storage and computing for data curation and processing that could be delivered from on‐premise or clouds infrastructures. This paper discusses the evolution of big data computing, differences between traditional data warehousing and big data, taxonomy of big data computing and underpinning technologies, integrated platform of big data and clouds known as big data clouds, layered architecture and components of big data cloud, and finally open‐technical challenges and future directions. Copyright © 2015 John Wiley & Sons, Ltd.",105 - 79,2015.0,https://collins.com/search/bloghome.htm,Computer Science
1269,0442b04b4e8741900b65de0721f0c3e152e044ef,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",171-193,2015.0,http://www.duffy.biz/blog/searchmain.html,Computer Science
1270,64cc4ef5def3919049bdd3a645af198922d626c2,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",237-248,2016.0,http://williamson.com/blog/posts/appregister.html,Computer Science
1271,a4a84f5b19aa8c1eb8b31db67d2534ad3565ccab,Visualization of Time-Oriented Data,,1-267,2011.0,https://www.collins-martinez.info/blog/tagmain.htm,Computer Science
1272,d7385eb4c0482f4f224b591419b18da094f3c729,Engineering the singlet–triplet energy splitting in a TADF molecule,"The key to engineering an efficient TADF emitter is to achieve a small energy splitting between a pair of molecular singlet and triplet states. This work makes important contributions towards achieving this goal. By studying the new TADF emitter 2,7-bis(phenoxazin-10-yl)-9,9-dimethylthioxanthene-S,S-dioxide (DPO-TXO2) and the donor and acceptor units separately, the available radiative and non-radiative pathways of DPO-TXO2 have been identified. The energy splitting between singlet and triplet states was clearly identified in four different environments, in solutions and solid state. The results show that DPO-TXO2 is a promising TADF emitter, having ΔEST = 0.01 eV in zeonex matrix. We further show how the environment plays a key role in the fine tuning of the energy levels of the 1CT state with respect to the donor 3LED triplet state, which can then be used to control the ΔEST energy value. We elucidate the TADF mechanism dynamics when the 1CT state is located below the 3LE triplet state which it spin orbit couples to, and we also discuss the OLED device performance with this new emitter, which shows maximum external quantum efficiency (E.Q.E.) of 13.5% at 166 cd m−2.",3815-3824,2016.0,https://serrano.net/tag/categoriesregister.htm,Materials Science
1273,89535aa63bc5dac6f3beb60b813abb77aa4309d1,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",85 - 97,2017.0,https://www.mcclure-barker.com/explore/wp-contentfaq.php,Medicine
1274,9ada0c69d4d8cb6fefb8f2dd3370d32df3b627c5,Software engineering in start-up companies: An analysis of 88 experience reports,,68 - 102,2018.0,http://myers-roberts.com/category/applogin.jsp,Computer Science
1275,6ff5544640e0ac1e41b98c98a8ef2cd0ba62e9b5,Cloud Paradigms and Practices for Computational and Data-Enabled Science and Engineering,"Clouds are rapidly joining high-performance computing (HPC) systems, clusters, and grids as viable platforms for scientific exploration and discovery. As a result, understanding application formulations and usage modes that are meaningful in such a hybrid infrastructure, and how application workflows can effectively utilize it, is critical. Here, three hybrid HPC/grid and cloud cyber infrastructure usage modes are explored: HPC in the Cloud, HPC plus Cloud, and HPC as a Service, presenting illustrative scenarios in each case and outlining benefits, limitations, and research challenges.",10-18,2013.0,https://newton.org/list/blogindex.php,Computer Science
1276,efc91095d187abafeab8785eb57bbdef66c82d1c,Data and Computer Communications,"Data and Computer Communications, 9e, is a two-time winner of the best Computer Science and Engineering textbook of the year award from the Textbook and Academic Authors Association. It is ideal for one/two-semester courses in Computer Networks, Data Communications, and Communications Networks in CS, CIS, and Electrical Engineering departments. With a focus on the most current technology and a convenient modular format, this best-selling text offers a clear and comprehensive survey of the entire data and computer communications field. Emphasizing both the fundamental principles as well as the critical role of performance in driving protocol and network design, it explores in detail all the critical technical areas in data communications, wide-area networking, local area networking, and protocol design.",82-103,1985.0,https://www.smith-graham.biz/postshomepage.htm,Computer Science
1277,8de121442c5df6ebd1d93c132086c80ae7613b06,Handbook of software reliability engineering,Technical foundations introduction software reliability and system reliability the operational profile software reliability modelling survey model evaluation and recalibration techniques practices and experiences best current practice of SRE software reliability measurement experience measurement-based analysis of software reliability software fault and failure classification techniques trend analysis in validation and maintenance software reliability and field data analysis software reliability process assessment emerging techniques software reliability prediction metrics software reliability and testing fault-tolerant SRE software reliability using fault trees software reliability process simulation neural networks and software reliability. Appendices: software reliability tools software failure data set repository.,79-130,1996.0,http://www.parrish.com/explorecategory.htm,Computer Science
1278,fd51bef5d4c12bb1fa49457e5a44fef0b8bc1295,Ground: A Data Context Service,"Ground is an open-source data context service , a system to manage all the information that informs the use of data. Data usage has changed both philosophically and practically in the last decade, creating an opportunity for new data context services to foster further innovation. In this paper we frame the challenges of managing data context with basic ABCs: Applications , Behavior , and Change . We provide motivation and design guidelines, present our initial design of a common metamodel and API, and explore the current state of the storage solutions that could serve the needs of a data context service. Along the way we highlight opportunities for new research and engineering solutions.",74-128,2017.0,https://www.davenport.org/category/category/searchregister.php,Computer Science
1279,301311f883cb3df1b1c00077ddf5f2fc0ed2f4f8,Traffic engineering in software defined networks,"Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.",2211-2219,2013.0,https://casey.biz/mainlogin.asp,Computer Science
1280,902a8d4496020759dffd14265775d8e95381fdd9,Microseismic Imaging Of Hydraulic Fracturing : improved engineering of unconventional shale reservoirs,"Microseismic Imaging of Hydraulic Fracturing: Improved Engineering of Unconventional Shale Reservoirs (SEG Distinguished Instructor Series No. 17) covers the use of microseismic data to enhance engineering design of hydraulic fracturing and well completion. The book, which accompanies the 2014 SEG Distinguished Instructor Short Course, describes the design, acquisition, processing, and interpretation of an effective microseismic project. The text includes a tutorial of the basics of hydraulic fracturing, including the geologic and geomechanical factors that control fracture growth. In addition to practical issues associated with collecting and interpreting microseismic data, potential pitfalls and quality-control steps are discussed. Actual case studies are used to demonstrate engineering benefits and improved production through the use of microseismic monitoring. Providing a practical user guide for survey design, quality control, interpretation, and application of microseismic hydraulic fracture monitoring, this book will be of interest to geoscientists and engineers involved in development of unconventional reservoirs.",93-143,2014.0,http://grant-mckee.com/list/searchabout.htm,Engineering
1281,0e528eb8167c68930c2e1ab20ab5c14f98446927,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",40-64,2015.0,https://benton.info/category/mainmain.html,Computer Science
1282,43f3a76901595b91832e12e0b5f27a6ef4e9fb92,On the reproducibility of empirical software engineering studies based on data retrieved from development repositories,,75-89,2012.0,https://thomas-mcdonald.org/main/categories/blogsearch.php,Computer Science
1283,0fd8e04c9717409fa23e0d2fc995920a23c933be,Machine learning in biomedical engineering,,1 - 3,2018.0,https://www.nelson.info/searchterms.htm,Engineering
1284,eebd35be4d21e1485733c406ff6eebf7746e84ac,A Taxonomy of Data Quality Challenges in Empirical Software Engineering,"Reliable empirical models such as those used in software effort estimation or defect prediction are inherently dependent on the data from which they are built. As demands for process and product improvement continue to grow, the quality of the data used in measurement and prediction systems warrants increasingly close scrutiny. In this paper we propose a taxonomy of data quality challenges in empirical software engineering, based on an extensive review of prior research. We consider current assessment techniques for each quality issue and proposed mechanisms to address these issues, where available. Our taxonomy classifies data quality issues into three broad areas: first, characteristics of data that mean they are not fit for modeling, second, data set characteristics that lead to concerns about the suitability of applying a given model to another data set, and third, factors that prevent or limit data accessibility and trust. We identify this latter area as of particular need in terms of further research.",97-106,2013.0,https://williams-gilbert.com/tagregister.asp,Computer Science
1285,660fa773b70629aa8c36f4e11812b2b07fcfa04d,Starfish: A Self-tuning System for Big Data Analytics,"Timely and cost-effective analytics over “Big Data” is now a key ingredient for success in many businesses, scientific and engineering disciplines, and government endeavors. The Hadoop software stack—which consists of an extensible MapReduce execution engine, pluggable distributed storage engines, and a range of procedural to declarative interfaces—is a popular choice for big data analytics. Most practitioners of big data analytics—like computational scientists, systems researchers, and business analysts—lack the expertise to tune the system to get good performance. Unfortunately, Hadoop’s performance out of the box leaves much to be desired, leading to suboptimal use of resources, time, and money (in payas-you-go clouds). We introduce Starfish, a self-tuning system for big data analytics. Starfish builds on Hadoop while adapting to user needs and system workloads to provide good performance automatically, without any need for users to understand and manipulate the many tuning knobs in Hadoop. While Starfish’s system architecture is guided by work on self-tuning database systems, we discuss how new analysis practices over big data pose new challenges; leading us to different design choices in Starfish.",261-272,2011.0,http://mcknight.com/tag/exploreabout.php,Computer Science
1286,30dd12a894eff2a488c83f565bc287b4dd03c0cc,Howard: A Dynamic Excavator for Reverse Engineering Data Structures,"Even the most advanced reverse engineering techniques and products are weak in recovering data structures in stripped binaries—binaries without symbol tables. Unfortunately, forensics and reverse engineering without data structures is exceedingly hard. We present a new solution, known as Howard, to extract data structures from C binaries without any need for symbol tables. Our results are significantly more accurate than those of previous methods — sufficiently so to allow us to generate our own (partial) symbol tables without access to source code. Thus, debugging such binaries becomes feasible and reverse engineering becomes simpler. Also, we show that we can protect existing binaries from popular memory corruption attacks, without access to source code. Unlike most existing tools, our system uses dynamic analysis (on a QEMU-based emulator) and detects data structures by tracking how a program uses memory.",93-124,2011.0,http://www.ross.com/blogauthor.jsp,Computer Science
1287,6bc29d62b0a0fd3e37f37d6740156c95b619c23c,Green Chemistry,"LzDByswE7 XKuNyvyZ5 ylXPD0seb eblOzjDNV 70TI6s0jO kchMB1AQZ YYvstT6wF ZyRVmeKkO sCYKiBNhF F3hbMRF06 FRWtspXv2 cCWlGYZYM QBYX3EhFP e6TKPXSsc nnCm6eZsk znFbTIOvL JRBi5slQo SpnSiTsAL tWmBgvRAi 2HMHG7SDo MKqLbxJAR fIGrCjjPc HksYXoBw0 2RGOqOkhv rkspisllP RGe6BNzCM LERbhvQnF Db7Jtpbve Green Chemistry: Theory and Practice PR-92080 US/Data/Engineering-Transportation 5/5 From 504 Reviews Paul T. Anastas, John C. Warner DOC | *audiobook | ebooks | Download PDF | ePub",50-122,2018.0,http://martin-hanna.net/tag/wp-contenthome.jsp,Technology
1288,c5c00002b3adf106219bc6b31a7a5cd9f998168c,Intelligent Fault Diagnosis and Prognosis for Engineering Systems,"PREFACE. ACKNOWLEDGMENTS. PROLOGUE. 1 INTRODUCTION. 1.1 Historical Perspective. 1.2 Diagnostic and Prognostic System Requirements. 1.3 Designing in Fault Diagnostic and Prognostic Systems. 1.4 Diagnostic and Prognostic Functional Layers. 1.5 Preface to Book Chapters. 1.6 References. 2 SYSTEMS APPROACH TO CBM/PHM. 2.1 Introduction. 2.2 Trade Studies. 2.3 Failure Modes and Effects Criticality Analysis (FMECA). 2.4 System CBM Test-Plan Design. 2.5 Performance Assessment. 2.6 CBM/PHM Impact on Maintenance and Operations: Case Studies. 2.7 CBM/PHM in Control and Contingency Management. 2.8 References. 3 SENSORS AND SENSING STRATEGIES. 3.1 Introduction. 3.2 Sensors. 3.3 Sensor Placement. 3.4 Wireless Sensor Networks. 3.5 Smart Sensors. 3.6 References. 4 SIGNAL PROCESSING AND DATABASE MANAGEMENT SYSTEMS. 4.1 Introduction. 4.2 Signal Processing in CBM/PHM. 4.3 Signal Preprocessing. 4.4 Signal Processing. 4.5 Vibration Monitoring and Data Analysis. 4.6 Real-Time Image Feature Extraction and Defect/Fault Classification. 4.7 The Virtual Sensor. 4.8 Fusion or Integration Technologies. 4.9 Usage-Pattern Tracking. 4.10 Database Management Methods. 4.11 References. 5 FAULT DIAGNOSIS. 5.1 Introduction. 5.2 The Diagnostic Framework. 5.3 Historical Data Diagnostic Methods. 5.4 Data-Driven Fault Classification and Decision Making. 5.5 Dynamic Systems Modeling. 5.6 Physical Model-Based Methods. 5.7 Model-Based Reasoning. 5.8 Case-Based Reasoning (CBR). 5.9 Other Methods for Fault Diagnosis. 5.10 A Diagnostic Framework for Electrical/Electronic Systems. 5.11 Case Study: Vibration-Based Fault Detection and Diagnosis for Engine Bearings. 5.12 References. 6 FAULT PROGNOSIS. 6.1 Introduction. 6.2 Model-Based Prognosis Techniques. 6.3 Probability-Based Prognosis Techniques. 6.4 Data-Driven Prediction Techniques. 6.5 Case Studies. 6.6 References. 7 FAULT DIAGNOSIS AND PROGNOSIS PERFORMANCE METRICS. 7.1 Introduction. 7.2 CBM/PHM Requirements Definition. 7.3 Feature-Evaluation Metrics. 7.4 Fault Diagnosis Performance Metrics. 7.5 Prognosis Performance Metrics. 7.6 Diagnosis and Prognosis Effectiveness Metrics. 7.7 Complexity/Cost-Benefit Analysis of CBM/PHM Systems. 7.8 References. 8 LOGISTICS: SUPPORT OF THE SYSTEM IN OPERATION. 8.1 Introduction. 8.2 Product-Support Architecture, Knowledge Base, and Methods for CBM. 8.3 Product Support without CBM. 8.4 Product Support with CBM. 8.5 Maintenance Scheduling Strategies. 8.6 A Simple Example. 8.7 References. APPENDIX. INDEX.",48-124,2006.0,https://burnett-poole.com/blogmain.php,Engineering
1289,ebd1c86b670da4f10ac691828fba9f860327bee9,Internet Engineering Task Force (ietf) the Javascript Object Notation (json) Data Interchange Format,"JavaScript Object Notation (JSON) is a lightweight, text-based, language-independent data interchange format. It was derived from the ECMAScript Programming Language Standard. JSON defines a small set of formatting rules for the portable representation of structured data. This document removes inconsistencies with other specifications of JSON, repairs specification errors, and offers experience-based interoperability guidance. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained at in effect on the date of publication of this document. Please review these documents carefully, as they describe your rights and restrictions with respect to this document. Code Components extracted from this document must include Simplified BSD License text as described in Section 4.e of the Trust Legal Provisions and are provided without warranty as described in the Simplified BSD License. This document may contain material from IETF Documents or IETF Contributions published or made publicly available before November 10, 2008. The person(s) controlling the copyright in some of this material may not have granted the IETF Trust the right to allow modifications of such material outside the IETF Standards Process. Without obtaining an adequate license from the person(s) controlling the copyright in such materials, this document may not be modified outside the IETF Standards Process, and derivative works of it may not be created outside the IETF Standards Process, except to format it for publication as an RFC or to translate it into languages other than English.",69-137,,https://mahoney.biz/app/categoryabout.htm,Technology
1290,e7d9c306138b3a583c48d9a3d46a5597221deaae,Semantic data mining: A survey of ontology-based approaches,"Semantic Data Mining refers to the data mining tasks that systematically incorporate domain knowledge, especially formal semantics, into the process. In the past, many research efforts have attested the benefits of incorporating domain knowledge in data mining. At the same time, the proliferation of knowledge engineering has enriched the family of domain knowledge, especially formal semantics and Semantic Web ontologies. Ontology is an explicit specification of conceptualization and a formal way to define the semantics of knowledge and data. The formal structure of ontology makes it a nature way to encode domain knowledge for the data mining use. In this survey paper, we introduce general concepts of semantic data mining. We investigate why ontology has the potential to help semantic data mining and how formal semantics in ontologies can be incorporated into the data mining process. We provide detail discussions for the advances and state of art of ontology-based approaches and an introduction of approaches that are based on other form of knowledge representations.",244-251,2015.0,http://www.thompson-ruiz.org/main/main/wp-contentlogin.html,Computer Science
1291,70f0bdfad8d671b95a1840e3954e0194989806e6,The Informatics Transform: Re-Engineering Libraries for the Data Decade,"In this paper, Liz Lyon explores how libraries can re-shape to better reflect the requirements and challenges of today’s data-centric research landscape. The Informatics Transform presents five assertions as potential pathways to change, which will help libraries to re-position, re-profile, and re-structure to better address research data management challenges. The paper deconstructs the institutional research lifecycle and describes a portfolio of ten data support services which libraries can deliver to support the research lifecycle phases. Institutional roles and responsibilities for research data management are also unpacked, building on the framework from the earlier Dealing with Data Report. Finally, the paper examines critical capacity and capability challenges and proposes some innovative steps to addressing the significant skills gaps.",126-138,2012.0,http://ford-joseph.com/categoriesindex.jsp,Computer Science
1292,35fd9f4c4289a293e209846717ab28e1eb7fc994,Developing employability in engineering education: a systematic review of the literature,"ABSTRACT In this systematic review of the research literature on engineering employability, curricular and pedagogical arrangements that prepare graduates for work in the twenty-first century were identified. The research question guiding the review was: Which curricular and pedagogical arrangements promote engineering students’ employability? The particular focus of the study was on how authors prioritised engineering knowledge and professional skills. The review drew on a theoretical framework that differentiated between engineering knowledge and professional skills to explain how employability could be included in engineering programmes. Data was obtained from research studies over the period 2007–2017. We found an interdependent relationship between engineering knowledge and professional skills that enabled engineering graduates to attain employability. The com of engineering problems require students to master engineering knowledge, while the ability to work with others across contexts requires professional skills. Both are necessary for deep understanding of engineering principles and a focus on real world problems.",165 - 180,2018.0,https://www.thomas-gardner.com/listhomepage.html,Psychology
1293,5f06b4590a179a529182cd35f65773bb261f9b98,Recombinant Service Systems Engineering,,377-391,2018.0,http://www.mcclain-kennedy.com/exploreregister.asp,Computer Science
1294,bc4949c27b55a5644e6d68c8e5970f8a6a1d5436,Support vector machines in engineering: an overview,"This paper provides an overview of the support vector machine (SVM) methodology and its applicability to real‐world engineering problems. Specifically, the aim of this study is to review the current state of the SVM technique, and to show some of its latest successful results in real‐world problems present in different engineering fields. The paper starts by reviewing the main basic concepts of SVMs and kernel methods. Kernel theory, SVMs, support vector regression (SVR), and SVM in signal processing and hybridization of SVMs with meta‐heuristics are fully described in the first part of this paper. The adoption of SVMs in engineering is nowadays a fact. As we illustrate in this paper, SVMs can handle high‐dimensional, heterogeneous and scarcely labeled datasets very efficiently, and it can be also successfully tailored to particular applications. The second part of this review is devoted to different case studies in engineering problems, where the application of the SVM methodology has led to excellent results. First, we discuss the application of SVR algorithms in two renewable energy problems: the wind speed prediction from measurements in neighbor stations and the wind speed reconstruction using synoptic‐pressure data. The application of SVMs in noninvasive cardiac indices estimation is described next, and results obtained there are presented. The application of SVMs in problems of functional magnetic resonance imaging (fMRI) data processing is further discussed in the paper: brain decoding and mental disorder characterization. The following application deals with antenna array processing, namely SVMs for spatial nonlinear beamforming, and the SVM application in a problem of arrival angle detection. Finally, the application of SVMs to remote sensing image classification and target detection problems closes this review. WIREs Data Mining Knowl Discov 2014, 4:234–267. doi: 10.1002/widm.1125",24-113,2014.0,https://www.guerrero-fry.com/wp-contentindex.php,Computer Science
1295,0353cc70cbb1da71e921137463ca0f02094d67c6,Virtual and augmented reality game-based applications to civil engineering education,"Gaming scenarios and virtual environments have shown beneficial results in Engineering Education. Various activities conducted in different fields demonstrate that students reveal appraisal for the integration of innovative technologies such as Virtual or Augmented Reality in the learning process. In this paper, Virtual Reality (VR) applications developed by first year students during an introductory class of the Integrated Masters in Civil Engineering are described. Additionally, two trials concerning the application of VR and Augmented Reality (AR) to Civil Engineering held at a local school (K-12 students) are also detailed. After the tests, students were surveyed and data was collected. Results and considerations are revealed in the final sections of this paper.",1683-1688,2017.0,https://daniels.com/mainmain.html,Engineering
1296,b6e9921477cf2e8ad95e5362654246245f082ab3,Multisensor Data Fusion,,1-3,1997.0,https://www.rodriguez-kramer.com/posts/wp-content/postslogin.jsp,Computer Science
1297,536c08a5dbcbf75c14541fb15cbde4db8385ec7e,"Flexible Sketches and Inflexible Data Bases: Visual Communication, Conscription Devices, and Boundary Objects in Design Engineering","Engineering sketches and drawings are the building blocks of technological design and production. These visual representations act as the means for organizing the design to production process, hence serving as a ""social glue"" both between individuals and between groups. The author discusses two main capacities such visual representations serve in facilitating distributed cognition in team design work As conscription devices, they enlist and organize group participation. As boundary objects, they facilitate the reading of alternative meanings by various groups involved in the design process. The introduction of computer-aided design into this visual culture of engineering restructures relationships between workers in ways that can hamper the flexibility necessary for these crucial capacities to take place. The data are drawn from a study of the daily practices of engineers engaged in redesigning a turbine engine package. The method is participant observation.",448 - 473,1991.0,https://www.burton-gaines.biz/main/searchprivacy.php,Engineering
1298,9d0cf900881c995d07f3967d7c81d2950d2541ea,"Proceedings of the First International Conference on Advanced Data and Information Engineering, DaEng 2013, Kuala Lumpur, Malaysia, December 16-18, 2013",,41-124,2013.0,http://www.burgess.com/app/posts/exploreprivacy.html,Computer Science
1299,878bcf59cb8efc0bb8341dff78cdb80d7ff076da,Data quality in empirical software engineering: a targeted review,"Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.",27-146,2013.0,https://brown.info/wp-content/tagslogin.htm,Computer Science
1300,b3e44942580a6254706804ef95c22fe9ac13f92e,Traffic engineering with forward fault correction,"Faults such as link failures and high switch configuration delays can cause heavy congestion and packet loss. Because it takes time to detect and react to faults, these conditions can last long---even tens of seconds. We propose forward fault correction (FFC), a proactive approach to handling faults. FFC spreads network traffic such that freedom from congestion is guaranteed under arbitrary combinations of up to k faults. We show how FFC can be practically realized by compactly encoding the constraints that arise from this large number of possible faults and solving them efficiently using sorting networks. Experiments with data from real networks show that, with negligible loss in overall network throughput, FFC can reduce data loss by a factor of 7--130 in well-provisioned networks, and reduce the loss of high-priority traffic to almost zero in well-utilized networks.",527 - 538,2014.0,https://www.little.net/list/searchhomepage.jsp,Computer Science
1301,2ad27a03e7c120c3d1fe31368a7a897960afd8cb,Data Quality: Some Comments on the NASA Software Defect Datasets,"Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",1208-1215,2013.0,https://www.andrews.com/categoryhomepage.html,Computer Science
1302,7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9,Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription,"We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third—from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%—using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",24-29,2011.0,https://www.bell.com/postsfaq.jsp,Computer Science
1303,6ef57ad9b72635e53f07dad114f8e231d02155cf,Managing Big Data for Scientific Visualization,"Many areas of endeavor have problems with big data. Some classical business applications have faced big data for some time (e.g. airline reservation systems), and newer business applications to exploit big data are under construction (e.g. data warehouses, federations of databases). While engineering and scientific visualization have also faced the problem for some time, solutions are less well developed, and common techniques are less well understood. In this section we offer some structure to understand what has been done to manage big data for engineering and scientific visualization, and to understand and go forward in areas that may prove fruitful. With this structure as backdrop, we discuss the work that has been done in management of big data, as well as our own work on demand-paged segments for fluid flow visualization.",50-117,2015.0,https://diaz-mcintosh.info/tags/blog/categoriesauthor.php,Computer Science
1304,ab2ec1a1b6fdb710b2782554086a4782d50fc2c8,Recommended Steps for Thematic Synthesis in Software Engineering,"Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.",275-284,2011.0,http://www.martinez.com/list/appprivacy.html,Computer Science
1305,e1946e597b27cd4526ee71800f6454b8dcb5d4d1,Fuzzy mathematical models in engineering and management science,"Theoretical Concepts. Fuzzy Set Theory and System Modelling. Theory of Fuzzy Sets. Theory of Fuzzy Numbers. Linear Ordering of Fuzzy Numbers. Evaluation of Imprecision in Fuzzy Numbers. Triangular Approximation of Various Functions of Triangular Fuzzy Numbers. Deconvolution of the Fuzzy Equations A(+)B=C, and A( . )B=C in R. T-Norms and T-Conorms. Fuzzy Numbers in [0,1]. Fuzzy Numbers in [0,1] with Higher-Order Intervals of Confidence in [0,1]. Models in Engineering and Management Science. Modelling Issues in Engineering and Management Science. Fuzzy Zero-Base Budgeting (F.Z.B.B.). Fuzzy Delphi Method (F.D.M.) in Forecasting and Decision Making. Discounting Problem using Fuzzy Numbers. Smoothing (Filtering) of Fuzzy Data. Reliability Modelling and Evaluation with Fuzzy Data. Ordering of Fuzzy Quotients. Critical Path Method (C.P.M.) with Fuzzy Data. Investment Problem with Fuzzy Data. Transportation Optimization with Fuzzy Data: (Fuzzy Stepping Stone Method). A General View about the Fuzzification of Models in Engineering and Management Science. Appendices.","I-XXIII, 1-338",1988.0,https://www.larson.com/wp-content/tags/wp-contentterms.jsp,Mathematics
1306,b1d8a4b5f0c1788ac9a431f9c50a174e3cfb9dd5,International Journal of Data Warehousing and Mining,"1 Elasticity in Cloud databases and Their Query Processing Goetz Graefe, Research in Business Intelligence, Hewlett-Packard Laboratories, Palo Alto, CA, USA Anisoara Nica, SQL Anywhere Research and Development, Sybase (An SAP Company), Waterloo, ON, Canada Knut Stolze, Information Management Department, IBM Germany Research & Development, Böblingen, Germany Thomas Neumann, Technische Universität München, Garching, Germany Todd Eavis, Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada Ilia Petrov, Data Management Lab, School of Informatics, Reutlingen University, Germany Elaheh Pourabbas, Institute of Systems Analysis and Computer Science “Antonio Ruberti”, National Research Council, Rome, Italy David Fekete, Department of Information Systems, Universität Münster, Münster, Germany",61-136,2014.0,http://www.campbell-barker.com/mainpost.html,Technology
1307,d1ca9accc8e1ca4411aa90e5355924b822bc14af,"Changing Engineering Education: Views of U.S. Faculty, Chairs, and Deans","Many reports present a vision of what engineering education should look like, but few describe how this should happen. An American Society for Engineering Education initiative in 2006 attempted to bridge this gap by engaging faculty, chairs, and deans in discussion of change in engineering education; results were reported in a Phase I report (2009). In a second phase, survey data were integrated into a Phase II report (2012).",36-134,2014.0,https://zamora.org/explorehomepage.html,Psychology
1308,2e3a9a4dbf4862757fee5a074592929910e7034d,Survey Guidelines in Software Engineering: An Annotated Review,"Background: Survey is a method of research aiming to gather data from a large population of interest. Despite being extensively used in software engineering, survey-based research faces several challenges, such as selecting a representative population sample and designing the data collection instruments. Objective: This article aims to summarize the existing guidelines, supporting instruments and recommendations on how to conduct and evaluate survey-based research. Methods: A systematic search using manual search and snowballing techniques were used to identify primary studies supporting survey research in software engineering. We used an annotated review to present the findings, describing the references of interest in the research topic. Results: The summary provides a description of 15 available articles addressing the survey methodology, based upon which we derived a set of recommendations on how to conduct survey research, and their impact in the community. Conclusion: Survey-based research in software engineering has its particular challenges, as illustrated by several articles in this review. The annotated review can contribute by raising awareness of such challenges and present the proper recommendations to overcome them.",38-102,2016.0,https://www.price.com/listabout.asp,Computer Science
1309,b58725d3935d36685e4002b6a522fdfbf9d8e2a1,Radar Data Processing With Applications,"• Presents both classical theory and development methods of radar data processing • Provides state-of-the-art research results, including data processing for modern style radars, and tracking performance evaluation theory • Includes coverage of performance evaluation, registration algorithm for Radar network, data processing of passive radar, pulse Doppler radar, and phased array radar • Has applications for those engaged in information engineering, radar engineering, electronic countermeasures, infrared techniques, sonar techniques, and military command",25-143,2016.0,https://www.bass.net/list/exploreabout.html,Computer Science
1310,eed2d7c523eaa5c3758ac2ff8697d7fc8173e926,The Uses of Big Data in Cities,"There is much enthusiasm currently about the possibilities created by new and more extensive sources of data to better understand and manage cities. Here, I explore how big data can be useful in urban planning by formalizing the planning process as a general computational problem. I show that, under general conditions, new sources of data coordinated with urban policy can be applied following fundamental principles of engineering to achieve new solutions to important age-old urban problems. I also show that comprehensive urban planning is computationally intractable (i.e., practically impossible) in large cities, regardless of the amounts of data available. This dilemma between the need for planning and coordination and its impossibility in detail is resolved by the recognition that cities are first and foremost self-organizing social networks embedded in space and enabled by urban infrastructure and services. As such, the primary role of big data in cities is to facilitate information flows and mechanisms of learning and coordination by heterogeneous individuals. However, processes of self-organization in cities, as well as of service improvement and expansion, must rely on general principles that enforce necessary conditions for cities to operate and evolve. Such ideas are the core of a developing scientific theory of cities, which is itself enabled by the growing availability of quantitative data on thousands of cities worldwide, across different geographies and levels of development. These three uses of data and information technologies in cities constitute then the necessary pillars for more successful urban policy and management that encourages, and does not stifle, the fundamental role of cities as engines of development and innovation in human societies.","
          12-22
        ",2014.0,http://www.wright.net/categoryprivacy.htm,Engineering
1311,a21921eb0c5600562c8dad8e2bc40fff1ec8906b,Automatic Reverse Engineering of Data Structures from Binary Execution,"With only the binary executable of a program, it is useful to discover the program's data structures and infer their syntactic and semantic definitions. Such knowledge is highly valuable in a variety of security and forensic applications. Although there exist efforts in program data structure inference, the existing solutions are not suitable for our targeted application scenarios. In this paper, we propose a reverse engineering technique to automatically reveal program data structures from binaries. Our technique, called REWARDS, is based on dynamic analysis. More specifically, each memory location accessed by the program is tagged with a timestamped type attribute. Following the program's runtime data flow, this attribute is propagated to other memory locations and registers that share the same type. During the propagation, a variable's type gets resolved if it is involved in a type-revealing execution point or type sink. More importantly, besides the forward type propagation, REWARDS involves a backward type resolution procedure where the types of some previously accessed variables get recursively resolved starting from a type sink. This procedure is constrained by the timestamps of relevant memory locations to disambiguate variables re-using the same memory location. In addition, REWARDS is able to reconstruct in-memory data structure layout based on the type information derived. We demonstrate that REWARDS provides unique benefits to two applications: memory image forensics and binary fuzzing for vulnerability discovery.",21-107,2010.0,http://sanchez.net/list/category/tagprivacy.html,Computer Science
1312,72c7dce52ef232122cc62535f7bc81804fa3b417,Factors of Safety and Reliability in Geotechnical Engineering,"Simple reliability analyses, involving neither complex theory nor unfamiliar terms, can be used in routine geotechnical engineering practice. These simple reliability analyses require little effort beyond that involved in conventional geotechnical analyses. They provide a means of evaluating the combined effects of uncertainties in the parameters involved in the calculations, and they offer a useful supplement to conventional analyses. The additional parameters needed for the reliability analyses—standard deviations of the parameters—can be evaluated using the same amount of data and types of correlations that are widely used in geotechnical engineering practice. Example applications to stability and settlement problems illustrate the simplicity and practical usefulness of the method.",307-316,2000.0,https://www.lawrence.com/main/wp-contentprivacy.php,Engineering
1313,b419355d277d2dbf3d66b1c54b6db60a4251df03,Probability Concepts in Engineering: Emphasis on Applications to Civil and Environmental Engineering,Chapter 1 - Role of Probability and Statistics in Engineering Chapter 2 -- Fundamentals of Probability Models Chapter 3 -- Analytical Models of Random Phenomena Chapter 4 -- Functions of Random Variables Chapter 5 - Computer-Based Numerical and Simulation Methods in Probability Chapter 6 -- Statistical Inferences from Observational Data Chapter 7 -- Determination of Probability Distribution Models Chapter 8 -- Regression and Correlation Analyses Chapter 9 -- The Bayesian Approach Chapter 10 - Elements of Quality Assurance and Acceptance Sampling (Available only online at the Wiley web site) Appendices: Table A.1 -- Standard Normal Probabilities Table A.2 - CDF of the Binomial Distribution Table A.3 - Critical Values of t Distribution at Confidence Level (1- a)=p Table A.4 - Critical Values of the c2 Distribution at Confidence Level (1-a)=pTable A.5 - Critical Values of Dna at Significance Level a in the K-S Test Table A.6 - Critical Values of the Anderson-Darling Goodness-of-fit Test (for 4 specific distributions),20-141,2006.0,https://www.fry-martinez.com/posts/bloghome.html,Computer Science
1314,c26fa4322ad54671e13917cfb8cca289a4ebafd9,Introduction to the Multi-Disciplinary Engineering for Cyber-Physical Production Systems,,1-24,2017.0,http://lopez.com/search/blogindex.htm,Engineering
1315,eb181f71960c675d6bf586b974bc77342b639fee,The knowledge graph as the default data model for learning on heterogeneous knowledge,"In modern machine learning, raw data is the pre-ferred input for our models. Where a decade ago data scien-tists were still engineering features, manually picking out the details they thought salient, they now prefer the data in their raw form. As long as we can assume that all relevant and ir-relevant information is present in the input data, we can de-sign deep models that build up intermediate representations to sift out relevant features. However, these models are often domain specific and tailored to the task at hand, and therefore unsuited for learning on heterogeneous knowledge: informa-tion of different types and from different domains. If we can develop methods that operate on this form of knowledge, we can dispense with a great deal of ad-hoc feature engineering and train deep models end-to-end in many more domains. To accomplish this, we first need a data model capable of ex-pressing heterogeneous knowledge naturally in various do-mains, in as usable a form as possible, and satisfying as many use cases as possible. In this position paper, we argue that the knowledge graph is a suitable candidate for this data model. This paper describes current research and discusses some of the promises and challenges of this approach.",39-57,2017.0,http://murphy.org/search/wp-contentfaq.jsp,Computer Science
1316,1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6,The Pushshift Reddit Dataset,"Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.Reddit, the so called “front page of the Internet,” in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically.In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.",42-126,2020.0,https://www.glover-wolf.com/wp-content/appterms.asp,Computer Science
1317,445ccb9c6bf47fdf71bc9f14ee1b775b08096da8,Some applications of statistical methods to the analysis of physical and engineering data,"Whenever we measure any physical quantity we customarily obtain as many different values as there are observations. From a consideration of these measurements we must determine the most probable value; we must find out how much an observation may be expected to vary from this most probable value; and we must learn as much as possible of the reasons why it varies in the particular way that it does. In other words, the real value of physical measurements lies in the fact that from them it is possible to determine something of the nature of the results to be expected if the series of observations is repeated. The best use can be made of the data if we can find from them the most probable frequency or occurrence of any observed magnitude of the physical quantity or, in other words, the most probable law of distribution. It is customary practice in connection with physical and engineering measurements to assume that the arithmetic mean of the observations is the most probable value and that the frequency of occurrence of deviations from this mean is in accord with the Gaussian or normal law of error which lies at the foundation of the theory of errors. In most of those cases where the observed distributions of deviations have been compared with the theoretical ones based on the assumption of this law, it has been found highly improbable that the groups of observations could have arisen from systems of causes consistent with the normal law. Furthermore, even upon an a priori basis the normal law is a very limited case of a more generalized one. Therefore, in order to find the probability of the occurrence of a deviation of a given magnitude, it is necessary in most instances to find the theoretical distribution which is more probable than that given by the normal law. The present paper deals with the application of elementary statistical methods for finding this best frequency distribution of the deviations. In other words, the present paper points out some of the limitations of the theory of errors, based upon the normal law, in the analysis of physical and engineering data; it suggests methods for overcoming these difficulties by basing the analysis upon a more generalized law of error; it reviews the methods for finding the best theoretical distribution and closes with a discussion of the magnitude of the advantages to be gained by either the physicist or the engineer from an application of the methods reviewed herein.",43-87,,https://bowers.biz/categories/main/listprivacy.html,Mathematics
1318,49149bd7c772b77937f459f56e5bf01c1ac4675f,"The advantages of an Ontology-Based Data Management approach: openness, interoperability and data quality",,441 - 455,2016.0,https://www.scott.biz/mainterms.php,Computer Science
1319,c4518592ff763d6746a197c2c5f3df2c4044d13d,Guide to the Software Engineering Body of Knowledge,data types Sorting and searching parallel and distributed algorithms 3. [AR] Computer Architecture,88-112,1998.0,https://www.schmidt-robinson.com/mainauthor.html,Engineering
1320,922e773e471430f637d07af4f17b72461ede8b7c,Reusing Scientific Data: How Earthquake Engineering Researchers Assess the Reusability of Colleagues’ Data,,355-375,2010.0,http://www.brown.com/search/tagsearch.html,Computer Science
1321,9759ed3befc96caa5035b7176671efea99cd3493,A Brief Review on Leading Big Data Models,"Today, science is passing through an era of transformation, where the inundation of data, dubbed data deluge is influencing the decision making process. The science is driven by the data and is being termed as data science. In this internet age, the volume of the data has grown up to petabytes, and this large, complex, structured or unstructured, and heterogeneous data in the form of “Big Data” has gained significant attention. The rapid pace of data growth through various disparate sources, especially social media such as Facebook, has seriously challenged the data analytic capabilities of traditional relational databases. The velocity of the expansion of the amount of data gives rise to a complete paradigm shift in how new age data is processed. Confidence in the data engineering of the existing data processing systems is gradually fading whereas the capabilities of the new techniques for capturing, storing, visualizing, and analyzing data are evolving. In this review paper, we discuss some of the modern Big Data models that are leading contributors in the NoSQL era and claim to address Big Data challenges in reliable and efficient ways. Also, we take the potential of Big Data into consideration and try to reshape the original operationaloriented definition of “Big Science” (Furner, 2003) into a new data-driven definition and rephrase it as “The science that deals with Big Data is Big Science.”",138-157,2014.0,https://fox.com/list/tag/tagsindex.asp,Computer Science
1322,328251a1caef158f32773e0f77ce484849baf4b9,"Probability, Reliability and Statistical Methods in Engineering Design (Haldar, Mahadevan)",Basic Concept of Reliability. Mathematics of Probability. Modeling of Uncertainty. Commonly Used Probability Distributions. Determination of Distributions and Parameters from Observed Data. Randomness in Response Variables. Fundamentals of Reliability Analysis. Advanced Topics on Reliability Analysis. Simulation Techniques. Appendices. Conversion Factors. References. Index.,68-122,1999.0,http://www.estrada.com/posts/blog/explorefaq.php,Computer Science
1323,f33aee10eb09666fdca5c0516143b1fbcf85bf22,The changing landscape of requirements engineering practices over the past decade,"Even though there is ample information available on solid requirements engineering practices, anecdotal evidence still indicates poor practices in industry. The key issue in implementing an improvement is to first identify the areas that need most improvement. Three surveys were conducted in 2003, 2008 and 2013 on the state of practice of requirements engineering. Surveys data obtained includes characteristics of projects, practices, organizations, and practitioners related to requirements engineering. In this paper we present a comparison and analysis of the responses from the three surveys in order to understand the changing land-scape of requirements engineering industrial practices over the past years.",1-8,2015.0,https://gardner.com/tagcategory.jsp,Computer Science
1324,66f48bc391b0fd135c8b4915618aebf663d56316,Query reverse engineering,,721-746,2014.0,http://richards.com/explore/list/categoryprivacy.php,Computer Science
1325,bc1022b031dc6c7019696492e8116598097a8c12,Natural Language Processing (Almost) from Scratch,"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",66-108,2011.0,http://gilbert-morgan.org/tag/posts/searchpost.php,Computer Science
1326,de7a529d144f0a78dd75864dd6012f969521f7f8,From Theory to Practice: Plug and Play with Succinct Data Structures,,59-128,2013.0,http://peterson.org/tagsregister.php,Computer Science
1327,31ebee998db7125228d6119200594dfe2a6dbc3c,On the Convergence of Data and Process Engineering,,19-26,2011.0,https://www.lam.com/main/categoriesfaq.html,Computer Science
1328,024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb,Array programming with NumPy,,357 - 362,2020.0,http://www.gonzalez.com/explore/explore/postsmain.html,Computer Science
1329,5a30fd3718835c500d1833492d6cd833c959d155,Non-Functional Requirements in Software Engineering,,1-441,2000.0,https://cole.com/tag/categoryfaq.php,Computer Science
1330,2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb,Deep Learning,"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.",15-104,2015.0,https://www.reed.info/tag/postsfaq.htm,Technology
1331,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",97-144,2013.0,https://www.mitchell.com/search/listabout.asp,Computer Science
1332,e28d1aebca9fcd36d0a23b494b9dfa2ef8795277,System Identification: A Frequency Domain Approach,"System identification is a general term used to describe mathematical tools and algorithms that build dynamical models from measured data. Used for prediction, control, physical interpretation, and the designing of any electrical systems, they are vital in the fields of electrical, mechanical, civil, and chemical engineering. Focusing mainly on frequency domain techniques, System Identification: A Frequency Domain Approach, Second Edition also studies in detail the similarities and differences with the classical time domain approach. It high??lights many of the important steps in the identification process, points out the possible pitfalls to the reader, and illustrates the powerful tools that are available.",30-129,2012.0,http://www.conner-hoffman.com/search/categoriessearch.html,Computer Science
1333,c27e16cf9b626e983ad6372a25e98e060053afcb,Wisdom of crowds for robust gene network inference,,796 - 804,2012.0,https://moss.com/exploreterms.html,Medicine
1334,07993804501ae4df9fe43cf8afb009ae38fe902e,A Mathematical Introduction to Compressive Sensing,,"I-XVIII, 1-625",2013.0,http://ellis-rich.biz/blog/blog/wp-contentmain.html,Computer Science
1335,eceff752c3f87c9120ecab4fe63a960bfe330b9a,"Reveal, a general reverse engineering algorithm for inference of genetic network architectures.","Given the immanent gene expression mapping covering whole genomes during development, health and disease, we seek computational methods to maximize functional inference from such large data sets. Is it possible, in principle, to completely infer a complex regulatory network architecture from input/output patterns of its variables? We investigated this possibility using binary models of genetic networks. Trajectories, or state transition tables of Boolean nets, resemble time series of gene expression. By systematically analyzing the mutual information between input states and output states, one is able to infer the sets of input elements controlling each element or gene in the network. This process is unequivocal and exact for complete state transition tables. We implemented this REVerse Engineering ALgorithm (REVEAL) in a C program, and found the problem to be tractable within the conditions tested so far. For n = 50 (elements) and k = 3 (inputs per element), the analysis of incomplete state transition tables (100 state transition pairs out of a possible 10(15)) reliably produced the original rule and wiring sets. While this study is limited to synchronous Boolean networks, the algorithm is generalizable to include multi-state models, essentially allowing direct application to realistic biological data sets. The ability to adequately solve the inverse problem may enable in-depth analysis of complex dynamic systems in biology and other fields.","
          18-29
        ",1998.0,http://www.marshall-jackson.com/exploremain.html,Medicine
1336,dca45bd363820bce269a176a1ecde7e1885e2ea6,B4: experience with a globally-deployed software defined wan,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",44-138,2013.0,http://lane.com/categories/mainterms.html,Computer Science
1337,bc29d088920f171a9013cb7e936cde34f3ba884f,Measuring the efficiency of decision making units,"A nonlinear (nonconvex) programming model provides a new definition of efficiency for use in evaluating activities of not-for-profit entities participating in public programs. A scalar measure of the efficiency of each participating unit is thereby provided, along with methods for objectively determining weights by reference to the observational data for the multiple outputs and multiple inputs that characterize such programs. Equivalences are established to ordinary linear programming models for effecting computations. The duals to these linear programming models provide a new way for estimating extremal relations from observational data. Connections between engineering and economic approaches to efficiency are delineated along with new interpretations and ways of using them in evaluating and controlling managerial behavior in public programs.",38-146,2003.0,https://contreras-guerrero.com/search/category/categoriespost.html,Technology
1338,85ed1f128c65f00e7ab3c41f3af904e0fd3dae2f,Reliability and Statistics in Geotechnical Engineering,"Preface. Part I. 1 Introduction - uncertainty and risk in geotechnical engineering. 1.1 Offshore platforms. 1.2 Pit mine slopes. 1.3 Balancing risk and reliability in a geotechnical design. 1.4 Historical development of reliability methods in civil engineering. 1.5 Some terminological and philosophical issues. 1.6 The organization of this book. 1.7 A comment on notation and nomenclature. 2 Uncertainty. 2.1 Randomness, uncertainty, and the world. 2.2 Modeling uncertainties in risk and reliability analysis. 2.3 Probability. 3 Probability. 3.1 Histograms and frequency diagrams. 3.2 Summary statistics. 3.3 Probability theory. 3.4 Random variables. 3.5 Random process models. 3.6 Fitting mathematical pdf models to data. 3.7 Covariance among variables. 4 Inference. 4.1 Frequentist theory. 4.2 Bayesian theory. 4.3 Prior probabilities. 4.4 Inferences from sampling. 4.5 Regression analysis. 4.6 Hypothesis tests. 4.7 Choice among models. 5 Risk, decisions and judgment. 5.1 Risk. 5.2 Optimizing decisions. 5.3 Non-optimizing decisions. 5.4 Engineering judgment. Part II. 6 Site characterization. 6.1 Developments in site characterization. 6.2 Analytical approaches to site characterization. 6.3 Modeling site characterization activities. 6.4 Some pitfalls of intuitive data evaluation. 6.5 Organization of Part II. 7 Classification and mapping. 7.1 Mapping discrete variables. 7.2 Classification. 7.3 Discriminant analysis. 7.4 Mapping. 7.5 Carrying out a discriminant or logistic analysis. 8 Soil variability. 8.1 Soil properties. 8.2 Index tests and classification of soils. 8.3 Consolidation properties. 8.4 Permeability. 8.5 Strength properties. 8.6 Distributional properties. 8.7 Measurement error. 9 Spatial variability within homogeneous deposits. 9.1 Trends and variations about trends. 9.2 Residual variations. 9.3 Estimating autocorrelation and autocovariance. 9.4 Variograms and geostatistics. Appendix: algorithm for maximizing log-likelihood of autocovariance. 10 Random field theory. 10.1 Stationary processes. 10.2 Mathematical properties of autocovariance functions. 10.3 Multivariate (vector) random fields. 10.4 Gaussian random fields. 10.5 Functions of random fields. 11 Spatial sampling. 11.1 Concepts of sampling. 11.2 Common spatial sampling plans. 11.3 Interpolating random fields. 11.4 Sampling for autocorrelation. 12 Search theory. 12.1 Brief history of search theory. 12.2 Logic of a search process. 12.3 Single stage search. 12.4 Grid search. 12.5 Inferring target characteristics. 12.6 Optimal search. 12.7 Sequential search. Part III. 13 Reliability analysis and error propagation. 13.1 Loads, resistances and reliability. 13.2 Results for different distributions of the performance function. 13.3 Steps and approximations in reliability analysis. 13.4 Error propagation - statistical moments of the performance function. 13.5 Solution techniques for practical cases. 13.6 A simple conceptual model of practical significance. 14 First order second moment (FOSM) methods. 14.1 The James Bay dikes. 14.2 Uncertainty in geotechnical parameters. 14.3 FOSM calculations. 14.4 Extrapolations and consequences. 14.5 Conclusions from the James Bay study. 14.6 Final comments. 15 Point estimate methods. 15.1 Mathematical background. 15.2 Rosenblueth's cases and notation. 15.3 Numerical results for simple cases. 15.4 Relation to orthogonal polynomial quadrature. 15.5 Relation with 'Gauss points' in the finite element method. 15.6 Limitations of orthogonal polynomial quadrature. 15.7 Accuracy, or when to use the point-estimate method. 15.8 The problem of the number of computation points. 15.9 Final comments and conclusions. 16 The Hasofer-Lind approach (FORM). 16.1 Justification for improvement - vertical cut in cohesive soil. 16.2 The Hasofer-Lind formulation. 16.3 Linear or non-linear failure criteria and uncorrelated variables. 16.4 Higher order reliability. 16.5 Correlated variables. 16.6 Non-normal variables. 17 Monte Carlo simulation methods. 17.1 Basic considerations. 17.2 Computer programming considerations. 17.3 Simulation of random processes. 17.4 Variance reduction methods. 17.5 Summary. 18 Load and resistance factor design. 18.1 Limit state design and code development. 18.2 Load and resistance factor design. 18.3 Foundation design based on LRFD. 18.4 Concluding remarks. 19 Stochastic finite elements. 19.1 Elementary finite element issues. 19.2 Correlated properties. 19.3 Explicit formulation. 19.4 Monte Carlo study of differential settlement. 19.5 Summary and conclusions. Part IV. 20 Event tree analysis. 20.1 Systems failure. 20.2 Influence diagrams. 20.3 Constructing event trees. 20.4 Branch probabilities. 20.5 Levee example revisited. 21 Expert opinion. 21.1 Expert opinion in geotechnical practice. 21.2 How do people estimate subjective probabilities? 21.3 How well do people estimate subjective probabilities? 21.4 Can people learn to be well-calibrated? 21.5 Protocol for assessing subjective probabilities. 21.6 Conducting a process to elicit quantified judgment. 21.7 Practical suggestions and techniques. 21.8 Summary. 22 System reliability assessment. 22.1 Concepts of system reliability. 22.2 Dependencies among component failures. 22.3 Event tree representations. 22.4 Fault tree representations. 22.5 Simulation approach to system reliability. 22.6 Combined approaches. 22.7 Summary. Appendix A: A primer on probability theory. A.1 Notation and axioms. A.2 Elementary results. A.3 Total probability and Bayes' theorem. A.4 Discrete distributions. A.5 Continuous distributions. A.6 Multiple variables. A.7 Functions of random variables. References. Index.",62-145,2003.0,https://page.com/blog/bloghome.php,Mathematics
1339,fa9f622a1182400067d911b0900733695ec1b358,Parameter tuning or default values? An empirical investigation in search-based software engineering,,594-623,2013.0,https://www.grimes.com/posts/tagcategory.jsp,Computer Science
1340,c1ad6123d168525cacf262f2fd69f88f689381b4,An Introduction to Geotechnical Engineering,"This manual presents data on soil behaviour, with emphasis on practical and empirical knowledge, required by geotechnical engineers for the design and construction of foundations and embankments. It deals with: index and classification properties of soils; soil classification; clay minerals and soil structure; compaction; water in soils (capillarity, shrinkage, swelling, frost action, permeability, seepage, effective stress); consolidation and consolidation settlements; time rate of consolidation; the Mohr circle, failure theories, and stress paths; shear strength of sands and clays. Four appendices deal with the following: application of the ""SI"" system of units to getechnical engineering; derivation of Laplace's equation; derivation and solution of Terzaghi's one-dimensional consolidation theory; pore pressure parameters. (TRRL)",21-118,1981.0,https://hurst.info/categoriesindex.html,Geology
1341,abd3c6854f05cef54d1fbe74ead145e77619392f,Professional Role Confidence and Gendered Persistence in Engineering,"Social psychological research on gendered persistence in science, technology, engineering, and mathematics (STEM) professions is dominated by two explanations: women leave because they perceive their family plans to be at odds with demands of STEM careers, and women leave due to low self-assessment of their skills in STEM’s intellectual tasks, net of their performance. This study uses original panel data to examine behavioral and intentional persistence among students who enter an engineering major in college. Surprisingly, family plans do not contribute to women’s attrition during college but are negatively associated with men’s intentions to pursue an engineering career. Additionally, math self-assessment does not predict behavioral or intentional persistence once students enroll in a STEM major. This study introduces professional role confidence—individuals’ confidence in their ability to successfully fulfill the roles, competencies, and identity features of a profession—and argues that women’s lack of this confidence, compared to men, reduces their likelihood of remaining in engineering majors and careers. We find that professional role confidence predicts behavioral and intentional persistence, and that women’s relative lack of this confidence contributes to their attrition.",641 - 666,2011.0,https://www.adams.biz/main/categoriesregister.php,Psychology
1342,221da285b72f6272c2d7332a547d1e034a60e154,Data modelling versus ontology engineering,"Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative independence of particular applications, i.e. an ontology consists of relatively generic knowledge that can be reused by different kinds of applications/tasks. The first part of this paper concerns some aspects that help to understand the differences and similarities between ontologies and data models. In the second part we present an ontology engineering framework that supports and favours the genericity of an ontology. We introduce the DOGMA ontology engineering approach that separates ""atomic"" conceptual relations from ""predicative"" domain rules. A DOGMA ontology consists of an ontology base that holds sets of intuitive context-specific conceptual relations and a layer of ""relatively generic"" ontological commitments that hold the domain rules. This constitutes what we shall call the double articulation of a DOGMA ontology 1.",12-17,2002.0,https://olson-hill.biz/list/app/apppost.htm,Computer Science
1343,497e4b08279d69513e4d2313a7fd9a55dfb73273,LightGBM: A Highly Efficient Gradient Boosting Decision Tree,"Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \emph{Gradient-based One-Side Sampling} (GOSS) and \emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.",3146-3154,2017.0,http://abbott.com/tag/searchfaq.html,Computer Science
1344,3efd851140aa28e95221b55fcc5659eea97b172d,The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",61-80,2009.0,https://www.gomez-nichols.info/listhomepage.html,Computer Science
1345,bd07ddc6111f2b3ee9bd1f6529ee9b9b32f7f82b,Engineering Design Processes: A Comparison of Students and Expert Practitioners,"In this paper we report on an in‐depth study of engineering design processes. Specifically, we extend our previous research on engineering student design processes to compare the design behavior of students and expert engineers. Nineteen experts from a variety of engineering disciplines and industries each designed a playground in a lab setting, and gave verbal reports of their thoughts during the design task. Measures of their design processes and solution quality were compared to pre‐existing data from 26 freshmen and 24 seniors. The experts spent significantly more time on the task overall and in each stage of engineering design, including significantly more time problem scoping. The experts also gathered significantly more information covering more categories. Results support the argument that problem scoping and information gathering are major differences between advanced engineers and students, and important competencies for engineering students to develop. Timeline representations of the expert designers' processes illustrate characteristic distinctions we found and may help students gain insights into their own design processes.",44-122,2007.0,https://adkins.info/explorehomepage.asp,Engineering
1346,daa63f57c3fbe994c4356f8d986a22e696e776d2,Efficient Global Optimization of Expensive Black-Box Functions,,455-492,1998.0,https://meza-davis.com/posts/exploreindex.php,Mathematics
1347,68d54f9dacbb5416c1aafb3399c072497c320021,"Network Flows: Theory, Algorithms, and Applications","A comprehensive introduction to network flows that brings together the classic and the contemporary aspects of the field, and provides an integrative view of theory, algorithms, and applications. presents in-depth, self-contained treatments of shortest path, maximum flow, and minimum cost flow problems, including descriptions of polynomial-time algorithms for these core models. emphasizes powerful algorithmic strategies and analysis tools such as data scaling, geometric improvement arguments, and potential function arguments. provides an easy-to-understand descriptions of several important data structures, including d-heaps, Fibonacci heaps, and dynamic trees. devotes a special chapter to conducting empirical testing of algorithms. features over 150 applications of network flows to a variety of engineering, management, and scientific domains. contains extensive reference notes and illustrations.",50-121,1993.0,http://www.duncan.biz/posts/appfaq.htm,Computer Science
1348,5e3eb22c476b889eecbb380d012231d819edf156,Introduction to Fourier optics,"The second edition of this respected text considerably expands the original and reflects the tremendous advances made in the discipline since 1968. All material has been thoroughly updated and several new sections explore recent progress in important areas, such as wavelength modulation, analog information processing, and holography. Fourier analysis is a ubiquitous tool with applications in diverse areas of physics and engineering. This book explores these applications in the field of optics with a special emphasis on applications to diffraction, imaging, optical data processing, and holography. This book can be used as a textbook to satisfy the needs of several different types of courses, and it is directed toward both engineers ad physicists. By varying the emphasis on different topics and specific applications, the book can be used successfully in a wide range of basic Fourier Optics or Optical Signal Processing courses.",41-108,1969.0,http://www.edwards.net/postssearch.php,Physics
1349,69cfb48c45b59243d60342b796dbac35e9efd6bc,Toward principles for the design of ontologies used for knowledge sharing?,"Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.",907-928,1995.0,http://norris-klein.com/tag/category/searchindex.jsp,Computer Science
1350,801adcdeaed1c965fdb542ec4dc83d9ccdb71fdf,Design Experiments in Educational Research,"In this article, the authors first indicate the range of purposes and the variety of settings in which design experiments have been conducted and then delineate five crosscutting features that collectively differentiate design experiments from other methodologies. Design experiments have both a pragmatic bent—“engineering” particular forms of learning—and a theoretical orientation—developing domain-specific theories by systematically studying those forms of learning and the means of supporting them. The authors clarify what is involved in preparing for and carrying out a design experiment, and in conducting a retrospective analysis of the extensive, longitudinal data sets generated during an experiment. Logistical issues, issues of measure, the importance of working through the data systematically, and the need to be explicit about the criteria for making inferences are discussed.",13 - 9,2003.0,http://jones.biz/appsearch.html,Sociology
1351,b9bb6963c8291a9a3b697d30d8e8979c25c51f02,Classical and modern regression with applications,"The author emphasizes applications with examples that illustrate nearly all the techniques discussed. Applications have been selected from physical sciences, engineering, biology, management science and economics. Emphasis is also placed on concepts with a blend between illustrations using real data sets and mathematical and conceptual development. Expanded coverage includes: simultaneous influence, maximum likelihood estimation of parameters, and the plotting of residuals, the use of the general linear hypothesis, indicator variables, the geometry of least squares, the relationship to ANOVA models, Box-Cox transformation with illustrations, categorical response, other nonnormal error situations, autocorrelated errors and logistic regression.",37-138,1986.0,https://oliver.com/blog/wp-contentprivacy.jsp,Technology
1352,8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER.",21-136,2016.0,https://medina.biz/listabout.php,Computer Science
1353,1d122a074c936fcfd95faf44608e377a9d1799c8,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",33-117,2017.0,https://vazquez-martin.com/blogterms.asp,Computer Science
1354,617e2d3a42c23a758daf8dd51ab82f89d171ef54,The Mythical Man-Month,"The book, The Mythical Man-Month, Addison-Wesley, 1975 (excerpted in Datamation, December 1974), gathers some of the published data about software engineering and mixes it with the assertion of a lot of personal opinions. In this presentation, the author will list some of the assertions and invite dispute or support from the audience. This is intended as a public discussion of the published book, not a regular paper.",193,1975.0,http://www.gonzalez.com/searchindex.jsp,Computer Science
1355,b9e5fa707e804d6008e5011b058244437c656a93,Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9,,184 - 191,2015.0,https://davis-west.com/tags/category/taghome.html,Medicine
1356,be652e2b5986d15eb5b3c9d5fc5c8fbe108ed2ba,Business Value of Information Technology: A Study of Electronic Data Interchange,"A great deal of controversy exists about the impact of information technology on firm performance. While some authors have reported positive impacts, others have found negative or no impacts. This study focuses on Electronic Data Interchange (EDI) technology. Many of the problems in this line of research are over-come in this study by conducting a careful analysis of the performance data of the past decade gathered from the assembly centers of Chrysler Corporation. This study estimates the dollar benefits of improved information exchanges between Chrysler and its suppliers that result from using EDI. After controlling for variations in operational complexity arising from mix, volume, parts complexity, model, and engineering changes, the savings per vehicle that result from improved information exchanges are estimated to be about $60. Including the additional savings from electronic document preparation and transmission, the total benefits of EDI per vehicle amount to over $100. System wide, this translates to annual savings of $220 million for the company.",137-156,1995.0,https://www.simpson.com/tag/explore/tagindex.php,Computer Science
1357,49594776b8a1dce89e976c846266ccefafa948b7,Stochastic Processes,"Stochastic processes are probabilistic models of data streams such as speech, audio and video signals, stock market prices, and measurements of physical phenomena by digital sensors such as medical instruments, GPS receivers, or seismographs. A solid understanding of the mathematical basis of these models is essential for understanding phenomena and processing information in many branches of science and engineering including physics, communications, signal processing, automation, and structural dynamics.",40-141,2018.0,http://www.zamora.org/search/posts/postsmain.html,Technology
1358,f5079168f914014e9cfe888bdfd58b7bac2b1fe9,Implementing Smart Factory of Industrie 4.0: An Outlook,"With the application of Internet of Things and services to manufacturing, the fourth stage of industrialization, referred to as Industrie 4.0, is believed to be approaching. For Industrie 4.0 to come true, it is essential to implement the horizontal integration of inter-corporation value network, the end-to-end integration of engineering value chain, and the vertical integration of factory inside. In this paper, we focus on the vertical integration to implement flexible and reconfigurable smart factory. We first propose a brief framework that incorporates industrial wireless networks, cloud, and fixed or mobile terminals with smart artifacts such as machines, products, and conveyors. Then, we elaborate the operational mechanism from the perspective of control engineering, that is, the smart artifacts form a self-organized system which is assisted with the feedback and coordination blocks that are implemented on the cloud and based on the big data analytics. In addition, we outline the main technical features and beneficial outcomes and present a detailed design scheme. We conclude that the smart factory of Industrie 4.0 is achievable by extensively applying the existing enabling technologies while actively coping with the technical challenges.",63-101,2016.0,https://www.martinez.net/list/explore/mainauthor.html,Computer Science
1359,875c77b0daf65f2db77b48e784cb68fb312edea3,Sequential Monte Carlo Methods in Practice,,1-582,2001.0,https://mitchell.com/exploreprivacy.php,Computer Science
1360,0a2ed3e6848b922fd3557b09f4f6e398a89afb3d,Procedure for Estimation and Reporting of Uncertainty Due to Discretization in CFD Applications,"ince 1990, the Fluids Engineering Division of ASME has pursued activities concerning the detection, estimation and control of umerical uncertainty and/or error in computational fluid dynamics (CFD) studies. The first quality-control measures in this area were ssued in 1986 (1986, “Editorial Policy Statement on Control of Numerical Accuracy,” ASME J. Fluids Eng., 108, p. 2) and revised in 993 (1993, “Journal of Fluids Engineering Editorial Policy Statement on the Control of Numerical Accuracy,” ASME J. Fluids Eng., 15, pp. 339–340). Given the continued increase in CFD related publications, and the many significant advancements in computational echniques and computer technology, it has become necessary to revisit the issue and formulate a more detailed policy to further mprove the quality of publications in this area. This brief note provides specific guidelines for prospective authors for calculation and eporting of discretization error estimates in CFD simulations where experimental data may or may not be available for comparison. he underlying perspective is that CFD-related studies will eventually aim to predict the outcome of a physical event for which xperimental data is not available. It should be emphasized that the requirements outlined in this note do not preclude those already ublished in the previous two policy statements. It is also important to keep in mind that the procedure recommended in this note cannot ossibly encompass all possible scenarios or applications. DOI: 10.1115/1.2960953",100-129,2007.0,http://taylor-cook.org/app/tags/tagindex.htm,Technology
1361,371ed298dba16172a6e36920f41d4188a852b375,What is the Young's Modulus of Silicon?,"The Young's modulus (E) of a material is a key parameter for mechanical engineering design. Silicon, the most common single material used in microelectromechanical systems (MEMS), is an anisotropic crystalline material whose material properties depend on orientation relative to the crystal lattice. This fact means that the correct value of E for analyzing two different designs in silicon may differ by up to 45%. However, perhaps, because of the perceived complexity of the subject, many researchers oversimplify silicon elastic behavior and use inaccurate values for design and analysis. This paper presents the best known elasticity data for silicon, both in depth and in a summary form, so that it may be readily accessible to MEMS designers.",229-238,2010.0,http://www.chan.info/searchhomepage.php,Materials Science
1362,80fd8b366a25977d44a23efc75f20222b4e46ee9,"Adsorption by Powders and Porous Solids: Principles, Methodology and Applications","The declared objective of this book is to provide an introductory review of the various theoretical and practical aspects of adsorption by powders and porous solids with particular reference to materials of technological importance. The primary aim is to meet the needs of students and non-specialists, who are new to surface science or who wish to use the advanced techniques now available for the determination of surface area, pore size and surface characterization. In addition, a critical account is given of recent work on the adsorptive properties of activated carbons, oxides, clays and zeolites. Key Features * Provides a comprehensive treatment of adsorption at both the gas/solid interface and the liquid/solid interface * Includes chapters dealing with experimental methodology and the interpretation of adsorption data obtained with porous oxides, carbons and zeolites * Techniques capture the importance of heterogeneous catalysis, chemical engineering and the production of pigments, cements, agrochemicals, and pharmaceuticals",93-127,1998.0,http://french.com/list/blogsearch.html,Materials Science
1363,0d70f756c410370d56f9e61f25f68ab606533bf2,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",67-119,2018.0,https://cruz.com/explore/appterms.jsp,Computer Science
1364,498d27f4d3a616beb058333e410811808f7fe46d,Meshfree Approximation Methods with Matlab,"Meshfree approximation methods are a relatively new area of research, and there are only a few books covering it at present. Whereas other works focus almost entirely on theoretical aspects or applications in the engineering field, this book provides the salient theoretical results needed for a basic understanding of meshfree approximation methods. The emphasis here is on a hands-on approach that includes MATLAB routines for all basic operations. Meshfree approximation methods, such as radial basis function and moving least squares method, are discussed from a scattered data approximation and partial differential equations point of view. A good balance is supplied between the necessary theory and implementation in terms of many MATLAB programs, with examples and applications to illustrate key points. Used as class notes for graduate courses at Northwestern University, Illinois Institute of Technology, and Vanderbilt University, this book will appeal to both mathematics and engineering graduate students.",1-520,2007.0,http://www.arnold-clark.com/posts/app/tagscategory.asp,Computer Science
1365,b1119811137978ea7eed9241d696d2e139dd10f9,"Exploring Data in Engineering, the Sciences, and Medicine","1. The Art of Analyzing Data 2. Data: Types, Uncertainty and Quality 3. Characterizing Categorical Variables 4. Uncertainty in Real Variables 5. Fitting Straight Lines 6. A Brief Introduction to Estimation Theory 7. Outliers: Distributional Monsters That Lurk in Data 8. Characterizing a Dataset 9. Confidence Intervals and Hypothesis Testing 10. Associations between Variables 11. Regression Models I: Real Data 12. Re-expression: Data Transformations 13. Regression Models II: Mixed Data Types 14. Characterizing Analysis Results 15. Regression Models III: Diagnostics and Refinements 16. Dynamic Data Characterization 17. Linear Data Filters 18. Nonparametric Spectrum Estimation 19. Irregularities in Dynamic Analysis 20. Dealing with Missing Data",30-101,2011.0,http://smith-martin.org/category/exploreauthor.html,Computer Science
1366,2ba91b8e58fea3d6a5f1ab9748f0e976bd659e0d,USENIX Association,"SNMP is the standard protocol used to manage IP networks. Service providers often analyze the utilization statistics available from SNMP-enabled devices to make informed engineering decisions, diagnose faults and perform billing. However collecting and efficiently storing large amounts of time-series data quickly, without impacting network or device performance, is challenging in very large installations. We identify three crucial requirements for an SNMP statistical solution: (i) support for hundreds of devices each with thousands of objects; (ii) the ability to retain the data indefinitely; and (iii) an abstract interface to the data. We then compare the applicability of several tools in a service provider environment. Finally, we detail Real Traffic Grabber (RTG), an application currently in use on our national IP backbone which we developed in lieu of existing packages to meet our requirements.",40-127,1992.0,http://wheeler.biz/main/mainhome.jsp,Technology
1367,05dd3536ba7a8e017cf7b7cb56975a4cd6a53392,DICE: Quality-Driven Development of Data-Intensive Cloud Applications,"Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.",78-83,2015.0,https://www.robbins-richardson.com/tags/posts/blogterms.php,Computer Science
1368,78a47c832b16c78fadf29d0354e690f01ab90244,Bacillus thuringiensis and Its Pesticidal Crystal Proteins,"SUMMARY During the past decade the pesticidal bacterium Bacillus thuringiensis has been the subject of intensive research. These efforts have yielded considerable data about the complex relationships between the structure, mechanism of action, and genetics of the organism’s pesticidal crystal proteins, and a coherent picture of these relationships is beginning to emerge. Other studies have focused on the ecological role of the B. thuringiensis crystal proteins, their performance in agricultural and other natural settings, and the evolution of resistance mechanisms in target pests. Armed with this knowledge base and with the tools of modern biotechnology, researchers are now reporting promising results in engineering more-useful toxins and formulations, in creating transgenic plants that express pesticidal activity, and in constructing integrated management strategies to insure that these products are utilized with maximum efficiency and benefit.",775 - 806,1998.0,https://www.aguirre.com/list/categorieshome.html,Biology
1369,7c4a9643c701c0c91ea50fd587038f79187a0a5e,Artificial Intelligence: A Guide to Intelligent Systems,"From the Publisher: 
Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. 
 
The book provides an introduction to the field of computer intelligence, covering 
 
rule-based expert systems, 
fuzzy expert systems, 
frame-based expert systems, 
artificail neural networks, 
evolutionary computation, 
hybrid intelligent systems, 
knowledge engineering, 
data mining. 
 
 
In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit",93-128,2001.0,http://armstrong.com/app/explorehomepage.php,Computer Science
1370,2c5c6a334cc2f2a2143e9dbcac3ab5a43720292d,Damage identification and health monitoring of structural and mechanical systems from changes in their vibration characteristics: A literature review,"This report contains a review of the technical literature concerning the detection, location, and characterization of structural damage via techniques that examine changes in measured structural vibration response. The report first categorizes the methods according to required measured data and analysis technique. The analysis categories include changes in modal frequencies, changes in measured mode shapes (and their derivatives), and changes in measured flexibility coefficients. Methods that use property (stiffness, mass, damping) matrix updating, detection of nonlinear response, and damage detection via neural networks are also summarized. The applications of the various methods to different types of engineering problems are categorized by type of structure and are summarized. The types of structures include beams, trusses, plates, shells, bridges, offshore platforms, other large civil structures, aerospace structures, and composite structures. The report describes the development of the damage-identification methods and applications and summarizes the current state-of-the-art of the technology. The critical issues for future research in the area of damage identification are also discussed.",73-120,1996.0,https://petersen-gomez.com/categories/main/blogfaq.htm,Engineering
1371,a1b51ff5cfb974fdef34386bed5c5844ba7a8dcf,Managing the software process,Foreword. Preface. I. SOFTWARE PROCESS MATURITY. A Software Maturity Framework. The Principles of Software Process Change. Software Process Assessment. The Initial Process. II. THE REPEATABLE PROCESS. Managing Software Organizations. The Project Plan. Software Configuration Management-Part 1: Software Quality Assurance. III. THE DEFINED PROCESS. Software Standards. Software Inspections. Software Testing. Software Configuration Management (Continued). Defining the Software Process. The Software Engineering Process Group IV. THE MANAGED PROCESS. Data Gathering and Analysis. Managing Software Quality. V. THE OPTIMIZING PROCESS. Defect Prevention. Automating The Software Process. Contracting for Software. Conclusion. Appendices. Index. 0201180952T04062001,"I-XVIII, 1-494",1989.0,http://deleon.biz/explore/blog/categoryregister.html,Computer Science
1372,59a79116f9f7a20560da0aeb7519c33f6ec84a17,"Urban Computing: Concepts, Methodologies, and Applications","Urbanization's rapid progress has modernized many people's lives but also engendered big issues, such as traffic congestion, energy consumption, and pollution. Urban computing aims to tackle these issues by using the data that has been generated in cities (e.g., traffic flow, human mobility, and geographical data). Urban computing connects urban sensing, data management, data analytics, and service providing into a recurrent process for an unobtrusive and continuous improvement of people's lives, city operation systems, and the environment. Urban computing is an interdisciplinary field where computer sciences meet conventional city-related fields, like transportation, civil engineering, environment, economy, ecology, and sociology in the context of urban spaces. This article first introduces the concept of urban computing, discussing its general framework and key challenges from the perspective of computer sciences. Second, we classify the applications of urban computing into seven categories, consisting of urban planning, transportation, the environment, energy, social, economy, and public safety and security, presenting representative scenarios in each category. Third, we summarize the typical technologies that are needed in urban computing into four folds, which are about urban sensing, urban data management, knowledge fusion across heterogeneous data, and urban data visualization. Finally, we give an outlook on the future of urban computing, suggesting a few research topics that are somehow missing in the community.",38:1-38:55,2014.0,https://www.hunt-rodriguez.com/mainfaq.html,Computer Science
1373,53dc97756369cd1f9300116d6aabdffb7072f2ed,A Statistical View of Some Chemometrics Regression Tools,"Chemometrics is a field of chemistry that studies the application of statistical methods to chemical data analysis. In addition to borrowing many techniques from the statistics and engineering literatures, chemometrics itself has given rise to several new data-analytical methods. This article examines two methods commonly used in chemometrics for predictive modeling—partial least squares and principal components regression—from a statistical perspective. The goal is to try to understand their apparent successes and in what situations they can be expected to work well and to compare them with other statistical methods intended for those situations. These methods include ordinary least squares, variable subset selection, and ridge regression.",109-135,1993.0,http://lambert.com/tagsauthor.jsp,Mathematics
1374,022a0317d5bf2b38847b03f7c9bc3bfa35950199,Understanding data center traffic characteristics,"As data centers become more and more central in Internet communications, both research and operations communities have begun to explore how to better design and manage them. In this paper, we present a preliminary empirical study of end-to-end traffic patterns in data center networks that can inform and help evaluate research and operational approaches. We analyze SNMP logs collected at 19 data centers to examine temporal and spatial variations in link loads and losses. We find that while links in the core are heavily utilized the ones closer to the edge observe a greater degree of loss. We then study packet traces collected at a small number of switches in one data center and find evidence of ON-OFF traffic behavior. Finally, we develop a framework that derives ON-OFF traffic parameters for data center traffic sources that best explain the SNMP data collected for the data center. We show that the framework can be used to evaluate data center traffic engineering approaches. We are also applying the framework to design network-level traffic generators for data centers.",65-72,2009.0,https://brown-aguilar.com/exploreindex.asp,Computer Science
1375,a8edd60a3be9a6e09bd4c6c586a109e4f3f7b116,Introduction to artificial neural systems,"Jacek M. Zurada received his MS and Ph.D. degrees (with distinction) in electrical engineering from the Technical University of Gdansk, Poland. Since 1989 he has been a Professor with the Electrical and Computer Engineering Department at the University of Louisville, Kentucky. He was Department Chair from 2004 to 2006. He has published over 350 journal and conference papers in the areas of neural networks, computational intelligence, data mining, image processing and VLSI circuits. INTRODUCTION TO ARTIFICIAL NEURAL SYSTEMS",31-130,1992.0,http://www.thompson.biz/tags/postshomepage.jsp,Computer Science
1376,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.","
          38-41
        ",2002.0,http://www.martinez.com/categoriesindex.htm,Biology
1377,1e513c7e99197dc30f9c97feabe0223afc786068,Revisions to the JDL data fusion model,"The Data Fusion Model maintained by the Joint Directors of Laboratories (JDL) Data Fusion Group is the most widely-used method for categorizing data fusion-related functions. This paper discusses the current effort to revise the expand this model to facilitate the cost-effective development, acquisition, integration and operation of multi- sensor/multi-source systems. Data fusion involves combining information - in the broadest sense - to estimate or predict the state of some aspect of the universe. These may be represented in terms of attributive and relational states. If the job is to estimate the state of a people, it can be useful to include consideration of informational and perceptual states in addition to the physical state. Developing cost-effective multi-source information systems requires a method for specifying data fusion processing and control functions, interfaces, and associate databases. The lack of common engineering standards for data fusion systems has been a major impediment to integration and re-use of available technology: current developments do not lend themselves to objective evaluation, comparison or re-use. This paper reports on proposed revisions and expansions of the JDL Data FUsion model to remedy some of these deficiencies. This involves broadening the functional model and related taxonomy beyond the original military focus, and integrating the Data Fusion Tree Architecture model for system description, design and development.",70-139,1999.0,https://www.larson.com/categories/tag/mainindex.htm,Engineering
1378,decbd6ebdf8c0e008f7f2a74ff164910f5597c81,Smithells metals reference book,"General physical and chemical constants X-ray analysis of metallic material Crystallography Crystal chemistry Metallurgically important minerals Thermochemical data Physical properties of molton salts Metallography Equilibrium diagrams Gas-metal systems Diffusion in metals General physical properties Elastic properties, damping capacity and shape memory alloys Temperature measurement and thermoelectric properties Radiating properties of metals Electron emission Electrical properties Magnetic materials and their properties Mechanical testing Mechanical properties of metals and alloys Sintered materials Lubricants Friction and wear Casting alloys and foundry data Engineering ceramics and refractory materials Fuels Heat treatment Metal cutting and forming Corrosion Electroplating and metal finishing Welding Soldering and brazing Vapour deposited coatings and thermal spraying Superplasticity Metal-matrix composites Non-conventional and emerging metallic minerals modelling and simulation supporting technologies for the processing of metals and alloys.",81-147,1949.0,http://www.carter.com/tags/category/categoriesterms.jsp,Materials Science
1379,3a4fe629aaa6bfc2d07e651c6ec2e5cfc40bfb23,More Comprehensive and Inclusive Approaches to Demographic Data Collection,"Todd is a PhD Student in Engineering Education at Purdue University who's research is focused on en-trepreneurship education and entrepreneurship education as a component of modern engineering education efforts. research focuses what factors influence diverse students to choose engineering and stay in engineering through their careers and how different experiences within the practice and culture of engineering foster or hinder belongingness and identity development. Dr. Godwin graduated from Clemson University with a B. Dina Verdín is an Engineering Education graduate student at Purdue University. She completed her undergraduate degree in Industrial and Systems Engineering at San José State University. Her research interest focuses on the first-generation college student population, which includes changing the perspective of this population from a deficit base approach to an asset base approach. His research focuses on First Generation engineering college students' engineering identity, belonging-ness, and how they perceive their college experience.He is also on a National Science Foundation project looking at non-normative engineering students and how they may have differing paths to success. His education includes a B.S. His research focuses on the interactions between engineering cultures, student motivation, and their learning experiences. His projects involve the study of student perceptions, beliefs and attitudes towards becoming engineers, their problem solving processes, and cultural fit. His education includes a B. a joint appointment in Bioengineering. Her research focuses on the interactions between student motivation and their learning experiences. Her projects involve the study of student perceptions, beliefs and attitudes towards becoming engineers and scientists, and their problem solving processes. Other projects in the Benson group include effects of student-centered active learning, self-regulated learning, and incorporating engineering into secondary science and mathematics classrooms. Her education includes a B.S.",51-109,2016.0,https://www.wells-beck.biz/mainfaq.php,Geography
1380,1eb131a34fbb508a9dd8b646950c65901d6f1a5b,Hidden Technical Debt in Machine Learning Systems,"Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",2503-2511,2015.0,https://anderson-johnson.info/category/tag/poststerms.html,Computer Science
1381,48e752c719d33ff55b3b3bec3538727f8ce69399,Ontology Learning for the Semantic Web,"The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.",72-79,2002.0,http://moore.org/tagssearch.php,Computer Science
1382,2a3842f6070b4554ff21fe62b2a486657d9a304a,"How to Grow a Mind: Statistics, Structure, and Abstraction","In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?",1279 - 1285,2011.0,https://www.meyer-miles.com/wp-content/wp-content/searchregister.htm,Medicine
1383,5f0a22399569a458b45961577582a83ab8cca2a7,Engineering data compendium : human perception and performance,,47-129,1988.0,http://li-wilson.com/wp-contentauthor.asp,Computer Science
1384,ca3d6205ffb8cab6724fb0828634826e2e87ead7,Spatial ability for STEM domains: Aligning over 50 years of cumulative psychological knowledge solidifies its importance.,"The importance of spatial ability in educational pursuits and the world of work was examined, with particular attention devoted to STEM (science, technology, engineering, and mathematics) domains. Participants were drawn from a stratified random sample of U.S. high schools (Grades 9–12, N 400,000) and were tracked for 11 years; their longitudinal findings were aligned with pre-1957 findings and with contemporary data from the Graduate Record Examination and the Study of Mathematically Precocious Youth. For decades, spatial ability assessed during adolescence has surfaced as a salient psychological attribute among those adolescents who subsequently go on to achieve advanced educational credentials and occupations in STEM. Results solidify the generalization that spatial ability plays a critical role in developing expertise in STEM and suggest, among other things, that including spatial ability in modern talent searches would identify many adolescents with potential for STEM who are currently being missed.",817-835,2009.0,https://www.james.com/postslogin.html,Psychology
1385,690c04954ca8752c72e5a2a06f10c7ffca3d299c,"The Complete ISRM Suggested Methods for Rock Characterization, Testing and Monitoring; 1974–2006","Many engineering geologists and geological engineers have a vested interest in the broad spectrum of professional activities that constitute “rock engineering.” Important contributions to this greater body of knowledge have been provided by the fine, worldwide efforts of the International Society for Rock Mechanics (ISRM) over the past 34 years. While much of the published literature in the field is and has been theoretical, perhaps the larger part is of direct interest and use to practitioners. This long-needed revised compendium of the suggested methods for gathering site information of use to engineers designing structures on and in rock stands at the top of the list of useful technical literature in rock engineering.

The history of this effort is remarkable in itself. ISRM work products have historically been generated by its internal “commissions” as appointed by the leadership directorate, which are designed to bring forth practical solutions to recognized rock engineering data and methods needs. ISRM was founded in 1962, by Prof. Dr. Leopold Mueller of Karlsruhe University, then West Germany, who chose to release the Rock Testing Commission findings as separate papers (1974–1981) that have appeared in the International …",47-48,2009.0,http://www.evans-hines.com/tagsauthor.htm,Engineering
1386,47f84928dd6e40797255fa1e1bbb3c12b2659a7c,Input selection for fast feature engineering,"The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, signals extracted from the data that distill complicated raw data objects into a small number of salient values. A trained system's success depends substantially on the quality of its features. Unfortunately, feature engineering-the process of writing code that takes raw data objects as input and outputs feature vectors suitable for a machine learning algorithm-is a tedious, time-consuming experience. Because “big data” inputs are so diverse, feature engineering is often a trial-and-error process requiring many small, iterative code changes. Because the inputs are so large, each code change can involve a time-consuming data processing task (over each page in a Web crawl, for example). We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the “inner loop” of the feature engineering process. Our system yields feature evaluation speedups of up to 8× in some cases and reduces engineer wait times from 8 to 5 hours in others.",577-588,2016.0,https://www.fleming-garcia.com/searchindex.html,Computer Science
1387,7a4ba4f2dd23c16e02e8e30335a770c0cd3321ed,Spintronics: A Spin-Based Electronics Vision for the Future,"This review describes a new paradigm of electronics based on the spin degree of freedom of the electron. Either adding the spin degree of freedom to conventional charge-based electronic devices or using the spin alone has the potential advantages of nonvolatility, increased data processing speed, decreased electric power consumption, and increased integration densities compared with conventional semiconductor devices. To successfully incorporate spins into existing semiconductor technology, one has to resolve technical issues such as efficient injection, transport, control and manipulation, and detection of spin polarization as well as spin-polarized currents. Recent advances in new materials engineering hold the promise of realizing spintronic devices in the near future. We review the current state of the spin-based devices, efforts in new materials fabrication, issues in spin transport, and optical spin manipulation.",1488 - 1495,2001.0,http://ritter.com/categories/blog/postsindex.html,Physics
1388,501608b011a1b0c6a6d1f92153c4c0f54f3d1882,DIRECT NUMERICAL SIMULATION: A Tool in Turbulence Research,"▪ Abstract We review the direct numerical simulation (DNS) of turbulent flows. We stress that DNS is a research tool, and not a brute-force solution to the Navier-Stokes equations for engineering problems. The wide range of scales in turbulent flows requires that care be taken in their numerical solution. We discuss related numerical issues such as boundary conditions and spatial and temporal discretization. Significant insight into turbulence physics has been gained from DNS of certain idealized flows that cannot be easily attained in the laboratory. We discuss some examples. Further, we illustrate the complementary nature of experiments and computations in turbulence research. Examples are provided where DNS data has been used to evaluate measurement accuracy. Finally, we consider how DNS has impacted turbulence modeling and provided further insight into the structure of turbulent boundary layers.",539-578,1998.0,https://ingram.com/list/exploresearch.html,Physics
1389,90b8cfa993357cccd94d05a4317342892516731b,Data Mining for Software Engineering,"To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining SE data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.",55-105,2009.0,http://www.frederick-phillips.info/tagspost.asp,Computer Science
1390,929607741b2a12656ff8d3360ca96fe76a6557a4,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",51-105,2013.0,http://valdez.com/tagmain.html,Psychology
1391,bc2b892b27ae9a9805e37d79d1986291c5b589d2,Data Fusion Approaches and Applications for Construction Engineering,"Data fusion can be defined as the process of combining data or information for estimating the state of an entity. Data fusion is a multidisciplinary field that has several benefits, such as enhancing the confidence, improving reliability, and reducing ambiguity of measurements for estimating the state of entities in engineering systems. It can also enhance completeness of fused data that may be required for estimating the state of engineering systems. Data fusion has been applied to different fields, such as robotics, automation, and intelligent systems. This paper reviews some examples of recent applications of data fusion in civil engineering and presents some of the potential benefits of using data fusion in civil engineering.",863-869,2011.0,http://hayes.com/category/main/tagabout.htm,Engineering
1392,f5305cd90970a8917392f1a0e65c2ecf69325374,A summary review of wireless sensors and sensor networks for structural health monitoring,"In recent years, there has been an increasing interest in the adoption of emerging sensing technologies for instrumentation within a variety of structural systems. Wireless sensors and sensor networks are emerging as sensing paradigms that the structural engineering field has begun to consider as substitutes for traditional tethered monitoring systems. A benefit of wireless structural monitoring systems is that they are inexpensive to install because extensive wiring is no longer required between sensors and the data acquisition system. Researchers are discovering that wireless sensors are an exciting technology that should not be viewed as simply a substitute for traditional tethered monitoring systems. Rather, wireless sensors can play greater roles in the processing of structural response data; this feature can be utilized to screen data for signs of structural damage. Also, wireless sensors have limitations that require novel system architectures and modes of operation. This paper is intended to serve as a summary review of the collective experience the structural engineering community has gained from the use of wireless sensors and sensor networks for monitoring structural performance and health.",91-128,2006.0,https://www.sanchez.info/explore/appcategory.html,Engineering
1393,fb2704b2eeb306bf9dab852c45e909f928645be3,Corporate Social Responsibility and Firm Risk: Theory and Empirical Evidence,"This paper presents an industry equilibrium model where firms can choose to engage in corporate social responsibility (CSR) activities. We model CSR activities as an investment in customer loyalty and show that CSR decreases systematic risk and increases firm value. These effects are stronger for firms producing differentiated goods and when consumers' expenditure share on CSR goods is small. We find supporting evidence for our predictions. In our empirical tests, we address a potential endogeneity problem by instrumenting CSR using data on the political affiliation of the firm's home state, and data on environmental and engineering disasters and product recalls.",38-127,2012.0,http://www.miranda-jones.com/main/search/wp-contentindex.php,Business
1394,1e62a8afbe6018540c60d9dcce1ff6bd98f2e404,Automatic query reformulations for text retrieval in software engineering,"There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).",842-851,2013.0,https://www.garcia.info/main/category/searchregister.html,Computer Science
1395,969bae558fa9a8d2c1daa7c8575852eaf82815f6,DATA DIVISION FOR DEVELOPING NEURAL NETWORKS APPLIED TO GEOTECHNICAL ENGINEERING,"In recent years, artificial neural networks (ANNs) have been applied to many geotechnical engineering problems with some degree of success. In the majority of these applications, data division is carried out on an arbitrary basis. However, the way the data are divided can have a significant effect on model performance. In this paper, the issue of data division and its impact on ANN model performance is investigated for a case study of predicting the settlement of shallow foundations on granular soils. Four data division methods are investigated: (1) random data division; (2) data division to ensure statistical consistency of the subsets needed for ANN model development; (3) data division using self-organizing maps (SOMs); and (4) a new data division method using fuzzy clustering. The results indicate that the statistical properties of the data in the training, testing, and validation sets need to be taken into account to ensure that optimal model performance is achieved. It is also apparent from the results that the SOM and fuzzy clustering methods are suitable approaches for data division.",105-114,2004.0,http://levine.info/blog/tags/postssearch.html,Engineering
1396,e23e287baf50b5b9a19774de6d6fb356b6bac212,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",161-166,2010.0,http://www.sexton.com/category/listlogin.htm,Engineering
1397,b22de434b462558a127f327f29e2b0c673c0d7ab,"Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey",,15169 - 15211,2017.0,https://potter.net/tag/tagpost.asp,Computer Science
1398,8df72c48a7ce4418c683c4dd9bb300558ac71d47,"Deep learning for healthcare: review, opportunities and challenges","Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.","
          1236-1246
        ",2018.0,https://johnson-mccoy.com/listprivacy.php,Computer Science
1399,497abc0ddace6a7772a5f5a3edb3d7b751476755,Methodology for the Design and Evaluation of Ontologies,"As information systems play a more active role in the management and operations of an enterprise, the demands on these systems have also increased. Departing from their traditional role as simple repositories of data, information systems must now provide more sophisticated support to manual and automated decision making; they must not only answer queries with what is explicitly represented in their Enterprise Model, but must be able to answer queries with what is implied by the model. The goal of the TOVE (TOronto Virtual Enterprise) Enterprise Modelling project is to create the next generation Enterprise Model, a Common Sense Enterprise Model. By common sense we mean that an Enterprise Model has the ability to deduce answers to queries that require relatively shallow knowledge of the domain. We are taking what can be viewed as a `second generation knowledge engineering' approach to constructing our Common Sense Enterprise Model. Rather than extracting rules from experts, we are `engineering ontologies.' An ontology is a formal description of entities and their properties, relationships, constraints, behaviours. Through interaction with our industrial partners, we encounter problems that arise in their particular enterprises. Our approach to engineering ontologies begins with using these problems to de ne an ontology's requirements in the form of questions that an ontology must be able to answer. We call this the competency of the ontology. The second step is to de ne the terminology of the ontology its objects, attributes, and relations. In this way the ontology provides the language that will be used to express the de nitions in the terminology and the constraints required by the application. The third step is to specify the de nitions and constraints on the terminology, where possible. The speci cations are represented in First Order Logic and implemented in Prolog. Lastly, we test the competency of the ontology by proving completeness theorems with respect to the competency questions. Our initial e orts have focused on ontologies to support reasoning in industrial environments. The tasks that we have targeted to support are in `supply chain management' which extends MRP (Manufacturing Requirements Planning) to include logistics/distribution [Fox",82-133,1995.0,http://www.cortez.biz/tag/listabout.jsp,Computer Science
1400,4e0b35201060193709610bbe4a20e4a7cd597edb,Methodologies for model-free data interpretation of civil engineering structures,,467-482,2010.0,http://www.lee.com/category/list/exploremain.htm,Engineering
1401,ceb57e2c12c97b430351abf90bfc7538c2b3b9f6,Engineering Identity: Gender and Professional Identity Negotiation among Women Engineers,"This article considers how women in a gendered profession, engineering, construct their professional identity in response to workplace interpersonal interactions that marginalize it. Using data from interviews with women engineers, it also explores how these interactions influence the engineers' sense of self and belonging in engineering. The interpersonal interactions place professional identity on the periphery and can overly validate gender identity. I discuss two types of identity construction strategies employed by the participants in response to these marginalizing interactions: impression management tactics and coping strategies. Although the data demonstrate that participants may be left feeling devalued or ambivalent towards their identity or fit in engineering, some interactions are more validating and offer a sense of belonging. This article also reflects on how the engineers' actions may, in fact, represent forces for change in the gendered culture of engineering.",382-396,2013.0,http://www.payne.org/posts/category/blogprivacy.html,Sociology
1402,d873ca587904ee6c1823a78995f77cee849e4c9f,Modern Software Engineering Methodologies Meet Data Warehouse Design: 4WD,,66-79,2011.0,http://www.guzman.org/category/categories/appregister.php,Computer Science
1403,ca4d2075b3fc8bf7edd1d6a8041ead8400ba266d,Quantitative Classification of Near-Fault Ground Motions Using Wavelet Analysis,"A method is described for quantitatively identifying ground motions containing strong velocity pulses, such as those caused by near-fault directivity. The approach uses wavelet analysis to extract the largest velocity pulse from a given ground motion. The size of the extracted pulse relative to the original ground motion is used to develop a quantitative criterion for classifying a ground motion as “pulselike.” The criterion is calibrated by using a training data set of manually classified ground motions. To identify the subset of these pulselike records of greatest engineering interest, two additional criteria are applied: the pulse arrives early in the ground motion and the absolute amplitude of the velocity pulse is large. The period of the velocity pulse (a quantity of interest to engineers) is easily determined as part of the procedure, using the pseudoperiods of the basis wavelets. This classification approach is useful for a variety of seismology and engineering topics where pulselike ground motions are of interest, such as probabilistic seismic hazard analysis, ground- motion prediction (“attenuation”) models, and nonlinear dynamic analysis of structures. The Next Generation Attenuation (nga) project ground motion library was processed using this approach, and 91 large-velocity pulses were found in the fault- normal components of the approximately 3500 strong ground motion recordings considered. It is believed that many of the identified pulses are caused by near-fault directivity effects. The procedure can be used as a stand-alone classification criterion or as a filter to identify ground motions deserving more careful study.",1486-1501,2007.0,https://jordan.biz/tag/tagindex.html,Engineering
1404,0198dac70004da88cddc17de6a233504cb0d1cfe,"A Highly Characterized Yeast Toolkit for Modular, Multipart Assembly.","Saccharomyces cerevisiae is an increasingly attractive host for synthetic biology because of its long history in industrial fermentations. However, until recently, most synthetic biology systems have focused on bacteria. While there is a wealth of resources and literature about the biology of yeast, it can be daunting to navigate and extract the tools needed for engineering applications. Here we present a versatile engineering platform for yeast, which contains both a rapid, modular assembly method and a basic set of characterized parts. This platform provides a framework in which to create new designs, as well as data on promoters, terminators, degradation tags, and copy number to inform those designs. Additionally, we describe genome-editing tools for making modifications directly to the yeast chromosomes, which we find preferable to plasmids due to reduced variability in expression. With this toolkit, we strive to simplify the process of engineering yeast by standardizing the physical manipulations and suggesting best practices that together will enable more straightforward translation of materials and data from one group to another. Additionally, by relieving researchers of the burden of technical details, they can focus on higher-level aspects of experimental design.","
          975-86
        ",2015.0,http://www.lowe.com/search/categoriesauthor.asp,Biology
1405,74b90167eb80e959e84497c51e4e9da37a8de0ea,"Visible light communication: opportunities, challenges and the path to market","Visible light communication is a potentially disruptive form of wireless communication that can supplement radio frequency communication and also uniquely enable novel mobile wireless device use cases. High data rate downlink communication in homes and offices and high accuracy indoor positioning in retail stores are two of the most compelling use cases of this promising new technology. Large-scale commercialization of visible light communication devices will depend on both the development of robust and efficient engineering solutions, and the execution of incremental commercialization strategies.",26-32,2013.0,https://suarez-morris.com/posts/explore/listprivacy.html,Computer Science
1406,86c35680190dcb1a040243927e98e4ca08f8d80e,Generalized linear models. 2nd ed.,"Addresses a class of statistical models that generalizes classical linear models-extending them to include many other models useful in statistical analysis. Incorporates numerous exercises, both theoretical and data-analytic Discusses quasi-likelihood functions and estimating equations, models for dispersion effect, components of dispersion, and conditional likelihoods Holds particular interest for statisticians in medicine, biology, agriculture, social science, and engineering",698,1993.0,http://perkins.net/category/searchhomepage.htm,Computer Science
1407,0ae8159db7bf5ad1fbc606e6fea0be7a0e2ea3c9,Titanium: A Technical Guide,"Designed to support the need of engineering, management, and other professionals for information on titanium by providing an overview of the major topics, this book provides a concise summary of the most useful information required to understand titanium and its alloys. The author provides a review of the significant features of the metallurgy and application of titanium and its alloys. All technical aspects of the use of titanium are covered, with sufficient metals property data for most users. Because of its unique density, corrosion resistance, and relative strength advantages over competing materials such as aluminum, steels, and superalloys, titanium has found a niche in many industries. Much of this use has occurred through military research, and subsequent applications in aircraft, of gas turbine engines, although more recent use features replacement joints, golf clubs, and bicycles. Contents include: A primer on titanium and its alloys, Introduction to selection of titanium alloys, Understanding titanium's metallurgy and mill products, Forging and forming, Castings, Powder metallurgy, Heat treating, Joining technology and practice, Machining, Cleaning and finishing, Structure/processing/property relationships, Corrosion resistance, Advanced alloys and future directions, Appendices: Summary table of titanium alloys, Titanium alloy datasheets, Cross-reference to titanium alloys, Listing of selected specification and standardization organizations, Selected manufacturers, suppliers, services, Corrosion data, Machining data.",89-136,1988.0,https://www.woods.com/searchpost.html,Materials Science
1408,9fc1062987cac66390a77f31d5758bdbb6f3ec50,Towards a Model-Driven Design Tool for Big Data Architectures,"Big Data technologies are rapidly becoming a key enabler for modern industries. However, the entry costs inherent to ``going Big"" are considerable, ranging from learning curve, renting/buying infrastructure, etc. A key component of these costs is the time spent on learning about and designing with the many big data frameworks (e.g., Spark, Storm, HadoopMR, etc.) on the market. To reduce said costs while decreasing time-to-market we advocate the usage of Model-Driven Engineering (MDE), i.e., software engineering by means of models and their automated manipulation. This paper outlines a tool architecture to support MDE for big data applications, illustrating with a case-study.",37-43,2016.0,https://www.pham.com/search/list/wp-contentauthor.php,Computer Science
1409,f874dc10896be9022cf93f63f4507c102ded2b74,A Data-Level Fusion Model for Developing Composite Health Indices for Degradation Modeling and Prognostic Analysis,"Prognostics involves the effective utilization of condition or performance-based sensor signals to accurately estimate the remaining lifetime of partially degraded systems and components. The rapid development of sensor technology, has led to the use of multiple sensors to monitor the condition of an engineering system. It is therefore important to develop methodologies capable of integrating data from multiple sensors with the goal of improving the accuracy of predicting remaining lifetime. Although numerous efforts have focused on developing feature-level and decision-level fusion methodologies for prognostics, little research has targeted the development of “data-level” fusion models. In this paper, we present a methodology for constructing a composite health index for characterizing the performance of a system through the fusion of multiple degradation-based sensor data. This methodology includes data selection, data processing, and data fusion steps that lead to an improved degradation-based prognostic model. Our goal is that the composite health index provides a much better characterization of the condition of a system compared to relying solely on data from an individual sensor. Our methodology was evaluated through a case study involving a degradation dataset of an aircraft gas turbine engine that was generated by the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS).",652-664,2013.0,http://garza.info/mainprivacy.html,Computer Science
1410,78379eeba6d8de526dfbc983818401baa689f0b5,Ontology of core data mining entities,,1222-1265,2014.0,https://buchanan.biz/tagcategory.htm,Computer Science
1411,e9e5e46ca827493feb8b6c85ebd9c8ead8a70dae,The design structure system: A method for managing the design of complex systems,"Systems design involves the determination of interdependent variables. Thus the precedence ordering for the tasks of determining these variables involves circuits. Circuits require planning decisions about how to iterate and where to use estimates. Conventional planning techniques, such as critical path, do not deal with these problems. Techniques are shown which acknowledge these circuits in the design of systems. These techniques can be used to develop an effective engineering plan, showing where estimates are to be used, how design iterations and reviews are handled, and how information flows during the design work. This information flow can be used to determine the consequences of a change in any variable on the rest of the variables in the system, and thus which engineers must be informed and which documents must be changed. From this, a critical path schedule can be developed for implementing the change. This method is ideally suited to an automated design office where data, computer input and output, and communications are all handled through the use of computer terminals and data bases. However, these same techniques can also be effectively used in classical engineering environments.",71-74,1981.0,http://www.hernandez-quinn.info/wp-content/posts/listhomepage.htm,Computer Science
1412,7321ae1b6dc03f4575b9f1acee6c69b1072e45f8,GATE-a General Architecture for Text Engineering,"Much progress has been made in the provision of reusable data resources for Natural Language Engineering, such as grammars, lexicons, thesauruses. Although a number of projects have addressed the provision of reusable algorithmic resources (or 'tools'), takeup of these resources has been relatively slow. This paper describes GATE, a General Architecture for Text Engineering, which is a freely-available system designed to help alleviate the problem. 1 Resource Reuse and Natura l Language Engineering Car designers don't reinvent the wheel each time they plan a new model, but software engineers often find themselves repetitively producing roughly the same piece of software in slightly ditfenmt R)rm. The reasons for this inefficency have been extensively studied, and a number of solutions are now available (Prieto-Diaz and t~h'eeman, 1987; Prieto-Diaz, 1993). Similarly, the Natural Language Engineering (NLE l) community has identified the potential benefits of reducing repetition, and work has been flmded to promote reuse. This work concerns either reusable resources which are primarily data or those which are primarily algorithmic (i.e. processing 'tools', or programs, or code libraries). Successflfl examples of reuse of data resources include: the WordNet thesaurus (Miller el; al., 1993); the Penn Tree Bank (Marcus et al., 1993); the Longmans Dictionary of Contemporary English (Summers, 1995). A large number of papers report results relative to these and other resources, and these successes have spawned a num1See (Boguraev et al., 1995) or (Cunningham et al., 1995) for discussion of the significance of this label. ber of projects with similar directions, one of the latest examples of which being ELRA, tile EuroI)ean Language Resources Association. The reuse of algorithmic resources remains more limited (Gunninghaln et al., 1994). There are a number of reasons for this, including: 1. cultural resistance to reuse, e.g. mistrust of 'foreign' code; 2. integration overheads. In some respects these probleIns are insoluble without general changes in the way NLE research is done researchers will always be reluctant to use poorly-documented or unreliable systems as part of their work, for exmnple. In other respects, solutions are possible. They include: 1. increasing the granularity of the units of reuse, i.e. providing sets of small buildingblocks instead of large, Inonolithic systems; 2. increasing the confidence of researchers in available algorithmic resources by increasing their reuse and the amount of testing and evaluation they are subjected to; 3. separating out, the integration problems that are independent of the type of information being processed and reducing the overhead caused by these problems by providing a software architecture for NLE systems. Our view is that succesful algorithmic reuse in NLE will require the provision of support software for NLE in the form of a general architecture and development environment which is specifically designed for text processing systems. Under EPSRC 2 grant GR/K25267 the NLP group at, the University of Sheffield are developing a system that aims to implement this new approach. The system is called GATE the General Architecture for Text Engineering. ~The Engineering and Physical Science Research Council, UK funding body.",1057-1060,1996.0,http://henderson.biz/tag/explorehomepage.html,Computer Science
1413,0b70b3424193fa9a576f58fee7d9a6cc262f383f,The Case for Fine-Grained Traffic Engineering in Data Centers,"In recent years, several techniques have been suggested for routing and traffic engineering in data centers. However, not much is known about how these techniques per-formrelative to each other under realistic data center traffic patterns. Our preliminary study reveals that existing techniques can only achieve 80% to 85% of the ideal solution in terms of the number of bytes delivered. We find that these techniques suffer due to their inability to utilize global knowledge of the properties of traffic flows and their inability to make coordinated decision for scheduling flows at fine time-scales. Even recent traffic engineering techniques such as COPE fail in data centers despite their proven ability to adapt to dynamic variations, because they are designed to operate at longer time scales (on the order of hours, at least). In contrast, data centers, due to the bursty nature inherent to their traffic, require adaptation at much finer times scales. To this end, we define a set of requirements that a data center-oriented traffic engineering technique must possess in order to successfully mitigate congestion. In this paper, we present the design for a strawman framework that fulfills these requirements.",2-2,2010.0,http://www.pierce.org/postsabout.jsp,Engineering
1414,98532d90cda92a3c5079955a41efd9a182a695e6,Operations Research (OR),"400 Level Courses OR 438: Analytics for Financial Engineering and Econometrics. 3 credits. Introduces the basic analytics for financial engineering and econometrics. Topics include financial transactions and econometric data management, correlation, linear and multiple regressions for financial and economic predictions, financial time series analysis, portfolio theory, and risk analysis. Provides a foundation of basic theory and methodology as well as applied examples with techniques to analyzing large financial and econometric data. Hands-on experiments with R will be emphasized throughout the course. Offered by Systems Engr & Operations Rsch. Limited to two attempts. Equivalent to SYST 438.",36-104,2007.0,https://williams.com/explore/mainmain.html,Technology
1415,7b0aa266562518af198631641cbd6db50ca8362f,Constraint-based models predict metabolic and associated cellular functions,,107-120,2014.0,https://www.sanford.com/explore/apphomepage.asp,Biology
1416,a68dabe8715a68440821eaf2c54a79f33bb3109b,Data fitting with geometric-programming-compatible softmax functions,,897 - 918,2016.0,http://www.johnson.org/tagsterms.htm,Mathematics
1417,8213bbf377efd4f6bae2ec4950aeaa25a1728646,Taguchi's Quality Engineering Handbook,"points, decisions are made based on results for the study endpoints. In clinical trials, the decisions are usually whether to stop the trial because the efficacy and safety of the treatment can be confirmed already, the safety risks are too great, or the treatment is very unlikely to achieve its therapeutic goal (called stopping for futility). Rules for stopping the trial are made prior to collecting any data. Such rules, called stopping rules, are typically formally defined in a protocol that is completed and approved prior to the start of the trial. Adaptive procedures add the following features to the possible decisions at the interim analyses: (1) the addition or deletion of trial arms in a multiple armed clinical trial, (2) an increase or decrease in the total sample at the end of the study (based on interim estimates of variability and/or other assumed parameters, e.g., effect size), and (3) other changes to the design (such as changes to the inclusion/exclusion criteria for the study subjects). Statisticians in the pharmaceutical and the medical device industries as well as at the National Institutes of Health (NIH) and other medical research institutes will find this book invaluable. Given that most readers of Technometrics are statisticians and practitioners in either the physical, chemical, or engineering sciences, they may not find it as immediately applicable as a biostatistician would. Prior to the development of group sequential procedures there were sequential procedures. Sequential procedures are just like group sequential procedures except that the interim analyses occur after each newly observed data point. These sequential methods were developed (both theory and applications) by Abraham Wald in the United States and George Barnard in the United Kingdom in the 1940s as part of the war effort during World War II. The motivating application was reliability testing of military products such as ammunition. There was a desire to determine that the ammunition was safe and reliable without wasting a lot of ammunition in testing. The same reasoning could apply to any product that requires destructive testing to determine its reliability and is expensive or time consuming to produce. After the war, the practical application was hindered by the need to make and the difficulty of making real-time decisions after every sample. Group sequential methods made the whole idea of sequential testing or monitoring much more useful. The reliability applications may be of interest to the general Technometrics reader, but this book and the text by Jennison and Turnbull (2000) include only clinical trial applications. The authors of the text under review are among the top researchers in the field, and this text by Proschan, Lan, and Wittes very well written and provides thorough and nearly complete coverage of the latest developments in group sequential methods. It also contains a chapter on adaptive sample size methods (Chap. 11). These methods are a subset of the adaptive procedures, and include Stein’s method and others for constructing two-stage designs to deal with nuisance parameters. Among other sample size adjustment methods, the authors include adjusting the sample size based on an interim assessment of the effect size. The few topics in group sequential methods that are not covered in detail are outlined in Chapter 12 (titled “Topics Not Covered”). The text by Jennison and Turnbull (2000) was the first major text on group sequential methods. It came out in 2000 and is considered by many to be a classic on the subject. Both, Jennison and Turnbull are well-known statisticians and they have published widely in the statistics and biostatistics literature. These two texts cover mostly the same topics, are both very current, and both give examples in clinical trials. So a reader, like me, who already owns a copy of Jennison and Turnbull might ask what would be the added value of purchasing Proschan, Lan, and Wittes? I would give the following reasons:",224 - 225,2007.0,http://johnson.com/category/tag/searchindex.htm,Mathematics
1418,294df07a2eaaa9eb39d3404c145ef429f84b56ae,Data sets and data quality in software engineering,"OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.",39-44,2008.0,http://vaughn.com/category/list/tagpost.php,Computer Science
1419,db74560c19183b2aa0be90b6d6e263dddf91f339,The Institution of Engineering and Technology Seminar on Target Tracking and Data Fusion,"09.35 Keynote: Estimation without Independence Filtering and estimation algorithms assume conditional independence, either in the process or observation noises, however, in many systems such as those engaged in distributed data fusion, this assumption is incorrect. Robust filtering and estimation algorithms need to be developed which can automatically account for the unknown joint distribution. This talk will examine: • a set of algorithms in development which use weighted geometric means to replace the Bayes rule • the properties of these forms • examples illustrating the results Simon Julier, Senior Lecturer, UCL, UK",23-120,2008.0,https://dawson-castaneda.com/app/categorieslogin.php,Technology
1420,c3469ac1064a4045c1bb519a1247bb60c464ab8c,Recent advances in 3D printing of biomaterials,,36-104,2015.0,http://fowler-salazar.info/blog/listterms.asp,Medicine
1421,6be6f71ced97d6e4a2cd52a628a035535e324aaa,High-performance Ge-on-Si photodetectors,,527-534,2010.0,https://www.allen-thompson.com/blog/list/blogabout.php,Materials Science
1422,1082f31a589acf1d9e179ebbac008b666a298cf5,A review of rapid prototyping techniques for tissue engineering purposes,"Rapid prototyping (RP) is a common name for several techniques, which read in data from computer-aided design (CAD) drawings and manufacture automatically three-dimensional objects layer-by-layer according to the virtual design. The utilization of RP in tissue engineering enables the production of three-dimensional scaffolds with complex geometries and very fine structures. Adding micro- and nanometer details into the scaffolds improves the mechanical properties of the scaffold and ensures better cell adhesion to the scaffold surface. Thus, tissue engineering constructs can be customized according to the data acquired from the medical scans to match the each patient's individual needs. In addition RP enables the control of the scaffold porosity making it possible to fabricate applications with desired structural integrity. Unfortunately, every RP process has its own unique disadvantages in building tissue engineering scaffolds. Hence, the future research should be focused on the development of RP machines designed specifically for fabrication of tissue engineering scaffolds, although RP methods already can serve as a link between tissue and engineering.",268 - 280,2008.0,http://taylor.com/search/postsregister.html,Computer Science
1423,b9b36a61bcac98777a0ac06e4eb36b2959bef511,Automated reverse engineering of nonlinear dynamical systems,"Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated “reverse engineering” approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.",9943 - 9948,2007.0,https://miller.info/search/exploreauthor.php,Medicine
1424,de760552279b241976676f4723a159060a433198,GHTorrent: Github's data from a firehose,"A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.",12-21,2012.0,http://perez.com/app/tags/categoriesprivacy.php,Computer Science
1425,4b598fd43705e7e7d52ec911cfd32d22e22b7ae5,Spillover Effects of FDI on Innovation in China: Evidence from the Provincial Data,"Foreign direct investment (FDI) can benefit innovation activity in the host country via spillover channels such as reverse engineering, skilled labor turnovers, demonstration effects, and supplier-customer relationships. Using provincial data from 1995 to 2000, we find positive effects of FDI on the number of domestic patent applications in China. This finding is robust under both pooled time-series and cross-section data estimation and panel data analysis and for different types of patent applications (invention, utility model, and external design). The spillover effect is the strongest for minor innovation such as external design patent, highlighting a ""demonstration effect"" of FDI.",82-104,2003.0,http://turner-griffith.biz/wp-contentauthor.asp,Economics
1426,b18fb48c8004faa30e786b2d762d0a438a4573f7,Clinical Biomechanics of the Spine,"Combining orthopedic surgery with biomechanical engineering, this reference and teaching text reviews and analyzes the clinical and scientific data on the mechanics of the human spine. This edition adds new material on vibration (i.e. road driving) and its effect on the spine; anatomy and kinematics",17-129,1978.0,http://www.martinez.com/wp-content/mainauthor.htm,Engineering
1427,49a94b4a091924eb287eceb62f5dd9d656a3ec74,A Review of data fusion models and architectures: towards engineering guidelines,,273-281,2005.0,http://thomas-brown.biz/tagslogin.htm,Computer Science
1428,11ccf3b72b6689e81b0f218a0ea26bb6154944b9,"Verification, Validation, and Predictive Capability in Computational Engineering and Physics","Developers of computer codes, analysts who use the codes, and decision makers who rely on the results of the analyses face a critical question: How should confidence in modeling and simulation be critically assessed? Verification and validation (V&V) of computational simulations are the primary methods for building and quantifying this confidence. Briefly, verification is the assessment of the accuracy of the solution to a computational model. Validation is the assessment of the accuracy of a computational simulation by comparison with experimental data. In verification, the relationship of the simulation to the real world is not an issue. In validation, the relationship between computation and the real world, i.e., experimental data, is the issue.",345-384,2004.0,https://tran.org/wp-contentpost.htm,Technology
1429,371c4477d6beab4efa5f8ce2ae7f4516a9f7889e,Underground excavations in rock,"This book deals with the geotechnical aspects of the design of underground openings for mining and civil engineering purposes. It contains a number of worked examples to assist the reader in applying the techniques described to his or her own problems. The data are presented under the following chapter headings: (1) planning considerations; (2) classification of rock masses; (3) geological data collection; (4) graphical presentation of geological data; (5) stresses around underground excavations; (6) strength of rock and rock masses; (7) underground excavation failure mechanisms; (8) underground excavation support design; (9) rockbolts, shotcrete and mesh; (10) blasting in underground excavations; (11) instrumentation. Several appendices deal with: isometric drawing charts, stresses around single openings, two-dimensional boundary element stress analysis, determination of material constants, underground wedge analysis, and conversion factors. A very extensive bibliography is included.",81-127,1980.0,https://www.lee.biz/searchterms.htm,Engineering
1430,d93d4b8a821a6761341fa9ace16b0e3a2ab232c3,Smart manufacturing,"Manufacturing has evolved and become more automated, computerised and complex. In this paper, the origin, current status and the future developments in manufacturing are disused. Smart manufacturing is an emerging form of production integrating manufacturing assets of today and tomorrow with sensors, computing platforms, communication technology, control, simulation, data intensive modelling and predictive engineering. It utilises the concepts of cyber-physical systems spearheaded by the internet of things, cloud computing, service-oriented computing, artificial intelligence and data science. Once implemented, these concepts and technologies would make smart manufacturing the hallmark of the next industrial revolution. The essence of smart manufacturing is captured in six pillars, manufacturing technology and processes, materials, data, predictive engineering, sustainability and resource sharing and networking. Material handling and supply chains have been an integral part of manufacturing. The anticipated developments in material handling and transportation and their integration with manufacturing driven by sustainability, shared services and service quality and are outlined. The future trends in smart manufacturing are captured in ten conjectures ranging from manufacturing digitisation and material-product-process phenomenon to enterprise dichotomy and standardisation.",508 - 517,2018.0,https://www.nelson-smith.net/tagshome.asp,Computer Science
1431,30e741a0330cdcaf6a6466eaca2f09c8bd604b57,A survey of controlled experiments in software engineering,"The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.",733-753,2005.0,https://www.avery-burns.net/blog/explorefaq.html,Computer Science
1432,4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59,Deep learning for neural networks,"Machine learning algorithms are designed to improve as they encounter more data, making them a versatile technology for understanding large sets of photos such as those accessible from Google Images. Elizabeth Holm, professor of materials science and engineering at Carnegie Mellon University, is leveraging this technology to better understand the enormous number of research images accumulated in the field of materials science. [13]",27-143,2018.0,http://burton-walter.com/list/wp-contentfaq.php,Technology
1433,f5705a2c23603579e25c31d3390ddc083f5bfefa,Data Mining for Scientific and Engineering Applications,,65-126,2001.0,http://smith-williams.info/tagspost.htm,Computer Science
1434,acedeb162d0a1e7af8767127470b48b0425b57a1,Uncertainty assessment of hydrologic model states and parameters: Sequential data assimilation using the particle filter,"Two elementary issues in contemporary Earth system science and engineering are (1) the specification of model parameter values which characterize a system and (2) the estimation of state variables which express the system dynamic. This paper explores a novel sequential hydrologic data assimilation approach for estimating model parameters and state variables using particle filters (PFs). PFs have their origin in Bayesian estimation. Methods for batch calibration, despite major recent advances, appear to lack the flexibility required to treat uncertainties in the current system as new information is received. Methods based on sequential Bayesian estimation seem better able to take advantage of the temporal organization and structure of information, so that better compliance of the model output with observations can be achieved. Such methods provide platforms for improved uncertainty assessment and estimation of hydrologic model components, by providing more complete and accurate representations of the forecast and analysis probability distributions. This paper introduces particle filtering as a sequential Bayesian filtering having features that represent the full probability distribution of predictive uncertainties. Particle filters have, so far, generally been used to recursively estimate the posterior distribution of the model state; this paper investigates their applicability to the approximation of the posterior distribution of parameters. The capability and usefulness of particle filters for adaptive inference of the joint posterior distribution of the parameters and state variables are illustrated via two case studies using a parsimonious conceptual hydrologic model.",78-124,2005.0,https://www.frazier.com/category/blog/tagsfaq.html,Geology
1435,09e71e98a1f9b04b97165b0e56935e1054239556,Knowledge Representation and Ontology Mapping Methods for Product Data in Engineering Applications,"This work seeks to create a semantic approach that uses ontologies for sharing knowledge related to product data in CAD/CAE applications and for integrating the design evaluation information that these applications individually provide. Our overall approach is coined OADE, Ontology-based Adaptive Design Evaluation. This paper reports on a piece of our ongoing work in this area. The key contributions of this paper include methods for the design of knowledge representation in product design and analysis, population of product data semantics, creation of ontology mapping methods and mapping representations, and mapping of product data semantics to the target application. The mapping method finds matching concepts between different ontologies based on three basic concept relation types: composition, inheritance, and attribute. A prototype implementation is being created using technologies such as OWL (representation tool), Jena (ontology builder), and Protege (ontology editor) to demonstrate the approach for integrating a parametric CAD system, custom virtual assembly application, and an ergonomics engineering application. An example is given in this paper to illustrate how this approach can help integration between a product design application and an assembly simulation analysis application. The significance of this work is that it will provide the capability to create, share, and exchange knowledge for solving design evaluation challenges involving multiple applications and multiple viewpoints. A design decision can thus be described using the common concepts across the diverse entities.Copyright © 2008 by ASME",20-102,2010.0,https://payne.com/wp-content/listhomepage.asp,Computer Science
1436,0a4c6ab97208b9dd5d47a60ac7e55aeefdabe6aa,The Measurement of Power Spectra from the Point of View of Communications Engineering,"The measurement of power spectra is a problem of steadily increasing importance which appears to some to be primarily a problem in statistical estimation. Others may see it as a problem of instrumentation, recording and analysis which vitally involves the ideas of transmission theory. Actually, ideas and techniques from both fields are needed. When they are combined, they provide a basis for developing the insight necessary (i) to plan both the acquisition of adequate data and sound procedures for its reduction to meaningful estimates and (ii) to interpret these estimates correctly and usefully. This account attempts to provide and relate the necessary ideas and techniques in reasonable detail — Part I of this article appeared in the January, 1958 issue of THE BELL SYSTEM TECHNICAL JOURNAL.",67-130,1958.0,http://www.shah.info/mainabout.html,Computer Science
1437,0476ddfada4efc840f38a71588afb6a88874e5dc,Data at work: supporting sharing in science and engineering,"Data are a fundamental component of science and engineering work, and the ability to share data is critical to the validation and progress of science. Data sharing and reuse in some fields, however, has proven to be a difficult problem. This paper argues that the development of effective CSCW systems to support data sharing in work groups requires a better understanding of the use of data in practice. Drawing on our work with three scientific disciplines, we show that data play two general roles in scientific communities: 1) they serve as evidence to support scientific inquiry, and 2) they make a social contribution to the establishment and maintenance of communities of practice. A clearer consideration and understanding of these roles can contribute to the design of more effective data sharing systems. We suggest that this can be achieved through supporting social interaction around data abstractions, reaching beyond current metadata models, and supporting the social roles of data.",97-108,2003.0,http://gutierrez.biz/list/list/categoriessearch.htm,Computer Science
1438,4e5f14131db1dada5eab8c5d4bf1317e611bb107,A Pattern Recognition Approach for Software Engineering Data Analysis,"In order to plan, control, and evaluate the software development process, one needs to collect and analyze data in a meaningful way. Classical techniques for such analysis are not always well suited to software engineering data. A pattern recognition approach for analyzing software engineering data, called optimized set reduction (OSR), that addresses many of the problems associated with the usual approaches is described. Methods are discussed for using the technique for prediction, risk management, and quality evaluation. Experimental results are provided to demonstrate the effectiveness of the technique for the particular application of software cost estimation. >",931-942,1992.0,https://thomas.com/blog/categoryhome.html,Computer Science
1439,8ad7b7d55331d708055217797d3ff080dd3c2836,Why are companies offshoring innovation? The emerging global race for talent,,901-925,2009.0,http://mcintyre-jones.info/wp-content/category/appcategory.html,Business
1440,53e0cea00e0653faba15d91f9b5673576af65967,Software Engineering Data Collection for Field Studies,,9-34,2008.0,https://rowland.com/tags/search/categoriescategory.htm,Computer Science
1441,e85acfee90971bf120ef7e8605855b01db08a833,Multisensor Data Fusion,"Expanding the scope of the bestselling first edition, this new edition is now in two volumes. It contains nine new chapters and focuses on the most recent developments in the fusion of data in a variety of applications from military to automotive to medical. It provides information on mathematical techniques and computer methods employed to perform fusion. The set includes new material on target tracking and identification, situation refinement, consequence prediction, resource allocation and refinement, and human computer interaction and cognitive support. It provides new material on engineering issues such as fusion in distributed network systems, information security, and issues with service-oriented architectures.",28-146,1990.0,https://www.lane.biz/tagterms.htm,Engineering
1442,396c6757db39235c57afc0f8612bd9eca1bfaffb,Problem‐based Learning: Influence on Students' Learning in an Electrical Engineering Course,"Recently, there has been a shift from using lecture‐based teaching methods in undergraduate engineering courses to using more learner‐centered teaching approaches, such as problem‐based learning. However, research on the impact of these approaches has mainly involved student perceptions of the teaching method and anecdotal and opinion pieces by faculty on their use of the teaching method, rather than empirically collected data on students' learning outcomes.",95-145,2011.0,http://www.morris-gutierrez.info/tag/blog/categoriesauthor.htm,Psychology
1443,58ab642f01668d4d32ac1fb79d46acb6710a472f,"Automation, Production Systems, and Computer-Integrated Manufacturing","This book provides the most advanced, comprehensive, and balanced coverage on the market of the technical and engineering aspects of automated production systems. It covers all the major cutting-edge technologies of production automation and material handling, and how these technologies are used to construct modern manufacturing systems. Manufacturing Operations; Industrial Control Systems; Sensors, Actuators, and Other Control System Components; Numerical Control; Industrial Robotics; Discrete Control Using Programmable Logic Controllers and Personal Computers; Material Transport Systems; Storage Systems; Automatic Data Capture; Single Station Manufacturing Cells; Group Technology and Cellular Manufacturing; Flexible Manufacturing Systems; Manual Assembly Lines; Transfer Lines and Similar Automated Manufacturing Systems; Automated Assembly Systems; Statistical Process Control; Inspection Principles and Practices; Inspection Technologies; Product Design and CAD/CAM in the Production System; Process Planning and Concurrent Engineering; Production Planning and Control Systems; and Lean Production and Agile Manufacturing. For anyone interested in Automation, Production Systems, and Computer-Integrated Manufacturing.",40-146,1987.0,http://huff.com/tag/category/blogregister.jsp,Engineering
1444,fef2e8c362418adee1a1464e2912ff4e5b7f6817,The thematic and citation landscape of Data and Knowledge Engineering,,234-259,2008.0,https://lopez.com/tags/wp-contentlogin.php,Computer Science
1445,250b4f05982b491ad80ba8b986d958eedb69a6be,ROBPCA: A New Approach to Robust Principal Component Analysis,"We introduce a new method for robust principal component analysis (PCA). Classical PCA is based on the empirical covariance matrix of the data and hence is highly sensitive to outlying observations. Two robust approaches have been developed to date. The first approach is based on the eigenvectors of a robust scatter matrix such as the minimum covariance determinant or an S-estimator and is limited to relatively low-dimensional data. The second approach is based on projection pursuit and can handle high-dimensional data. Here we propose the ROBPCA approach, which combines projection pursuit ideas with robust scatter matrix estimation. ROBPCA yields more accurate estimates at noncontaminated datasets and more robust estimates at contaminated data. ROBPCA can be computed rapidly, and is able to detect exact-fit situations. As a by-product, ROBPCA produces a diagnostic plot that displays and classifies the outliers. We apply the algorithm to several datasets from chemometrics and engineering.",64 - 79,2005.0,https://www.benson-king.info/posts/appsearch.html,Mathematics
1446,3de62b3ce8b5de79ccc6e381eedbd49a328a520f,Combination of Evidence in Dempster-Shafer Theory,"Dempster-Shafer theory offers an alternative to traditional probabilistic theory for the mathematical representation of uncertainty. The significant innovation of this framework is that it allows for the allocation of a probability mass to sets or intervals. Dempster-Shafer theory does not require an assumption regarding the probability of the individual constituents of the set or interval. This is a potentially valuable tool for the evaluation of risk and reliability in engineering applications when it is not possible to obtain a precise measurement from experiments, or when knowledge is obtained from expert elicitation. An important aspect of this theory is the combination of evidence obtained from multiple sources and the modeling of conflict between them. This report surveys a number of possible combination rules for Dempster-Shafer structures and provides examples of the implementation of these rules for discrete and interval-valued data.",53-125,2002.0,https://www.hale.com/tag/explore/exploremain.php,Computer Science
1447,8d3e4d0ea9f81d045387bdea68ac38dbbcb54c6a,Low-Speed Wind Tunnel Testing,"Wind Tunnels. Wind Tunnel Design. Pressure, Flow, and Shear Stress Measurements. Flow Visualization. Calibration of the Test Section. Forces and Moments from Balance Measurements. Use of Wind Tunnel Data: Scale Effects. Boundary Corrections I: Basics and Two- Dimensional Cases. Boundary Corrections II: Three-Dimensional Flow. Boundary Corrections III: Additional Applications. Additional Considerations for Aerodynamic Experiments. Aircraft and Aircraft Components. Ground Vehicles. Marine Vehicles. Wind Engineering. Small Wind Tunnels. Dynamic Tests. Appendices. Index.",35-124,1966.0,http://www.walker.com/list/app/categoriesmain.html,Engineering
1448,f18c8e5970e47d3ea5079412c6e05bcdfc7f84da,Computational Thermodynamics: The Calphad Method,"Phase diagrams are used in materials research and engineering to understand the interrelationship between composition, microstructure and process conditions. In complex systems, computational methods such as CALPHAD are employed to model thermodynamic properties for each phase and simulate multicomponent phase behavior. Written by recognized experts in the field, this is the first introductory guide to the CALPHAD method, providing a theoretical and practical approach. Building on core thermodynamic principles, this book applies crystallography, first principles methods and experimental data to computational phase behavior modeling using the CALPHAD method. With a chapter dedicated to creating thermodynamic databases, the reader will be confident in assessing, optimizing and validating complex thermodynamic systems alongside database construction and manipulation. Several case studies put the methods into a practical context, making this suitable for use on advanced materials design and engineering courses and an invaluable reference to those using thermodynamic data in their research or simulations.",20-139,2007.0,http://davis.biz/tags/listpost.php,Computer Science
1449,a996987a529f6c436f1c348af2ff4c1983b18b23,EPPA: An Efficient and Privacy-Preserving Aggregation Scheme for Secure Smart Grid Communications,"The concept of smart grid has emerged as a convergence of traditional power system engineering and information and communication technology. It is vital to the success of next generation of power grid, which is expected to be featuring reliable, efficient, flexible, clean, friendly, and secure characteristics. In this paper, we propose an efficient and privacy-preserving aggregation scheme, named EPPA, for smart grid communications. EPPA uses a superincreasing sequence to structure multidimensional data and encrypt the structured data by the homomorphic Paillier cryptosystem technique. For data communications from user to smart grid operation center, data aggregation is performed directly on ciphertext at local gateways without decryption, and the aggregation result of the original data can be obtained at the operation center. EPPA also adopts the batch verification technique to reduce authentication cost. Through extensive analysis, we demonstrate that EPPA resists various security threats and preserve user privacy, and has significantly less computation and communication overhead than existing competing approaches.",1621-1631,2012.0,http://patrick.com/explore/posts/postsabout.php,Computer Science
1450,15b43cf0c415aa428b1c7913786cbfc4aded36be,Engineering Design via Surrogate Modelling: A Practical Guide,"Preface. About the Authors. Foreword. Prologue. Part I: Fundamentals. 1. Sampling Plans. 1.1 The 'Curse of Dimensionality' and How to Avoid It. 1.2 Physical versus Computational Experiments. 1.3 Designing Preliminary Experiments (Screening). 1.3.1 Estimating the Distribution of Elementary Effects. 1.4 Designing a Sampling Plan. 1.4.1 Stratification. 1.4.2 Latin Squares and Random Latin Hypercubes. 1.4.3 Space-filling Latin Hypercubes. 1.4.4 Space-filling Subsets. 1.5 A Note on Harmonic Responses. 1.6 Some Pointers for Further Reading. References. 2. Constructing a Surrogate. 2.1 The Modelling Process. 2.1.1 Stage One: Preparing the Data and Choosing a Modelling Approach. 2.1.2 Stage Two: Parameter Estimation and Training. 2.1.3 Stage Three: Model Testing. 2.2 Polynomial Models. 2.2.1 Example One: Aerofoil Drag. 2.2.2 Example Two: a Multimodal Testcase. 2.2.3 What About the k -variable Case? 2.3 Radial Basis Function Models. 2.3.1 Fitting Noise-Free Data. 2.3.2 Radial Basis Function Models of Noisy Data. 2.4 Kriging. 2.4.1 Building the Kriging Model. 2.4.2 Kriging Prediction. 2.5 Support Vector Regression. 2.5.1 The Support Vector Predictor. 2.5.2 The Kernel Trick. 2.5.3 Finding the Support Vectors. 2.5.4 Finding . 2.5.5 Choosing C and epsilon. 2.5.6 Computing epsilon : v -SVR 71. 2.6 The Big(ger) Picture. References. 3. Exploring and Exploiting a Surrogate. 3.1 Searching the Surrogate. 3.2 Infill Criteria. 3.2.1 Prediction Based Exploitation. 3.2.2 Error Based Exploration. 3.2.3 Balanced Exploitation and Exploration. 3.2.4 Conditional Likelihood Approaches. 3.2.5 Other Methods. 3.3 Managing a Surrogate Based Optimization Process. 3.3.1 Which Surrogate for What Use? 3.3.2 How Many Sample Plan and Infill Points? 3.3.3 Convergence Criteria. 3.3.4 Search of the Vibration Isolator Geometry Feasibility Using Kriging Goal Seeking. References. Part II: Advanced Concepts. 4. Visualization. 4.1 Matrices of Contour Plots. 4.2 Nested Dimensions. Reference. 5. Constraints. 5.1 Satisfaction of Constraints by Construction. 5.2 Penalty Functions. 5.3 Example Constrained Problem. 5.3.1 Using a Kriging Model of the Constraint Function. 5.3.2 Using a Kriging Model of the Objective Function. 5.4 Expected Improvement Based Approaches. 5.4.1 Expected Improvement With Simple Penalty Function. 5.4.2 Constrained Expected Improvement. 5.5 Missing Data. 5.5.1 Imputing Data for Infeasible Designs. 5.6 Design of a Helical Compression Spring Using Constrained Expected Improvement. 5.7 Summary. References. 6. Infill Criteria With Noisy Data. 6.1 Regressing Kriging. 6.2 Searching the Regression Model. 6.2.1 Re-Interpolation. 6.2.2 Re-Interpolation With Conditional Likelihood Approaches. 6.3 A Note on Matrix Ill-Conditioning. 6.4 Summary. References. 7. Exploiting Gradient Information. 7.1 Obtaining Gradients. 7.1.1 Finite Differencing. 7.1.2 Complex Step Approximation. 7.1.3 Adjoint Methods and Algorithmic Differentiation. 7.2 Gradient-enhanced Modelling. 7.3 Hessian-enhanced Modelling. 7.4 Summary. References. 8. Multi-fidelity Analysis. 8.1 Co-Kriging. 8.2 One-variable Demonstration. 8.3 Choosing X c and X e . 8.4 Summary. References. 9. Multiple Design Objectives. 9.1 Pareto Optimization. 9.2 Multi-objective Expected Improvement. 9.3 Design of the Nowacki Cantilever Beam Using Multi-objective, Constrained Expected Improvement. 9.4 Design of a Helical Compression Spring Using Multi-objective, Constrained Expected Improvement. 9.5 Summary. References. Appendix: Example Problems. A.1 One-Variable Test Function. A.2 Branin Test Function. A.3 Aerofoil Design. A.4 The Nowacki Beam. A.5 Multi-objective, Constrained Optimal Design of a Helical Compression Spring. A.6 Novel Passive Vibration Isolator Feasibility. References. Index.",43-105,2008.0,https://king-gonzalez.biz/app/wp-content/categorieshomepage.html,Mathematics
1451,911d20ba5972fb5660cdd6c4a8842d635366a577,An evolutionary‐based data mining technique for assessment of civil engineering systems,"Purpose – Analysis of many civil engineering phenomena is a complex problem due to the participation of a large number of factors involved. Traditional methods usually suffer from a lack of physical understanding. Furthermore, the simplifying assumptions that are usually made in the development of the traditional methods may, in some cases, lead to very large errors. The purpose of this paper is to present a new method, based on evolutionary polynomial regression (EPR) for capturing nonlinear interaction between various parameters of civil engineering systems.Design/methodology/approach – EPR is a data‐driven method based on evolutionary computing, aimed to search for polynomial structures representing a system. In this technique, a combination of the genetic algorithm and the least‐squares method is used to find feasible structures and the appropriate constants for those structures.Findings – Capabilities of the EPR methodology are illustrated by application to two complex practical civil engineering pro...",500-517,2008.0,http://page.net/posts/posts/wp-contentterms.php,Computer Science
1452,a9304a952cdcf732a2b771466a7b0efad8e499c8,Data Fusion by Matrix Factorization,"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.",41-53,2013.0,https://newton-robertson.org/tag/search/searchindex.html,Computer Science
1453,0cd130421ed3aa531ebc5932875aca1861f5e26d,Sensing global Birkeland currents with iridium® engineering magnetometer data,"The Iridium system consists of >70 satellites in low altitude, 780km, polar orbits in six equally spaced orbit planes with at least eleven satellites in each plane. Each satellite carries an engineering magnetometer with 48nT resolution. Techniques have been developed at JHU/APL to process Iridium magnetic field data and obtain global maps of magnetic perturbations due to field aligned currents (FACs). The noise level in the processed data is typically 70 to 100 nT and readings above the noise level occur at high magnetic latitudes consistent with an auroral signature. Time series also display well known features characteristic of FACs. Synoptic maps derived using three hours of data from 17 February, 2000, AE ∼100–300nT, show patterns consistent with the Region 1/2 currents previously determined statistically. The Indium data set provides new global measurements of the Birkeland currents in both hemispheres on time scales of a few hours or less.",73-148,2000.0,https://brown.org/main/categoryhome.htm,Physics
1454,4b8c870140a70aba606ebe320fd0be7ebc4abaa9,Genetic algorithms in engineering electromagnetics,"This paper presents a tutorial and overview of genetic algorithms for electromagnetic optimization. Genetic-algorithm (GA) optimizers are robust, stochastic search methods modeled on the concepts of natural selection and evolution. The relationship between traditional optimization techniques and the GA is discussed. Step-by-step implementation aspects of the GA are detailed, through an example with the objective of providing useful guidelines for the potential user. Extensive use is made of sidebars and graphical presentation to facilitate understanding. The tutorial is followed by a discussion of several electromagnetic applications in which the GA has proven useful. The applications discussed include the design of lightweight, broadband microwave absorbers, the reduction of array sidelobes in thinned arrays, the design of shaped-beam antenna arrays, the extraction of natural resonance modes of radar targets from backscattered response data, and the design of broadband patch antennas. Genetic-algorithm optimization is shown to be suitable for optimizing a broad class of problems of interest to the electromagnetic community. A comprehensive list of key references, organized by application category, is also provided.",7-21,1997.0,https://heath.info/blog/mainauthor.htm,Engineering
1455,8cc8af9a9cf9d77f72894a4580db0da7b4efcdef,An approach to engineering the requirements of data warehouses,,49-72,2008.0,http://www.fletcher.biz/search/blog/wp-contentsearch.htm,Computer Science
1456,c017f9ac893ebf4dc48c97415f0f095b18d7c43f,"Data-based mechanistic modelling of environmental, ecological, economic and engineering systems.",,105-122,1998.0,https://james.com/posts/bloghomepage.jsp,Computer Science
1457,982de3bd9da5b6793be3f1221e896621242cbcbf,Engineering applications of the self-organizing map,"The self-organizing map (SOM) method is a new, powerful software tool for the visualization of high-dimensional data. It converts complex, nonlinear statistical relationships between high-dimensional data into simple geometric relationships on a low-dimensional display. As it thereby compresses information while preserving the most important topological and metric relationships of the primary data elements on the display, it may also be thought to produce some kind of abstractions. The term self-organizing map signifies a class of mappings defined by error-theoretic considerations. In practice they result in certain unsupervised, competitive learning processes, computed by simple-looking SOM algorithms. Many industries have found the SOM-based software tools useful. The most important property of the SOM, orderliness of the input-output mapping, can be utilized for many tasks: reduction of the amount of training data, speeding up learning nonlinear interpolation and extrapolation, generalization, and effective compression of information for its transmission.",1358-1384,1996.0,http://alexander-gutierrez.net/blog/tag/tagauthor.jsp,Computer Science
1458,9ed09a07aa1e4a2010a16e687b56f12f4e7df58f,A Model-Driven Goal-Oriented Requirement Engineering Approach for Data Warehouses,,255-264,2007.0,https://www.nelson.info/categories/tag/wp-contentabout.htm,Computer Science
1459,7b7e46eb83df36cb3665a96c23d2547340ed43a9,The Magnetic Electron Ion Spectrometer (MagEIS) Instruments Aboard the Radiation Belt Storm Probes (RBSP) Spacecraft,,383 - 421,2013.0,http://www.wallace-price.biz/posts/wp-contenthome.html,Physics
1460,87f8ad665dd8dae1a65f8c5bfbd91da1c1e70e2d,Big Data: Unleashing information,,127 - 151,2013.0,https://www.bauer.org/tagsprivacy.htm,Computer Science
1461,535489ffb7a3c9a226a593e52f9a27466fa5f9af,Numerical Modeling of Wind Turbine Wakes,"An aerodynamical model for studying three-dimensional flow fields about wind turbine rotors is presented. The developed algorithm combines a three-dimensional Navier-Stokes solver with a so-called actuator line technique in which the loading is distributed along lines representing the blade forces. The loading is determined iteratively using a bladeelement approach and tabulated airfoil data. Computations are carried out for a 500 kW Nordtank wind turbine equipped with three LM19.1 blades. The computations give detailed information about basic features of wind turbine wakes, including distributions of interference factors and vortex structures. The model serves in particular to analyze and verify the validity of the basic assumptions employed in the simple engineering models",393-399,2002.0,https://porter.biz/wp-content/list/appcategory.jsp,Computer Science
1462,854055131cb3f029ccf2ea2f7ce8aa675d5d8f6e,Visual Reverse Engineering of Binary and Data Files,,1-17,2008.0,https://hamilton.info/main/blog/tagspost.htm,Computer Science
1463,ab7d2563a86a1cb51a60b4b70df62ffcbc9d7351,Constraining the metabolic genotype–phenotype relationship using a phylogeny of in silico methods,,291-305,2012.0,https://silva-miller.com/list/mainmain.html,Biology
1464,f6d5bdaa9ea5d90bf94608f68b1c8dd36d1ac843,How to infer gene networks from expression profiles,,78 - 78,2007.0,http://richardson.com/wp-contenthomepage.html,Biology
1465,49d1a61d240300c2485fc08ec2c2bb01d1dff11e,Toward data mining engineering: A software engineering approach,,87-107,2009.0,http://black-lopez.com/wp-contentterms.jsp,Computer Science
1466,651e43a8f88c15e13cb4b1567ec7828410bdcb34,Mobile Communications Engineering,"From the Publisher: 
This authoritative work was written to be the bible for advanced commercial mobile phone systems and military mobile communications systems. Emphasizing how to study and analyze mobile communications problems,the book thoroughly explains how to design a mobile radio communication system. Introductory material describes existing systems,bridging the gap for students unfamiliar with the field. The characteristics of mobile radio signals are covered,applying statistical communication theory to propagation and received signal characteristics. Functional design is introduced,and a discussion of system performance included to explain how to evaluate a new system. The author uses both theory and experiments to support practical design and development of systems in the microwave frequency ranges configured for high-capacity mobile telephone and data communications.",65-120,1982.0,https://www.phelps.com/categories/listlogin.php,Engineering
1467,fa26a55ec872f03388343c7bbde36f19d6abc429,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"One of the main goals of systems biology is reverse engineering network structures from experimental data. Using quantitative networks measures, biological processes can be characterized and understood on a systems level. This paper reviews the scientific literature on the transcriptional regulatory networks of Saccharomyces cerevisiae to discuss what kind of structures can be identified and what this means biologically. This discussion comprises network characteristics, network dynamics in biological processes, and robustness; an inherent emergent property of networks.",16-139,2009.0,https://smith.com/category/blog/wp-contentfaq.asp,Computer Science
1468,4ba01f5f01a3e6cc9f7f08529a49b024148c8843,Developing a data quality framework for asset management in engineering organisations,"Data Quality (DQ) is seen as critical to effective business decision-making. However, maintaining DQ is often acknowledged as problematic. Asset data is the key enabler in gaining control of assets. The quality asset data provides the foundation for effective Asset Management (AM). Researches have indicated that achieving AM DQ is the key challenge engineering organisations face today. This paper investigates the DQ issues emerging from the unique nature of engineering AM data. It presents an exploratory research of a large scale national-wide DQ survey on how Australian engineering organisations address DQ issues, and proposes an AM specific DQ framework.",100-126,2007.0,https://www.stewart-jenkins.info/postshome.php,Business
1469,4b3ecddd09912cb7de31ea2ab77411416a3f33f3,Industrial Adoption of Model-Driven Engineering: Are the Tools Really the Problem?,,1-17,2013.0,http://armstrong-ryan.net/posts/explorefaq.htm,Computer Science
1470,0ffae70637e5ceef913c56ad290d22585bd27d49,Engineering new paths to water data,,753-760,2009.0,https://www.castaneda.info/explore/searchprivacy.htm,Computer Science
1471,7c0d41c9a15842f53629c54f6cbef18dcb96f480,Evolutionary optimization with data collocation for reverse engineering of biological networks,"MOTIVATION
Modern experimental biology is moving away from analyses of single elements to whole-organism measurements. Such measured time-course data contain a wealth of information about the structure and dynamic of the pathway or network. The dynamic modeling of the whole systems is formulated as a reverse problem that requires a well-suited mathematical model and a very efficient computational method to identify the model structure and parameters. Numerical integration for differential equations and finding global parameter values are still two major challenges in this field of the parameter estimation of nonlinear dynamic biological systems.


RESULTS
We compare three techniques of parameter estimation for nonlinear dynamic biological systems. In the proposed scheme, the modified collocation method is applied to convert the differential equations to the system of algebraic equations. The observed time-course data are then substituted into the algebraic system equations to decouple system interactions in order to obtain the approximate model profiles. Hybrid differential evolution (HDE) with population size of five is able to find a global solution. The method is not only suited for parameter estimation but also can be applied for structure identification. The solution obtained by HDE is then used as the starting point for a local search method to yield the refined estimates.","
          1180-8
        ",2005.0,http://caldwell-gilbert.org/list/postshomepage.html,Computer Science
1472,a0af28958edf2bc84285152bc0bed734dfc250a9,On Using Conceptual Data Modeling for Ontology Engineering,,185-207,2004.0,http://www.murray.net/postshomepage.htm,Computer Science
1473,b0d04d05adf63cb635215f142e22f83d48cbb81b,Data mining in software engineering,"The increased availability of data created as part of the software development process allows us to apply novel analysis techniques on the data and use the results to guide the process's optimization. In this paper we describe various data sources and discuss the principles and techniques of data mining as applied on software engineering data. Data that can be mined is generated by most parts of the development process: requirements elicitation, development analysis, testing, debugging, and maintenance. Based on this classification we survey the mining approaches that have been used and categorize them according to the corresponding parts of the development process and the task they assist. Thus the survey provides researchers with a concise overview of data mining techniques applied to software engineering data, and aids practitioners on the selection of appropriate data mining techniques for their work.",413-441,2011.0,http://dixon.com/mainfaq.asp,Computer Science
1474,5677cfc24058078ef513c7d6c99be2d1eb8bf11e,Toyota's Principles of Set-Based Concurrent Engineering,"Not well documented to date, the design and development system of Toyota Motor Corporation contributes greatly to the firm's remarkably consistent growth in market share and its enviable profit per vehicle. This article, which extends the authors' previous study of the Toyota product development system, reports on further data collection in Japan and at the Toyota Technical Center in Michigan. Findings substantiate the authors' previous claims about the product development system and lead them to conclude that Toyota is ""set-based"" in its approaches. 

Set-based concurrent engineering (SBCE) begins by broadly considering sets of possible solutions (in parallel and relatively independently) and gradually narrowing the set of possibilities to converge on a final solution. Gradually eliminating weaker solutions increases the likelihood of finding the best or better solutions. In this way, Toyota can move more quickly toward convergence and production than their traditional, ""point-based"" counterparts. 

The authors develop the SBCE idea by describing three principles that guide Toyota's decision making in design: (1) simultaneous mapping of the design space according to functional expertise, (2) ""integration by intersection"" of mutually acceptable functional refinements introduced by the design and manufacturing engineering groups, and (3) establishment of feasibility before commitment. The authors also present a conceptual framework tied to the Toyota development system and discuss why the SBCE principles lead to highly effective product development.

Findings suggest that a change to a distributed, concurrent engineering environment should involve a corresponding change in design method to a set-based process. Product development organizations able to master and apply SBCE principles and Toyota's principles for integrating systems and cultivating organizational knowledge may be able to radically improve their design and development processes.",67-83,1999.0,http://www.turner.com/tagsindex.jsp,Business
1475,7bbc0a66e843e2ce8c01ea4ed531bd81b37cf3eb,Why Ricker wavelets are successful in processing seismic data: Towards a theoretical explanation,"In many engineering applications ranging from engineering seismology to petroleum engineering and civil engineering, it is important to process seismic data. In processing seismic data, it turns out to be very efficient to describe the signal's spectrum as a linear combination of Ricker wavelet spectra. In this paper, we provide a possible theoretical explanation for this empirical efficiency. Specifically, signal propagation through several layers is discussed, and it is shown that the Ricker wavelet is the simplest non-trivial solution for the corresponding data processing problem, under the condition that the described properties of the approximation family are satisfied.",11-16,2014.0,http://www.moses.net/exploreterms.html,Computer Science
1476,c9c73f5a1668b8bf12aae2efb6ac5a5a2c34002a,"CAP twelve years later: How the ""rules"" have changed","The CAP theorem asserts that any networked shared-data system can have only two of three desirable properties. However, by explicitly handling partitions, designers can optimize consistency and availability, thereby achieving some trade-off of all three. The featured Web extra is a podcast from Software Engineering Radio, in which the host interviews Dwight Merriman about the emerging NoSQL movement, the three types of nonrelational data stores, Brewer's CAP theorem, and much more.",23-29,2012.0,https://www.gill-choi.net/list/list/blogterms.htm,Computer Science
1477,fdd8cc8a0152d09d1e32da6e45196b569abe146a,Data-Driven Continuous Evolution of Smart Systems,"As Marc Andreessen said in his Wall Street Journal OpEd, software is eating the world. The systems that we are building today and in the near future will exhibit levels of autonomy that will put new demands on the engineering of such systems. Although promising examples of autonomous systems exist, there is no established methodology for systematically building autonomous systems that employ modern software engineering technology such as continuous deployment and data-driven engineering. The contribution of this paper is twofold. First, it identifies and presents the challenge of continuous evolution of autonomous systems as a well-defined problem that needs to be addressed by software engineering research. Second, it presents a conceptual solution to this problem that integrates the development of new software for autonomous systems by R&D teams with systematic experimentation by autonomous systems.",28-34,2016.0,https://weiss-smith.com/main/appindex.htm,Computer Science
1478,f9e74ce54eaa219b08333f9c129a736dd0619863,"A case study of multiservice, multipriority traffic engineering design for data networks","We present techniques, some of which are novel, for traffic engineering in QoS-supported data networks, and also illustrate the application of these techniques in a case study. In the interest of scalability all these techniques use multicommodity flow (MCF) solution techniques as primitives. The techniques address the design of topology and size of explicit routes in MPLS-supported IP networks and VPNs. The techniques are for network-wide optimization subject to constraints on routing imposed by end-to-end QoS and other considerations. The notion of admissible route sets specific to service class and source-destination pair is used to differentiate QoS constraints on real-time services, such as Internet telephony and video, and relatively delay insensitive services, such as premium data. Contrasting optimization techniques are given for services, such as best effort, for which no restriction on routes are imposed. Another important traffic engineering requirement addressed is priorities, for which an efficient and accurate design technique is given.",1077-1083 vol. 1b,1999.0,https://blackburn.com/category/list/appprivacy.jsp,Computer Science
1479,ec9196b9edd46c5c59124ca776ca71538655fbc6,Empirical Data Modeling in Software Engineering Using Radical Basis Functions,"Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.",567-576,2000.0,https://www.fritz-edwards.com/explore/main/searchfaq.php,Computer Science
1480,60cef66236eee4cc7c124abfa04d27cbc379362f,Missing Data in Software Engineering,,185-200,2008.0,https://www.scott-carrillo.com/tags/tag/categorieshome.php,Computer Science
1481,c2841ad5e94943d4fde4e7b059ffb51783922fca,The meaningful use of big data: four perspectives -- four challenges,"Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.",56-60,2012.0,https://thomas.com/list/blog/appindex.html,Computer Science
1482,5ed1d6261984992fea16a4a35ec331c0916bdb3c,Interpolation and Approximation,,435-483,2010.0,http://www.lewis.com/category/posts/wp-contentauthor.htm,Geology
1483,726cb75a8a10b915f463cb45a6dbaf266eabf7b4,Applications of data mining in software engineering,"Software engineering processes are complex, and the related activities often produce a large number and variety of artefacts, making them well-suited to data mining. Recent years have seen an increase in the use of data mining techniques on such artefacts with the goal of analysing and improving software processes for a given organisation or project. After a brief survey of current uses, we offer insight into how data mining can make a significant contribution to the success of current software engineering efforts.",243-257,2010.0,http://holmes.com/wp-contentmain.php,Computer Science
1484,12358b92076419f2b147fdabda5c3d8c1cfa02a4,Better library design: data‐driven protein engineering,"Data‐driven protein engineering is increasingly used as an alternative to rational design and combinatorial engineering because it uses available knowledge to limit library size, while still allowing for the identification of unpredictable substitutions that lead to large effects. Recent advances in computational modeling and bioinformatics, as well as an increasing databank of experiments on functional variants, have led to new strategies to choose particular amino acid residues to vary in order to increase the chances of obtaining a variant protein with the desired property. Strategies for limiting diversity at each position, design of small sub‐libraries, and the performance of scouting experiments, have also been developed or even automated, further reducing the library size.",57-122,2007.0,https://williams.com/search/explore/exploreterms.php,Medicine
1485,d6cc07dcf7d76a216c2dbe50b716e29d450141ee,Design Data for Engineering Ceramics: A Review of the Flexure Test,"The uniaxial strength of engineering ceramics is often measured by the well-known flexure strength test method there is a risk that flexure data are not representative of the properties of fabricated components. Reliability estimates for components based upon statistical extrapolation techniques from flexure data may not be valid. This paper reviews the problem and judges the usefulness of flexure data for design purposes. It is shown that some of the limitations of flexure data apply; to other modes of testing, including direct tension testing",2037-2066,1991.0,http://evans-michael.com/list/blogprivacy.html,Computer Science
1486,0166d107c091e2ea0c0d2ea172f48ab010677e4f,Anatomy of STEM teaching in North American universities,"Lecture is prominent, but practices vary A large body of evidence demonstrates that strategies that promote student interactions and cognitively engage students with content (1) lead to gains in learning and attitudinal outcomes for students in science, technology, engineering, and mathematics (STEM) courses (1, 2). Many educational and governmental bodies have called for and supported adoption of these student-centered strategies throughout the undergraduate STEM curriculum. But to the extent that we have pictures of the STEM undergraduate instructional landscape, it has mostly been provided through self-report surveys of faculty members, within a particular STEM discipline [e.g., (3–6)]. Such surveys are prone to reliability threats and can underestimate the complexity of classroom environments, and few are implemented nationally to provide valid and reliable data (7). Reflecting the limited state of these data, a report from the U.S. National Academies of Sciences, Engineering, and Medicine called for improved data collection to understand the use of evidence-based instructional practices (8). We report here a major step toward a characterization of STEM teaching practices in North American universities based on classroom observations from over 2000 classes taught by more than 500 STEM faculty members across 25 institutions.",1468 - 1470,2018.0,http://www.hunter.com/posts/posts/tagauthor.php,Medicine
1487,558d8f66074cc791b5a6504354ac1280e3a2ebf1,Mining Software Engineering Data,"Software engineering data (such as code bases, exe- cution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project¿s status, progress, and evolution. Using well- established data mining techniques, practitioners and re- searchers can explore the potential of this valuable data in order to better manage their projects and to produce higher-quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining Soft- ware Engineering (SE) data, discusses challenges associ- ated with mining SE data, highlights SE data mining suc- cess stories, and outlines future research directions. Partic- ipants will acquire knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice.",172-173,2007.0,http://jones.org/tags/search/listmain.php,Engineering
1488,388d8d15920fd7994bfe40b9247a8213e06e7a16,Engineering Privacy by Design,"The design and implementation of privacy requirements in systems is a difficult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns. “Privacy by design” consists of a number of principles that can be applied from the onset of systems development to mitigate privacy concerns and achieve data protection compliance. However, these principles remain vague and leave many open questions about their application when engineering systems. In this paper we show how starting from data minimization is a necessary and foundational first step to engineer systems in line with the principles of privacy by design. We first discuss what data minimization can mean from a security engineering perspective. We then present a summary of two case studies in which privacy is achieved by minimizing different types of data, according to the purpose of each application. First, we present a privacypreserving ePetition system, in which user’s privacy is guaranteed by hiding their identity from the provider while revealing their votes. Secondly, we study a road tolling system, in which users have to be identified for billing reasons and data minimization is applied to protect further sensitive information (in this case location information). The case studies make evident that the application of data minimization does not necessarily imply anonymity, but may also be achieved by means of concealing information related to identifiable individuals. In fact, different kinds of data minimization are possible, and each system requires careful crafting of data minimization best suited for its purpose. Most importantly, the two case studies underline that the interpretation of privacy by design principles requires specific engineering expertise, contextual analysis, and a balancing of multilateral security and privacy interests. They show that privacy by design is a productive space in which there is no one way of solving the problems. Based on our analysis of the two case studies, we argue that engineering systems according to the privacy by design principles requires the development of generalizable methodologies that build upon the principle of data minimization. However, the complexity of this engineering task demands caution against reducing such methodologies to “privacy by design check lists” that can easily be ticked away for compliance reasons while not mitigating some of the risks that privacy by design is meant to address.",54-128,2011.0,http://www.ramirez.biz/wp-content/listhome.php,Computer Science
1489,0a2b49f51fd6b65e26c458fdd3592f88b63a3b08,Applications of genome-scale metabolic reconstructions,,320 - 320,2009.0,http://wheeler.org/tags/blogterms.php,Biology
1490,d6cce4139471cf88035748f03e1a6a1df1ab9140,Evaluating the Relative Performance of Engineering Design Projects: A Case Study Using Data Envelopment Analysis,"This paper presents a case study of how Data Envelopment Analysis (DEA) was applied to generate objective cross-project comparisons of project duration within an engineering department of the Belgian Armed Forces. To date, DEA has been applied to study projects within certain domains (e.g., software and R&D); however, DEA has not been proposed as a general project evaluation tool within the project management literature. In this case study, we demonstrate how DEA fills a gap not addressed by commonly applied project evaluation methods (such as earned value management) by allowing the objective comparison of projects on actual measures, such as duration and cost, by explicitly considering differences in key input characteristics across these projects. Thus, DEA can overcome the paradigm of project uniqueness and facilitate cross-project learning. We describe how DEA allowed the department to gain new insight about the impact of changes to its engineering design process (redesigned based on ISO 15288), creating a performance index that simultaneously considers project duration and key input variables that determine project duration. We conclude with directions for future research on the application of DEA as a project evaluation tool for project managers, program office managers, and other decision-makers in project-based organizations",471-482,2006.0,https://www.james-cohen.com/postsindex.jsp,Engineering
1491,24f6cb1a80792138c856cc8f759672d29eaa4fb7,GreenDCN: A General Framework for Achieving Energy Efficiency in Data Center Networks,"The popularization of cloud computing has raised concerns over the energy consumption that takes place in data centers. In addition to the energy consumed by servers, the energy consumed by large numbers of network devices emerges as a significant problem. Existing work on energy-efficient data center networking primarily focuses on traffic engineering, which is usually adapted from traditional networks. We propose a new framework to embrace the new opportunities brought by combining some special features of data centers with traffic engineering. Based on this framework, we characterize the problem of achieving energy efficiency with a time-aware model, and we prove its NP-hardness with a solution that has two steps. First, we solve the problem of assigning virtual machines (VM) to servers to reduce the amount of traffic and to generate favorable conditions for traffic engineering. The solution reached for this problem is based on three essential principles that we propose. Second, we reduce the number of active switches and balance traffic flows, depending on the relation between power consumption and routing, to achieve energy conservation. Experimental results confirm that, by using this framework, we can achieve up to 50 percent energy savings. We also provide a comprehensive discussion on the scalability and practicability of the framework.",4-15,2013.0,http://malone.com/category/bloghome.html,Computer Science
1492,c5b885b9b777933d351d621b1ecd3ccd44b6f93c,READ: Reverse Engineering of Automotive Data Frames,"Security analytics and forensics applied to in-vehicle networks are growing research areas that gained relevance after recent reports of cyber-attacks against unmodified licensed vehicles. However, the application of security analytics algorithms and tools to the automotive domain is hindered by the lack of public specifications about proprietary data exchanged over in-vehicle networks. Since the controller area network (CAN) bus is the de-facto standard for the interconnection of automotive electronic control units, the lack of public specifications for CAN messages is a key issue. This paper strives to solve this problem by proposing READ: a novel algorithm for the automatic Reverse Engineering of Automotive Data frames. READ has been designed to analyze traffic traces containing unknown CAN bus messages in order to automatically identify and label different types of signals encoded in the payload of their data frames. Experimental results based on CAN traffic gathered from a licensed unmodified vehicle and validated against its complete formal specifications demonstrate that the proposed algorithm can extract and classify more than twice the signals with respect to the previous related work. Moreover, the execution time of signal extraction and classification is reduced by two orders of magnitude. Applications of READ to CAN messages generated by real vehicles demonstrate its usefulness in the analysis of CAN traffic.",1083-1097,2003.0,http://www.sharp-collins.com/tagsfaq.htm,Engineering
1493,440d228c4ee2a52a1564738105867f50c8bea733,Multivariate Hawkes processes: an application to financial data,"A Hawkes process is also known under the name of a self-exciting point process and has numerous applications throughout science and engineering. We derive the statistical estimation (maximum likelihood estimation) and goodness-of-fit (mainly graphical) for multivariate Hawkes processes with possibly dependent marks. As an application, we analyze two data sets from finance.",367 - 378,2011.0,http://diaz-garcia.info/tags/searchfaq.php,Mathematics
1494,1fb42852a47f77d26efdbacbf75cddcb294b5c3d,Deep-STORM: super-resolution single-molecule microscopy by deep learning,"We present an ultra-fast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically-blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities, and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking data-set. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.",95-150,2018.0,https://www.frazier-hartman.com/listmain.html,Computer Science
1495,bae9848263949626961936b90565d4b5b407c00c,Recent Techniques of Clustering of Time Series Data: A Survey,"Time-Series clustering is one of the important concepts of data mining that is used to gain insight into the mechanism that generate the time-series and predicting the future values of the given time-series. Time-series data are frequently very large and elements of these kinds of data have temporal ordering. The clustering of time series is organized into three groups depending upon whether they work directly on raw data either in frequency or time domain, indirectly with the features extracted from the raw data or with model built from raw data. In this paper, we have shown the survey and summarization of previous work that investigated the clustering of time series in various application domains ranging from science, engineering, business, finance, economic, health care, to government.",1-9,2012.0,https://howell.net/explore/main/categorypost.php,Computer Science
1496,070eb848983b6447cbc701c8213b7fb021dab527,Social Cognitive Predictors of Academic Interests and Goals in Engineering: Utility for Women and Students at Historically Black Universities.,"This study examined the utility of social cognitive career theory (SCCT; R. W. Lent, S. D. Brown, & G. Hackett, 1994) in predicting engineering interests and major choice goals among women and men and among students at historically Black and predominantly White universities. Participants (487 students in introductory engineering courses at 3 universities) completed measures of academic interests, goals, self-efficacy, outcome expectations, and environmental supports and barriers in relation to engineering majors. Findings indicated that the SCCT-based model of interest and choice goals produced good fit to the data across gender and university type. Implications for future research on SCCT's choice hypotheses, and particularly for the role of environmental supports and barriers in the choice of science and engineering fields, are discussed.",84-92,2005.0,https://www.atkinson.com/tags/searchcategory.html,Psychology
1497,b5fd36b9492a4682a4dd5f7454425c5c42b7e322,Engineering Complex Embedded Systems with State Analysis and the Mission Data System,"It has become clear that spacecraft system complexity is reaching a threshold where customary methods of control are no longer affordable or sufficiently reliable. At the heart of this problem are the conventional approaches to systems and software engineering based on subsystem-level functional decomposition, which fail to scale in the tangled web of interactions typically encountered in complex spacecraft designs. Furthermore, there is a fundamental gap between the requirements on software specified by systems engineers and the implementation of these requirements by software engineers. Software engineers must perform the translation of requirements into software code, hoping to accurately capture the systems engineer's understanding of the system behavior, which is not always explicitly specified. This gap opens up the possibility for misinterpretation of the systems engineer s intent, potentially leading to software errors. This problem is addressed by a systems engineering methodology called State Analysis, which provides a process for capturing system and software requirements in the form of explicit models. This paper describes how requirements for complex aerospace systems can be developed using State Analysis and how these requirements inform the design of the system software, using representative spacecraft examples.",507-536,2004.0,https://www.marquez.com/list/app/postsabout.php,Computer Science
1498,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",23 - 44,2008.0,http://wright.com/categories/listregister.php,Computer Science
1499,2114c0768c341b82fd22ce3df5a0aab1cf37134f,Information requirements engineering for data warehouse systems,"Information requirements analysis for data warehouse systems differs significantly from requirements analysis for conventional information systems. Based on interviews with project managers and information systems managers, requirements for a methodological support of information requirements analysis for data warehouse systems are derived. Existing approaches are reviewed with regard to these requirements. Using the method engineering approach, a comprehensive methodology that supports the entire process of determining information requirements of data warehouse users, matching information requirements with actual information supply, evaluating and homogenizing resulting information requirements, establishing priorities for unsatisfied information requirements, and formally specifying the results as a basis for subsequent phases of the data warehouse development (sub)project has been proposed. The most important sources for methodology components were four in-depth case studies of information requirements analysis practices observed in data warehousing development projects of large organizations. In this paper, these case studies are presented and the resulting consolidated methodology is summarized. While an application of the proposed methodology in its entirety is still outstanding, its components have been successfully applied in actual data warehouse development projects.",1359-1365,2004.0,https://lee-george.biz/explore/categoryfaq.php,Computer Science
1500,7fc9a268aeebfa25b77a784fb47d0959523cff00,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",171 - 179,2016.0,https://williams.com/categories/mainabout.html,Chemistry
1501,948fd800ecdd3c99488dde36b41480ca1b8acce3,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",D442 - D450,2018.0,https://white.com/tags/wp-contentprivacy.html,Computer Science
1502,95cd83603a0d2b6918a8e34a5637a8f382da96f5,"MIMIC-III, a freely accessible critical care database",,71-143,2016.0,http://www.green-wright.com/posts/explore/postsmain.htm,Computer Science
1503,da692ee969d9c33986196372c3f7cb87fa6b6f8f,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",D8 - D13,2017.0,https://www.ali.com/explore/list/exploreindex.php,Medicine
1504,98128fd412ebfa90201a276f2c59020ccc696a75,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",D1074 - D1082,2017.0,https://www.davis-anthony.net/main/tags/tagindex.html,Medicine
1505,6e1e6afb314f9c5a24d744252a30aa5efc313571,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",D362 - D368,2016.0,https://www.gonzalez.com/categories/blogauthor.php,Biology
1506,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",1613 - 1617,2017.0,https://www.kelly.com/tagmain.asp,Biology
1507,0f5c63182b5d40850c741888a89e6c055a3593af,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",D279 - D285,2015.0,https://johnson.com/posts/categoriesindex.php,Computer Science
1508,16b0744424f02e01fe2f01b3ea03e2862f1359fc,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",D733 - D745,2015.0,https://www.smith.com/tags/wp-content/exploreprivacy.html,Computer Science
1509,f986968735459e789890f24b6b277b0920a9725d,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",1452-1464,2018.0,http://www.doyle.biz/search/postscategory.html,Computer Science
1510,93d5369a0be3134c6018373d5290923f3d718815,The Molecular Signatures Database Hallmark Gene Set Collection,,417-425,2015.0,http://www.snyder-pace.com/explore/categoryprivacy.htm,Biology
1511,d2c733e34d48784a37d717fe43d9e93277a8c53e,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",248-255,2009.0,https://www.brewer-beard.biz/tags/tag/appindex.php,Computer Science
1512,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.","
          3389-402
        ",1997.0,https://nguyen-larsen.net/list/posts/listprivacy.htm,Biology
1513,7f19972754ac0c15329666b3a6efbf569b27d8d5,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",D427 - D432,2018.0,http://holmes-dawson.com/tagspost.asp,Computer Science
1514,3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2019. The number of submitted datasets to PRIDE Archive (the archival component of PRIDE) has reached on average around 500 datasets per month during 2021. In addition to continuous improvements in PRIDE Archive data pipelines and infrastructure, the PRIDE Spectra Archive has been developed to provide direct access to the submitted mass spectra using Universal Spectrum Identifiers. As a key point, the file format MAGE-TAB for proteomics has been developed to enable the improvement of sample metadata annotation. Additionally, the resource PRIDE Peptidome provides access to aggregated peptide/protein evidences across PRIDE Archive. Furthermore, we will describe how PRIDE has increased its efforts to reuse and disseminate high-quality proteomics data into other added-value resources such as UniProt, Ensembl and Expression Atlas.",D543 - D552,2021.0,http://kennedy.com/tags/blog/categoriesterms.htm,Computer Science
1515,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",D412 - D419,2020.0,https://sandoval.com/searchprivacy.html,Medicine
1516,b204970b0503a923359bff532726666f5e0e971b,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",D590 - D596,2012.0,http://www.swanson.com/tags/tags/exploreauthor.asp,Computer Science
1517,3aca912f21d54b3931fa1fdfac0c199c557374a4,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,1925 - 1927,2019.0,https://www.fernandez-camacho.com/app/wp-content/wp-contentterms.htm,Medicine
1518,633f318876c41fed36b3905b8af5fdc27f734615,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.","
          417-425
        ",2015.0,http://www.wagner.net/blog/categorymain.jsp,Medicine
1519,a98753021c6a076a5307f4dfb7fd1fcb14089910,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",D211 - D222,2007.0,http://miller-petersen.com/blogprivacy.php,Medicine
1520,68c03788224000794d5491ab459be0b2a2c38677,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",39-41,1995.0,https://medina.info/appabout.php,Computer Science
1521,d87ceda3042f781c341ac17109d1e94a717f5f60,Book Reviews: WordNet: An Electronic Lexical Database,"WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: ""Nouns in WordNet"" by George Miller, ""Modifiers in WordNet"" by Katherine Miller, ""A semantic network of English verbs"" by Christiane Fellbaum, and ""Design and implementation of the WordNet lexical database and search software"" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as ""extensions, enhancements and",53-124,1999.0,http://cisneros-kline.net/wp-content/tagsfaq.asp,Technology
1522,54cc1f2e86d1913521b466cef19d72ed02b6c800,Argonaute—a database for gene regulation by mammalian microRNAs,,D115 - D118,2005.0,https://clark.biz/wp-content/app/wp-contentlogin.html,Medicine
1523,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",D490 - D495,2013.0,https://www.roman-lopez.biz/list/category/categorieslogin.html,Biology
1524,6a074a3fa856e86b2e6bc60e83d66cc488090ae9,The ecoinvent database version 3 (part I): overview and methodology,,1218-1230,2016.0,http://www.wilson-moore.com/list/categoryabout.asp,Computer Science
1525,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.","
          716-21
        ",2012.0,http://www.howard.net/apphomepage.asp,Biology
1526,66470cf9df2f932f80094a309abcc14bcc1b9373,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",D447 - D456,2015.0,https://www.fitzgerald-yu.com/category/wp-content/tagsregister.htm,Biology
1527,bbe6e5fcc96e685db714d6aa11ffe6f49567c585,A global database of COVID-19 vaccinations,,947 - 953,2021.0,http://www.johnson.org/appfaq.asp,Medicine
1528,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.","
          380-8
        ",2002.0,https://gray.biz/mainmain.htm,Materials Science
1529,c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",16-126,2008.0,http://carter.info/categories/tags/apphome.asp,Computer Science
1530,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",D517 - D525,2019.0,https://warren-burns.com/list/list/maincategory.html,Medicine
1531,8b3b8848a311c501e704c45c6d50430ab7068956,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",2556-2563,2011.0,http://ellis-ali.com/categorieshome.html,Computer Science
1532,317325439a0ce543d7629848a35adea04b6e7d12,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",D344 - D354,2020.0,https://marsh.org/wp-content/main/tagsabout.html,Medicine
1533,f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.","
          1739-40
        ",2011.0,https://rodriguez.com/explore/blogfaq.php,Computer Science
1534,716000409a3a2e2c75801b3d58b9b17b68eeaef7,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",64-135,2005.0,https://butler.info/categories/wp-contentmain.php,Environmental Science
1535,76eb8e5688ee2951e5f04fb14956abf93a890149,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",D233 - D238,2008.0,https://velazquez.org/tag/bloglogin.jsp,Biology
1536,80e394ee3e1834091596e8b55c9ad9bf11456e09,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,R60 - R60,2003.0,https://www.williams-gordon.org/bloghomepage.php,Biology
1537,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",D141 - D145,2008.0,http://www.riley.info/wp-content/categoriesterms.asp,Biology
1538,1976c9eeccc7115d18a04f1e7fb5145db6b96002,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",1247-1250,2008.0,http://www.ferguson-singh.org/wp-content/tag/listauthor.php,Computer Science
1539,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.","
          5383-92
        ",2002.0,https://hester-vance.biz/wp-content/categoryprivacy.htm,Medicine
1540,298d799da82395a64a3bda38ef9d2a4646828ccb,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",212-219,1996.0,http://www.green.com/categoryterms.asp,Computer Science
1541,dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,22-104,2005.0,http://obrien.com/tagshomepage.jsp,Geology
1542,80777d42513103bede188b2eebbdce7fb6f91390,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",D608 - D617,2017.0,http://pierce-morales.com/categories/appregister.asp,Computer Science
1543,5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,,"
          536-40
        ",1995.0,https://www.chase.com/search/explorecategory.htm,Computer Science
1544,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",D561 - D568,2010.0,http://tanner-marshall.com/tag/main/bloghomepage.html,Biology
1545,761020759f7e9f84c3ac77f59a42862cc6a6004e,Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems,"cont.): Active database semantics can be supported on an existing SQL Server (we use Oracle SQL Server as the test SQL Server) by the ECA Agent between the SQL Server and multiple clients. ECA rules are completely supported through the ECA Agent without changing applications in the SQL Server. Both primitive and composite events can be detected in the ECA Agent and actions are invoked in SQL Server. All events are persistent in RDBMS. The Java Local Event Detector (Java LED) is used to notify and detect both primitive events and composite events. The ECA Agent uses Java Database Connectivity (JDBC) to connect to the SQL server. The architecture of the ECA Agent and implementation details are shown in this thesis. Alternative approaches are discussed in details, and the features and limitations are identified. Database and Expert Systems Applications This book contains the refereed proceedings of the 8th International Conference on Database and Expert Systems Applications, DEXA '97, held in Toulouse, France, September 1997. The 62 revised full papers presented in the book, together with three invited contributions, were selected from a total of 159 submissions. The papers are organized in sections on modeling, object-oriented databases, active and temporal aspects, images, integrity constraints, multimedia databases, deductive databases and knowledge-based systems, allocation concepts, data interchange, digital libraries, transaction concepts, learning issues, optimization and performance, query languages, maintenance, Page 3/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems fed rated databases, uncertainty handling and qualitative reasoning, and software engineering and reusable software. Advanced Database Systems ""Focused on the latest research on text and document management, this guide addresses the information management needs of organizations by providing the most recent findings. How the need for effective databases to house information is impacting organizations worldwide and how some organizations that possess a vast amount of data are not able to use the data in an economic and efficient manner is demonstrated. A taxonomy for object-oriented databases, metrics for controlling database complexity, and a guide to accommodating hierarchies in relational databases are provided. Also covered is how to apply Java-triggers for X-Link management and how to build signatures."" Flexible and Efficient Information Handling This book is the proceedings of a workshop held at Heriot-Watt University in Edinburgh in August 1993. The central theme of the workshop was rules in database systems, and the papers presented covered a range of different aspects of database rule systems. These aspects are reflected in the sessions of the workshop, which are the same as the sections in this proceedings: Active Databases Architectures Incorporating Temporal Rules Rules and Transactions Analysis and Debugging of Active Rules Integrating Graphs/Objects with Deduction Integrating Deductive and Active Rules Integrity Constraints Deductive Databases The incorporation of rules into database systems is an important area of research, as it is a major component in the integration of behavioural information with the structural data with which commercial databases have traditionally been associated. This integration of the behavioural aspects of an application with the data to which it applies in database systems leads to more straightforward application development and more efficient processing of data. Many novel applications seem to need database systems in which structural and behavioural information are fully integrated. Rules are only one means of expressing behavioural information, but it is clear that different types of rule can be used to capture directly different properties of an application which are cumbersome to support using conventional database architectures. In recent years there has been a surge of research activity focusing upon active database systems, and this volume opens with a collection of papers devoted specifically to this topic. Web Information Systems -WISE 2004 Active database systems enhance traditional database functionality with powerful rule-processing capabilities, providing a uniform and efficient mechanism for many database system applications. Among these applications are integrity constraints, views, authorization, statistics gathering, monitoring and alerting, knowledge-based systems, expert systems, and workflow management. This significant collection focuses on the most prominent research projects in active database systems. The project leaders for each Page 4/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems prototype system provide detailed discussion of their projects and the relevance of their results to the future of active database systems. Features: A broad overview of current active database systems and how they can be extended and improved A comprehensive introduction to the core topics of the field, including its motivation and history Coverage of active database (trigger) capabilities in commercial products Discussion of forthcoming standards Knowledge-Based Intelligent Information and Engineering Systems Knowledge Base Systems are an integration of conventional database systems with Artificial Intelligence techniques. They provide inference capabilities to the database system by encapsulating the knowledge of the application domain within the database. Knowledge is the most valuable of all corporate resources that must be captured, stored, re-used and continuously improved, in much the same way as database systems were important in the previous decade. Flexible, extensible, and yet efficient Knowledge Base Systems are needed to capture the increasing demand for knowledge-based applications which will become a significant market in the next decade. Knowledge can be expressed in many static and dynamic forms; the most prominent being domain objects, their relationships, and their rules of evolution and transformation. It is important to express and seamlessly use all types of knowledge in a single Knowledge Base System. Parallel, Object-Oriented, and Active Knowledge Base Systems presents in detail features that a Knowledge Base System should have in order to fulfill the above requirements. Parallel, Object-Oriented, and Active Knowledge Base Systems covers in detail the following topics: Integration of deductive, production, and active rules in sequential database systems. Integration and inter-operation of multiple rule types into the same Knowledge Base System. Parallel rule matching and execution, for deductive, production, and active rules, in parallel Export, Knowledge Base, and Database Systems. In-depth description of a Parallel, Object-Oriented, and Active Knowledge Base System that integrates all rule paradigms into a single database system without hindering performance. Parallel, Object-Oriented, and Active Knowledge Base Systems is intended as a graduate-level text for a course on Knowledge Base Systems and as a reference for researchers and practitioners in the areas of database systems, knowledge base systems and Artificial Intelligence. Active, Real-Time, and Temporal Database Systems The World Wide Web has become a ubiquitous global tool, used for finding infor mation, communicating ideas, carrying out distributed computation and conducting business, learning and science. The Web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology and content are evolv ing. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The Web's users are highly diverse and can access the Web from a variety of devices and interfaces, at different places and times, and for varying purposes. We thus also need techniques for personalising the presentation and content of Page 5/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems Web based information dependi g on how it i being accessed and on the specific user's requirements. As well as being accessed by human users, the Web is also accessed by appli cations. New applications in areas such as ebusiness, sensor networks, and mobile and ubiquitous computing need to be able to detect and react quickly to events and changes in Web-based information. Traditional approaches using query-based 'pull' of information to find out if events or changes of interest have occurred may not be able to scale to the quantity and frequency of events and changes being generated, and new 'push' -based techniques are needed. Advances in Databases and Information Systems This book constitutes the strictly refereed post-workshop proceedings of the International Workshop on Logic in Databases, LID'96, held in San Miniato, Italy, in July 1996, as the final meeting of an EC-US cooperative activity. The volume presents 21 revised full papers selected from 49 submissions as well as 3 invited contributions and a summary of a panel discussion on deductive databases: challenges, opportunities and future directions. The retrospective survey on logic and databases by Jack Minker deserves a special mention: it is a 56-page overview and lists 357 references. The papers are organized in sections on uncertainty, temporal and spatial reasoning, updates, active databases, semantics, advanced applications, query evaluation, language extensions, and logic constructs and expressive power. Data Management Systems This book constitutes the ",56-147,2022.0,http://nguyen.com/main/tags/exploreprivacy.asp,Technology
1546,cdad2f8ca559f425ab7fa402535354a86b0a370a,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",95-116,2019.0,https://jenkins.org/explore/tags/categoryhomepage.asp,Computer Science
1547,6dd9508b8311852afec88bce55283551da5aa7b7,The IPD-IMGT/HLA Database,"Abstract It is 24 years since the IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, was first released, providing the HLA community with a searchable repository of highly curated HLA sequences. The database now contains over 35 000 alleles of the human Major Histocompatibility Complex (MHC) named by the WHO Nomenclature Committee for Factors of the HLA System. This complex contains the most polymorphic genes in the human genome and is now considered hyperpolymorphic. The IPD-IMGT/HLA Database provides a stable and user-friendly repository for this information. Uptake of Next Generation Sequencing technology in recent years has driven an increase in the number of alleles and the length of sequences submitted. As the size of the database has grown the traditional methods of accessing and presenting this data have been challenged, in response, we have developed a suite of tools providing an enhanced user experience to our traditional web-based users while creating new programmatic access for our bioinformatics user base. This suite of tools is powered by the IPD-API, an Application Programming Interface (API), providing scalable and flexible access to the database. The IPD-API provides a stable platform for our future development allowing us to meet the future challenges of the HLA field and needs of the community.",D1053 - D1060,2022.0,https://ho-ingram.com/mainregister.htm,Medicine
1548,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.","
          308-11
        ",2001.0,https://www.mooney-morris.com/app/explore/wp-contentsearch.asp,Medicine
1549,d7c78b7071ea150346320e5b43a03824263e0fa9,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",D633 - D642,2013.0,http://stokes.net/app/categorycategory.htm,Biology
1550,82524ddee00fa0895dfca43995a7ec8bdb16f0d5,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",63-117,1989.0,https://robbins.com/categoriesterms.php,Computer Science
1551,d364903a626ad70e6ce057209d9b7e004dafd4be,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.","
          325-7
        ",2002.0,http://www.wade-lee.com/tags/categoriesauthor.htm,Biology
1552,b7599c8ba88e7c93edbce57df513152e8f5693e7,The COG database: an updated version includes eukaryotes,,41 - 41,2003.0,http://www.medina.com/listfaq.html,Medicine
1553,1f53996347086be3bd3a32da0976ba2db7687988,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",D127 - D131,2019.0,https://eaton.org/categories/postshome.html,Computer Science
1554,0e466ea033b982519f351022304dccb64a46b93c,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",D948 - D955,2019.0,https://griffin.com/blog/list/appfaq.html,Computer Science
1555,bb967168ead7a14adcb0121dcf24a930d1a383b3,The HITRAN 2008 molecular spectroscopic database,,139-204,2005.0,http://www.snyder.com/explore/categorieslogin.asp,Computer Science
1556,4bd970a37c59c97804ff93cbb2c108e081de3a37,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",235-244,1990.0,https://www.burns.net/wp-content/blog/categoryregister.jsp,Computer Science
1557,6ad9053940676fca029dabdc7937e5d854df61e0,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",D290 - D301,2011.0,https://www.smith.net/list/posts/categoryregister.htm,Medicine
1558,62f5ffb09a4c9543509c38f005b9c6eb308c6974,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.","
          33-6
        ",2000.0,https://www.leon.com/categories/appabout.htm,Biology
1559,95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,"Repbase Update, a database of repetitive elements in eukaryotic genomes",,74-146,2015.0,https://www.miller.com/wp-content/tag/listhomepage.asp,Biology
1560,072a0db716fb6f8332323f076b71554716a7271c,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",45-50,2001.0,http://www.hanna.com/blog/listregister.html,Engineering
1561,9667f8264745b626c6173b1310e2ff0298b09cfc,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",487-495,2014.0,http://www.davis-rodriguez.com/list/appauthor.php,Computer Science
1562,41abf43dc718e271299457bce65bccfe3feeb9d6,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",5069 - 5072,2006.0,https://www.reid.net/categoryindex.html,Medicine
1563,c369c9a40a36b013be3ef9c19a068f2d6578e4c3,Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",90-130,2006.0,https://werner-rivera.org/postsprivacy.html,Computer Science
1564,288b317e427c6bf4c94d455049bd1368ff2071eb,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",D339 - D343,2018.0,https://jones.com/searchfaq.htm,Medicine
1565,3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,,13 - 13,2014.0,https://www.russell.com/tagpost.html,Computer Science
1566,5a2892f91addeea2f4600d28b23e684be32f5b2c,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",18-31,2012.0,https://white.com/blog/exploresearch.html,Computer Science
1567,46f74231b9afeb0c290d6d550043c55045284e5f,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",141-142,2012.0,https://www.harrell.com/app/taghomepage.php,Computer Science
1568,908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",3485-3492,2010.0,https://wilson-gonzales.com/tags/appfaq.jsp,Computer Science
1569,092c275005ae49dc1303214f6d02d134457c7053,LabelMe: A Database and Web-Based Tool for Image Annotation,,157-173,2008.0,http://lee.com/category/tagmain.asp,Computer Science
1570,fc1e3ed87c15d62148f03ff99677e6be0fc6f5b1,The carbohydrate-active enzyme database: functions and literature,"Abstract Thirty years have elapsed since the emergence of the classification of carbohydrate-active enzymes in sequence-based families that became the CAZy database over 20 years ago, freely available for browsing and download at www.cazy.org. In the era of large scale sequencing and high-throughput Biology, it is important to examine the position of this specialist database that is deeply rooted in human curation. The three primary tasks of the CAZy curators are (i) to maintain and update the family classification of this class of enzymes, (ii) to classify sequences newly released by GenBank and the Protein Data Bank and (iii) to capture and present functional information for each family. The CAZy website is updated once a month. Here we briefly summarize the increase in novel families and the annotations conducted during the last 8 years. We present several important changes that facilitate taxonomic navigation, and allow to download the entirety of the annotations. Most importantly we highlight the considerable amount of work that accompanies the analysis and report of biochemical data from the literature.",D571 - D577,2021.0,https://evans.org/blog/wp-content/categoryabout.html,Computer Science
1571,90bc0ca3feebe0215079cf575b90017170a0089f,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D222 - D226,2014.0,https://www.cross-wilson.com/wp-content/tags/tagabout.htm,Medicine
1572,e8104b335a4e499f7b79b913a92d23263c82f6b6,The HITRAN2012 molecular spectroscopic database,,81-140,2013.0,https://www.rodriguez-george.net/list/tag/wp-contentsearch.html,Technology
1573,cb56121bc38e0f4b44bcb5296a12038626152e96,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",D566 - D573,2016.0,https://www.clayton-cruz.com/category/exploremain.html,Computer Science
1574,960e7494ef4ec5964407488080f249104cd218f0,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed® database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 34 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface and NCBI datasets. Additional resources that were updated in the past year include PMC, Bookshelf, Genome Data Viewer, SRA, ClinVar, dbSNP, dbVar, Pathogen Detection, BLAST, Primer-BLAST, IgBLAST, iCn3D and PubChem. All of these resources can be accessed through the NCBI home page at https://www.ncbi.nlm.nih.gov.",90-107,2020.0,https://www.simmons.info/poststerms.php,Computer Science
1575,6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,The AR face database,,57-122,1998.0,http://carter.biz/mainmain.asp,Computer Science
1576,d71af418eeb9f5a68062929bae12af74773ffcb2,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",D945 - D954,2016.0,http://www.shaw-ortiz.com/appabout.php,Medicine
1577,3a2b869533620d2dfa076522321983c537b3c175,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.","
          D258-61
        ",2004.0,https://lee.biz/main/searchhome.htm,Medicine
1578,a8db50edfe26a6ae33a6787e2049de5bacd18666,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",D1100 - D1107,2011.0,http://www.hill.com/app/appmain.php,Computer Science
1579,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D225 - D229,2010.0,http://lewis.com/app/wp-contentregister.php,Biology
1580,b307d55ba07058d6183991d2d2a81b340d558186,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",D501 - D504,2004.0,http://lopez.net/category/tagsmain.asp,Computer Science
1581,61533dd9e41f20e2f5deaf22afb04c94b4071eac,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",462 - 467,2005.0,http://martin.com/wp-content/categoryabout.html,Biology
1582,f8928221d290a9cdd84d1de52e121373bc836caa,New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",165-176,2001.0,http://www.carter.biz/main/listabout.asp,Economics
1583,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.","
          297-300
        ",1999.0,https://www.pratt.net/search/blog/taghome.html,Computer Science
1584,5cf0d213f3253cd46673d955209f8463db73cc51,IEMOCAP: interactive emotional dyadic motion capture database,,335-359,2008.0,https://www.novak-schneider.com/tag/search/categoriesprivacy.asp,Computer Science
1585,50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.","
          177-82
        ",2005.0,http://www.moran.net/wp-content/listindex.htm,Computer Science
1586,804836b8ad86ef8042e3dcbd45442a52f031ee03,A Database and Evaluation Methodology for Optical Flow,,1-31,2007.0,http://www.hayes.com/list/app/tagauthor.php,Computer Science
1587,e274c1b6e17825feab52de205fd0bc4917d5be6c,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,,289-302,1999.0,https://santiago.net/exploreauthor.php,Chemistry
1588,58a63e11a45dfdb50d994454ead70243626d8ed1,Encyclopedia of Database Systems,,84-123,2020.0,https://ward.com/postsabout.jsp,Technology
1589,e3f2391513693647e0ea87bfa86cd89e468f51d0,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",46-53,2000.0,https://brown.com/searchauthor.htm,Computer Science
1590,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",D801 - D807,2012.0,http://cervantes.biz/appterms.htm,Biology
1591,73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,The Gene Expression Omnibus Database,,"
          93-110
        ",2016.0,https://www.salazar.com/tagsprivacy.htm,Computer Science
1592,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",D529 - D541,2018.0,https://www.meza.org/tagsauthor.php,Medicine
1593,ea35cd7fd2c46f86232f21ef73239f34f2d180a6,Database resources of the National Center for Biotechnology Information.,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 35 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface, a sequence database search and a gene orthologs page. Additional resources that were updated in the past year include PMC, Bookshelf, My Bibliography, Assembly, RefSeq, viral genomes, the prokaryotic genome annotation pipeline, Genome Workbench, dbSNP, BLAST, Primer-BLAST, IgBLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",25-121,2019.0,https://www.ward.org/wp-contentcategory.html,Medicine
1594,eb960b5d56ed1368991eaa4f40cb7afee66edb1f,ONCOMINE: a cancer microarray database and integrated data-mining platform.,,"
          1-6
        ",2004.0,https://black.org/list/categoryhomepage.jsp,Biology
1595,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",D624 - D632,2017.0,http://castaneda-espinoza.com/searchregister.php,Biology
1596,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",D380 - D386,2017.0,https://www.miller-davis.com/blog/mainprivacy.asp,Biology
1597,dc8b25e35a3acb812beb499844734081722319b4,The FERET database and evaluation procedure for face-recognition algorithms,,295-306,1998.0,http://mitchell.com/explore/wp-content/listhome.asp,Computer Science
1598,f89df7381ea8febb419fae473725e44931f6b22c,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",,341 - 341,2009.0,http://miller-burns.info/listcategory.htm,Medicine
1599,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",D1087 - D1093,2015.0,http://lopez.net/wp-content/apphome.htm,Medicine
1600,f89df7381ea8febb419fae473725e44931f6b22c,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",,341 - 341,2009.0,https://www.humphrey-silva.com/wp-content/list/blogabout.htm,Medicine
1601,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",D1087 - D1093,2015.0,http://www.stout.net/blog/explore/categorycategory.htm,Medicine
1602,dd31f1439a0b80cb9447a112347836b3325e953e,The Standardized World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of significantly reduced comparability across observations. The goal of the Standardized World Income Inequality Database (SWIID) is to overcome these limitations. A custom missing-data algorithm was used to standardize the United Nations University's World Income Inequality Database and data from other sources; data collected by the Luxembourg Income Study served as the standard. The SWIID provides comparable Gini indices of gross and net income inequality for 173 countries for as many years as possible from 1960 to the present along with estimates of uncertainty in these statistics. By maximizing comparability for the largest possible sample of countries and years, the SWIID is better suited to broadly cross-national research on income inequality than previously available sources. 
 
In any papers or publications that use the SWIID, authors are asked to cite the article of record for the data set and give the version number as follows: 
 
Solt, Frederick. 2009. ""Standardizing the World Income Inequality Database."" Social Science Quarterly 90(2):231-242. SWIID Version 3.1, December 2011.",1267-1281,2016.0,http://www.pierce.info/categoriesmain.php,Economics
1603,3f376a9b2d659e52c98d911a1fb3aa3a834f66e5,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (Bookshelf, PubMed Central (PMC) and PubReader); medical genetics (ClinVar, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen); genes and genomics (BioProject, BioSample, dbSNP, dbVar, Epigenomics, Gene, Gene Expression Omnibus (GEO), Genome, HomoloGene, the Map Viewer, Nucleotide, PopSet, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser, Trace Archive and UniGene); and proteins and chemicals (Biosystems, COBALT, the Conserved Domain Database (CDD), the Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB), Protein Clusters, Protein and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for many of these databases. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of these resources can be accessed through the NCBI home page at http://www.ncbi.nlm.nih.gov.",D6 - D17,2014.0,https://www.turner.biz/search/categoriesindex.asp,Medicine
1604,439a453090e28f0858ad5ba0765cc2eeffb23626,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",53-148,1999.0,http://smith-schaefer.com/tag/explorecategory.htm,Business
1605,2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",2621 - 2632,2004.0,https://www.powers.com/tags/tagshome.html,Biology
1606,2ecbb9d6c6e698dd51134e081fa836319801ae27,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",,38-106,2018.0,https://young.biz/blogabout.html,Computer Science
1607,41e2692d9ac1434d1841d5a29e7ccb927b82b677,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",D948 - D954,2018.0,https://www.elliott.net/postsabout.jsp,Medicine
1608,01297b19ec00f5487a522a573ff6e0a9aeac4f05,Database,,432 - 432,1985.0,https://www.williams-jones.biz/explorehomepage.htm,Technology
1609,026668472fd8f0fa2ca710ce276be35d362637c2,"Federated database systems for managing distributed, heterogeneous, and autonomous databases","A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.",183-236,1990.0,http://jackson.com/category/category/apphome.htm,Computer Science
1610,685db71ce0715ddf1127d72ee991c9ba9c8f89ba,Human Mortality Database,,60-129,2019.0,http://www.smith-davis.com/wp-contentfaq.php,Geography
1611,3765df816dc5a061bc261e190acc8bdd9d47bec0,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",1377 - 1388,2010.0,https://christian-castro.com/posts/listmain.php,Psychology
1612,137832dd10d669a300b7751e0ed3e3b91172372a,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",D411 - D414,2005.0,https://www.henderson-carter.com/listpost.html,Medicine
1613,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",D423 - D431,2014.0,http://potter.com/list/explore/tagsterms.jsp,Computer Science
1614,1cf4a6954b419b5478c96119fc1e79aa90f87dea,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.","
          662-71
        ",2014.0,https://stanley.org/main/tags/appfaq.php,Engineering
1615,d2272dd9ff850edc448f7fde86eef6bcd57af2cc,Development of a global land cover characteristics database and IGBP DISCover from 1 km AVHRR data,"Researchers from the U.S. Geological Survey, University of Nebraska-Lincoln and the European Commission's Joint Research Centre, Ispra, Italy produced a 1 km resolution global land cover characteristics database for use in a wide range of continental-to global-scale environmental studies. This database provides a unique view of the broad patterns of the biogeographical and ecoclimatic diversity of the global land surface, and presents a detailed interpretation of the extent of human development. The project was carried out as an International Geosphere-Biosphere Programme, Data and Information Systems (IGBP-DIS) initiative. The IGBP DISCover global land cover product is an integral component of the global land cover database. DISCover includes 17 general land cover classes defined to meet the needs of IGBP core science projects. A formal accuracy assessment of the DISCover data layer will be completed in 1998. The 1 km global land cover database was developed through a continent-by-continent unsupervised classification of 1 km monthly Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) composites covering 1992-1993. Extensive post-classification stratification was necessary to resolve spectral/temporal confusion between disparate land cover types. The complete global database consists of 961 seasonal land cover regions that capture patterns of land cover, seasonality and relative primary productivity. The seasonal land cover regions were aggregated to produce seven separate land cover data sets used for global environmental modelling and assessment. The data sets include IGBP DISCover, U.S. Geological Survey Anderson System, Simple Biosphere Model, Simple Biosphere Model 2, Biosphere-Atmosphere Transfer Scheme, Olson Ecosystems and Running Global Remote Sensing Land Cover. The database also includes all digital sources that were used in the classification. The complete database can be sourced from the website: http://edcwww.cr.usgs.gov/landdaac/glcc/glcc.html.",1303 - 1330,2000.0,https://www.taylor-ramos.org/categories/tag/listterms.php,Geography
1616,107cbf209b1bd25c7bfc75882347a2655da05118,An international database for pesticide risk assessments and management,"ABSTRACT Despite a changing world in terms of data sharing, availability, and transparency, there are still major resource issues associated with collating datasets that will satisfy the requirements of comprehensive pesticide risk assessments, especially those undertaken at a regional or national scale. In 1996, a long-term project was initiated to begin collating and formatting pesticide data to eventually create a free-to-all repository of data that would provide a comprehensive transparent, harmonized, and managed extensive dataset for all types of pesticide risk assessments. Over the last 20 years, this database has been keeping pace with improving risk assessments, their associated data requirements, and the needs and expectations of database end users. In 2007, the Pesticide Properties DataBase (PPDB) was launched as a free-to-access website. Currently, the PPDB holds data for almost 2300 pesticide active substances and over 700 metabolites. For each substance around 300 parameters are stored, covering human health, environmental quality, and biodiversity risk assessments. With the approach of the twentieth anniversary of the database, this article seeks to elucidate the current data model, data sources, its validation, and quality control processes and describes a number of existing risk assessment applications that depend upon it.",1050 - 1064,2016.0,http://davis.com/explore/postsauthor.asp,Business
1617,2157f202c8c89d924dd4da4d1bcf92d16fcd8893,"Image database TID2013: Peculiarities, results and perspectives",,57-77,2015.0,https://jackson.com/tags/appfaq.php,Computer Science
1618,3ef0c7784bf446de5ce5977a35f86c8b30fd668f,Optimal database combinations for literature searches in systematic reviews: a prospective exploratory study,,34-101,2017.0,http://www.shah.com/posts/tag/categoriespost.php,Medicine
1619,60258897d250a41f11cfee27de828a0130110b5e,The CELEX Lexical Database (CD-ROM),,88-123,1996.0,http://www.hester-hansen.net/tagauthor.asp,Computer Science
1620,e4beeff8cf47dcc0faf6efc8f4c1b3fefc052afb,A database of German emotional speech,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",1517-1520,2005.0,http://smith.biz/app/search/searchfaq.php,Computer Science
1621,e57683f3eea6176441230c2b30bec2fda4984697,The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies,,15010,2015.0,http://mcknight.com/explore/main/listlogin.jsp,Physics
1622,3ff0d2c7621c40e6245c7ca0964b4b856255e0c2,"Development and validation of a global database of lakes, reservoirs and wetlands",,1-22,2004.0,http://www.hays.net/searchhomepage.html,Environmental Science
1623,b92ed5d8103faf9ccdfd5d3c4f60b722866038d0,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",3207 - 3209,2016.0,https://miller.info/tagsterms.html,Computer Science
1624,0414f6ffc086bf6c2015176d4b46a051d436cd2b,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",D801 - D806,2018.0,http://www.johnson.com/tags/bloglogin.html,Computer Science
1625,8257167186837ff6840d6c2f552b4d23ff26ec81,The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans.,"PURPOSE
The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.


METHODS
Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (""nodule > or =3 mm,"" ""nodule <3 mm,"" and ""non-nodule > or =3 mm""). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.


RESULTS
The Database contains 7371 lesions marked ""nodule"" by at least one radiologist. 2669 of these lesions were marked ""nodule > or =3 mm"" by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings.


CONCLUSIONS
The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice.","
          915-31
        ",2011.0,http://oconnor.com/list/wp-contenthome.php,Medicine
1626,58a93e9cd60ce331606d31ebed62599a2b7db805,The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/","
          45-8
        ",2000.0,https://patel.com/appterms.html,Medicine
1627,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",D1075 - D1079,2015.0,http://www.reid-kemp.com/tags/category/categoriesregister.htm,Medicine
1628,2aa2bbe5d567fdf997e96418e1deb5df48cb6715,FRED-MD: A Monthly Database for Macroeconomic Research,"This article describes a large, monthly frequency, macroeconomic database with the goal of establishing a convenient starting point for empirical analysis that requires “big data.” The dataset mimics the coverage of those already used in the literature but has three appealing features. First, it is designed to be updated monthly using the Federal Reserve Economic Data (FRED) database. Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. Third, it will relieve researchers from having to manage data changes and revisions. We show that factors extracted from our dataset share the same predictive content as those based on various vintages of the so-called Stock–Watson dataset. In addition, we suggest that diffusion indexes constructed as the partial sum of the factor estimates can potentially be useful for the study of business cycle chronology. Supplementary materials for this article are available online.",574 - 589,2015.0,https://silva-rodgers.com/explore/category/listfaq.html,Computer Science
1629,4c987ffb492e44acc010cbeb2347b92e257d7b59,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",D521 - D526,2007.0,https://watts-taylor.com/app/wp-content/blogcategory.html,Computer Science
1630,839c069fc576c816b89d84aa7c18849874de486a,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible resource for metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are experimentally determined, small-molecule metabolic pathways and are curated from the primary scientific literature. With more than 1400 pathways, MetaCyc is the largest collection of metabolic pathways currently available. Pathways reactions are linked to one or more well-characterized enzymes, and both pathways and enzymes are annotated with reviews, evidence codes, and literature citations. BioCyc (BioCyc.org) is a collection of more than 500 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the full genome and predicted metabolic network of one organism. The network, which is predicted by the Pathway Tools software using MetaCyc as a reference, consists of metabolites, enzymes, reactions and metabolic pathways. BioCyc PGDBs also contain additional features, such as predicted operons, transport systems, and pathway hole-fillers. The BioCyc Web site offers several tools for the analysis of the PGDBs, including Omics Viewers that enable visualization of omics datasets on two different genome-scale diagrams and tools for comparative analysis. The BioCyc PGDBs generated by SRI are offered for adoption by any party interested in curation of metabolic, regulatory, and genome-related information about an organism.",D473 - D479,2009.0,https://leon.biz/posts/search/blogpost.htm,Medicine
1631,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",D633 - D639,2017.0,https://berry.com/tagshomepage.jsp,Computer Science
1632,07b58c8bb6b9084fba84d464f5324b7933720665,Principles of Distributed Database Systems,,22-140,1990.0,https://harris.com/app/tagindex.html,Computer Science
1633,e4b52a1a00e9db941326fc857b95245cbfb60bce,Reactome graph database: Efficient access to complex pathway data,"Reactome is a free, open-source, open-data, curated and peer-reviewed knowledgebase of biomolecular pathways. One of its main priorities is to provide easy and efficient access to its high quality curated data. At present, biological pathway databases typically store their contents in relational databases. This limits access efficiency because there are performance issues associated with queries traversing highly interconnected data. The same data in a graph database can be queried more efficiently. Here we present the rationale behind the adoption of a graph database (Neo4j) as well as the new ContentService (REST API) that provides access to these data. The Neo4j graph database and its query language, Cypher, provide efficient access to the complex Reactome data model, facilitating easy traversal and knowledge discovery. The adoption of this technology greatly improved query efficiency, reducing the average query time by 93%. The web service built on top of the graph database provides programmatic access to Reactome data by object oriented queries, but also supports more complex queries that take advantage of the new underlying graph-based data storage. By adopting graph database technology we are providing a high performance pathway data resource to the community. The Reactome graph database use case shows the power of NoSQL database engines for complex biological data types.",66-130,2018.0,https://www.aguilar-ryan.com/categorylogin.html,Computer Science
1634,118cdcf302328a5896de4adbb1e2554641212091,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",D369 - D379,2016.0,http://www.walton.com/search/listfaq.jsp,Medicine
1635,f4b3598f49fbb81c0ea21b8130bbdd5133403efc,Using the National Cancer Database for Outcomes Research: A Review,"Importance The National Cancer Database (NCDB), a joint quality improvement initiative of the American College of Surgeons Commission on Cancer and the American Cancer Society, has created a shared research file that has changed the study of cancer care in the United States. A thorough understanding of the nuances, strengths, and limitations of the database by both readers and investigators is of critical importance. This review describes the use of the NCDB to study cancer care, with a focus on the advantages of using the database and important considerations that affect the interpretation of NCDB studies. Observations The NCDB is one of the largest cancer registries in the world and has rapidly become one of the most commonly used data resources to study the care of cancer in the United States. The NCDB paints a comprehensive picture of cancer care, including a number of less commonly available details that enable subtle nuances of treatment to be studied. On the other hand, several potentially important patient and treatment attributes are not collected in the NCDB, which may affect the extent to which comparisons can be adjusted. Finally, the NCDB has undergone several significant changes during the past decade that may affect its completeness and the types of available data. Conclusions and Relevance The NCDB offers a critically important perspective on cancer care in the United States. To capitalize on its strengths and adjust for its limitations, investigators and their audiences should familiarize themselves with the advantages and shortcomings of the NCDB, as well as its evolution over time.",1722–1728,2017.0,https://ellis.net/main/listsearch.html,Medicine
1636,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",D213 - D221,2014.0,https://www.edwards.com/blogterms.php,Biology
1637,82940092149703c4639169d8761dd907674a5b4d,Outpatient antibiotic use in Europe and association with resistance: a cross-national database study,,579-587,2005.0,https://www.williams.com/posts/tagsregister.php,Medicine
1638,ca6ac75d2408d9fbc3ff49e8d62341821574337b,SYFPEITHI: database for MHC ligands and peptide motifs,,213-219,1999.0,https://www.taylor-humphrey.com/tag/searchlogin.html,Biology
1639,683bc72f18d2eede7d4c46c55537cbce63a47f62,Tmbase-A database of membrane spanning protein segments,,166-166,1993.0,http://mccarthy.info/listsearch.asp,Biology
1640,62a134740314b4469c83c8921ae2e1beea22b8f5,A Database for Handwritten Text Recognition Research,"An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >",550-554,1994.0,http://spence-williams.com/app/tags/tagsabout.html,Computer Science
1641,c75ba6ef724c0c3a9c9510a70da4cc8729b59a35,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",413-425,2014.0,https://thompson-smith.com/categorieshome.jsp,Computer Science
1642,d90293c06067d1a3718b4fb8c167285bfd099702,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",1666 - 1670,2014.0,https://www.martin.org/tagregister.jsp,Biology
1643,3bbc9400429ad3d6bda6d12e4449053afa1114a2,The Comprehensive Antibiotic Resistance Database,"ABSTRACT The field of antibiotic drug discovery and the monitoring of new antibiotic resistance elements have yet to fully exploit the power of the genome revolution. Despite the fact that the first genomes sequenced of free living organisms were those of bacteria, there have been few specialized bioinformatic tools developed to mine the growing amount of genomic data associated with pathogens. In particular, there are few tools to study the genetics and genomics of antibiotic resistance and how it impacts bacterial populations, ecology, and the clinic. We have initiated development of such tools in the form of the Comprehensive Antibiotic Research Database (CARD; http://arpcard.mcmaster.ca). The CARD integrates disparate molecular and sequence data, provides a unique organizing principle in the form of the Antibiotic Resistance Ontology (ARO), and can quickly identify putative antibiotic resistance genes in new unannotated genome sequences. This unique platform provides an informatic tool that bridges antibiotic resistance concerns in health care, agriculture, and the environment.",3348 - 3357,2013.0,https://davis-nelson.com/categories/main/appregister.php,Medicine
1644,2ea5d8555fe5efb7b35bc40e55ead122cfac03b4,TRY – a global database of plant traits,"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy‐in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log‐normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait‐based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.",2905 - 2935,2011.0,http://www.martin-campbell.info/main/tag/wp-contentabout.jsp,Biology
1645,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",866-883,1996.0,http://www.russell.com/apppost.jsp,Computer Science
1646,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",D130 - D137,2014.0,http://olsen-schultz.com/listterms.html,Biology
1647,7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d,"The CMU Pose, Illumination, and Expression Database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",1615-1618,2003.0,http://www.chambers.net/mainpost.html,Computer Science
1648,448752b56fe4b2fc8fb15f22d9430c17aa306392,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",231-234,2014.0,https://www.robinson-lopez.com/tag/categories/listmain.asp,Computer Science
1649,9a7508c7aa498cb738d3c16d35e06a662168106b,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",D670 - D681,2014.0,https://rodriguez.com/searchabout.html,Computer Science
1650,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",D470 - D478,2014.0,http://gallagher.com/explorepost.php,Computer Science
1651,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.","
          51-4
        ",2003.0,http://www.johnson.com/appterms.php,Medicine
1652,e45a8d7176bd738c1e63de1f6791a88e704f8b4b,Universal database search tool for proteomics,,5277 - 5277,2014.0,http://www.stevenson-liu.biz/app/wp-content/bloghome.html,Computer Science
1653,fa69e5e6d21650daa286ed936af9c53d1aca00df,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.","
          258-61
        ",2003.0,https://ryan-gonzalez.info/tag/search/postsprivacy.php,Medicine
1654,429c25227c2225447fd3bf3d17582a19671cc872,The PLANTS Database,,29-118,2015.0,https://www.carter.info/app/tagabout.php,Biology
1655,2e5701b71ccf3352b30b584c2e48fdc307376385,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",D405 - D412,2014.0,http://www.chandler-escobar.biz/wp-content/bloghomepage.jsp,Computer Science
1656,66fb37fee3250f68d598e45b3097b50045edc499,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.","
          389-97
        ",2002.0,http://www.medina.com/searchlogin.php,Medicine
1657,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",D372 - D379,2015.0,https://www.potter.com/listfaq.asp,Computer Science
1658,00bc156bac2b39fab1689fe047b23c9f216d7f29,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",D35 - D40,2004.0,http://rangel-hall.com/search/explorehome.html,Biology
1659,269b9e182815a5d805cfa1f5c1137763b0b2b7cf,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",98-114,2008.0,https://www.white-holland.com/category/list/searchhomepage.php,Medicine
1660,89c433d6a7160bba49ef164fa93c3036d9f7ac5e,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",D471 - D480,2015.0,http://cummings-gonzalez.biz/tag/tags/categoriesabout.html,Computer Science
1661,dc0e51b870dba980b79a3c34a044e491e7cfd5c4,CHIANTI - an atomic database for emission lines - I. Wavelengths greater than 50 Å,"CHIANTI consists of a critically evaluated set of atomic data and transition probabilities necessary to calculate the emission line spectrum of astrophysical plasmas. The data consist of atomic energy levels, atomic radiative data such as wavelengths, weighted oscillator strengths and A values, and electron collisional excitation rates. A set of programs that use these data to calculate the spectrum in a desired wavelength range as a function of temperature and density is also provided. A suite of programs has been developed to carry out plasma diagnostics of astrophysical plasmas. The state-of-the-art contents of the CHIANTI database will be described and some of the most important results obtained from the use of the CHIANTI database will be reviewed.",149-173,1997.0,https://www.frazier.net/search/search/mainauthor.jsp,Physics
1662,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",D876 - D882,2010.0,http://www.smith.org/blog/blog/mainmain.htm,Medicine
1663,b7e54a5d6149cde03fd5144d38a0647d25a795f6,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",D1083 - D1090,2013.0,https://garrett-gomez.biz/categoryfaq.htm,Biology
1664,20740b17fd91394cfdf17ebf1c227312d6bb56cb,The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.","
          D449-51
        ",2004.0,http://www.ray-smith.biz/categories/postslogin.php,Computer Science
1665,88fc814fc568fdfa4704ba2326eb623e95621bec,Notes on CEPII’s Distances Measures: The GeoDist Database,"GeoDist makes available the exhaustive set of gravity variables used in Mayer and Zignago (2005). GeoDist provides several geographical variables, in particular bilateral distances measured using citylevel data to assess the geographic distribution of population inside each nation. We have calculated different measures of bilateral distances available for most countries across the world (225 countries in the current version of the database). For most of them, different calculations of “intra-national distances” are also available. The GeoDist webpage provides two distinct files: a country-specific one (geo_cepii)and a dyadic one (dist_cepii) including a set of different distance and common dummy variables used in gravity equations to identify particular links between countries such as colonial past, common languages, contiguity. We try to improve upon the existing similar datasets in terms of geographical coverage, quality of measurement and number of variables provided.",46-103,2011.0,http://cameron.info/categoriessearch.htm,Geography
1666,5995a2b4645bd73cd4b19d3c44d6a96382d5c880,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",D700 - D705,2011.0,http://cantrell-lang.biz/wp-content/searchterms.php,Computer Science
1667,7fa10285fd3532aea2e64d0d6de06608266b4366,An Overview of the Global Historical Climatology Network-Daily Database,"AbstractA database is described that has been designed to fulfill the need for daily climate data over global land areas. The dataset, known as Global Historical Climatology Network (GHCN)-Daily, was developed for a wide variety of potential applications, including climate analysis and monitoring studies that require data at a daily time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset contains records from over 80 000 stations in 180 countries and territories, and its processing system produces the official archive for U.S. daily data. Variables commonly include maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two-thirds of the stations report precipitation only. Quality assurance checks are routinely applied to the full dataset, but the data are not homogenized to account for artifacts associated with the various eras in reporting practice at any particular station (i.e., for changes in systematic...",897-910,2012.0,https://www.cox-garza.com/appregister.htm,Physics
1668,082bc77513862f8d709322916f44d6fe2f2d06d7,Spanner: Google's Globally-Distributed Database,"Spanner is Google’s scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.",251-264,2012.0,http://www.tucker.com/search/wp-content/listmain.php,Computer Science
1669,fa154947515cd58c0a273bb170bc6dc8a8c99847,Active fault database of Turkey,,3229-3275,2018.0,http://www.smith.com/tagsprivacy.jsp,Geology
1670,b89f0e4f43570688dd983813c9a3efa2fa7e7ebc,"The CMU Pose, Illumination, and Expression (PIE) database","Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.",53-58,2002.0,http://www.meyers-pierce.com/main/wp-contentterms.html,Computer Science
1671,475bbf493d8246031a5152c8005a5c567231307c,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.","
          1045-52
        ",2007.0,http://www.moss-ball.com/mainindex.jsp,Computer Science
1672,d5985ad8d754187dc36c4e2a221086f8530fe372,Completion of the 2006 National Land Cover Database for the conterminous United States.,,858-864,2011.0,https://ferguson-johnson.com/explore/blog/explorehome.htm,Geography
1673,31dbfb575ef802a89a5c08e632f42265bcf30684,High Resolution XPS of Organic Polymers: The Scienta ESCA300 Database,Description of the spectrometer x-ray source monochromator electron lens hemispherical analyser multichannel detector sample analysis chamber charge compensation performance on conducting samples performance on insulating samples performance on testing of the spectrometer experimental protocol sample mounting data acquisition correction of binding energy scale for sample charging curve fitting lineshapes shake-up structure valence bands impurities x-ray degradation organization of the database list of polymers and acronyms the database appendix 1 - primary C 1s shifts appendix 2 - secondary C 1s shifts appendix 3.1 - 0 1s binding energies in CHO polymers appendix 3.2 - 0 1s binding energies in other polymers appendix 4 - N 1s binding energies appendix 5 - F 1s binding energies appendix 6 - binding energies and spin-orbit constants for core-line doublets apendix 7 - binding energies of peaks appearing in the valence band region.,60-147,1992.0,http://rosario.com/postspost.asp,Chemistry
1674,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",D1063 - D1069,2012.0,https://blake.com/categories/wp-contentauthor.html,Computer Science
1675,87752bbb228c597e12cc7e86d9dba4539a04769e,An Overview of the China Meteorological Administration Tropical Cyclone Database,"AbstractThe China Meteorological Administration (CMA)’s tropical cyclone (TC) database includes not only the best-track dataset but also TC-induced wind and precipitation data. This article summarizes the characteristics and key technical details of the CMA TC database. In addition to the best-track data, other phenomena that occurred with the TCs are also recorded in the dataset, such as the subcenters, extratropical transitions, outer-range severe winds associated with TCs over the South China Sea, and coastal severe winds associated with TCs landfalling in China. These data provide additional information for researchers. The TC-induced wind and precipitation data, which map the distribution of severe wind and rainfall, are also helpful for investigating the impacts of TCs. The study also considers the changing reliability of the various data sources used since the database was created and the potential causes of temporal and spatial inhomogeneities within the datasets. Because of the greater number of ...",287-301,2014.0,https://white.info/searchterms.htm,Environmental Science
1676,9162a7e9434022c2ed6f249b129d6e50b90eb1a3,Biological control of insect pests by insect parasitoids and predators: the BIOCAT database.,"The structure of the BIOCAT database, which contains records of the introductions of insect natural enemies for the control of insect pests worldwide, and is now available online, is explained. It is a useful summary of biological control effort and a guide to factors which may influence the success of introduction programmes, but is not detailed enough for making firm predictions.",61-134,1992.0,http://www.reilly-robles.com/main/search/appabout.html,Biology
1677,16cf42b0481042514a983e48b9994d60145553f1,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",D211 - D215,2008.0,https://www.mcgee.biz/mainprivacy.htm,Biology
1678,34284e452a59db6a3d0a867cb01626151a7781b4,The Dartmouth Stellar Evolution Database,"The ever-expanding depth and quality of photometric and spectroscopic observations of stellar populations increase the need for theoretical models in regions of age-composition parameter space that are largely unexplored at present. Stellar evolution models that employ the most advanced physics and cover a wide range of compositions are needed to extract the most information from current observations of both resolved and unresolved stellar populations. The Dartmouth Stellar Evolution Database is a collection of stellar evolution tracks and isochrones that spans a range of [Fe/H] from –2.5 to +0.5, [α/Fe] from –0.2 to +0.8 (for [Fe/H] ⩽ 0) or +0.2 (for [Fe/H] > 0), and initial He mass fractions from Y = 0.245 to 0.40. Stellar evolution tracks were computed for masses between 0.1 and 4 M☉, allowing isochrones to be generated for ages as young as 250 Myr. For the range in masses where the core He flash occurs, separate He-burning tracks were computed starting from the zero age horizontal branch. The tracks and isochrones have been transformed to the observational plane in a variety of photometric systems including standard UBV(RI)C, Stromgren uvby, SDSS ugriz, 2MASS JHKs, and HST ACS/WFC and WFPC2. The Dartmouth Stellar Evolution Database is accessible through a Web site at http://stellar.dartmouth.edu/~models/ where all tracks, isochrones, and additional files can be downloaded.",89 - 101,2008.0,https://www.perez.com/appabout.php,Physics
1679,f49052eeb5607062941137c9468ff1122a19e016,Cochrane Database of Systematic Reviews,"Here, we bring readers extracts from the latest issues of The Cochrane Database of Systematic Reviews relevant to the fields of psychiatry and neurology. We feature a summary of the results, reviewers' conclusions, and implications for clinical practice and research, from selected new reviews featured in issues 6 and 7, 2011. For further information, visit www.thecochranelibrary.com. Copyright © 2011 Wiley Interface Ltd",83-122,2006.0,http://www.small-martin.biz/tag/blog/categoriesterms.html,Medicine
1680,789ae8da65d2b26be12a8a2dcba51073cea41182,Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD),,1501-1509,2013.0,https://www.rogers-smith.com/app/categories/wp-contentauthor.html,Materials Science
1681,1c3588499908417312d172f1b5102e76a90fbb2c,"HITEMP, the high-temperature molecular spectroscopic database",,2139-2150,2010.0,https://www.robinson.com/tagpost.html,Materials Science
1682,0e17d9327501603b39200c5eb9db886de1581a09,Systemic Banking Crises Database,,225-270,2013.0,https://www.wilson-king.com/tagprivacy.htm,Technology
1683,7400d5e1d211294ee2147355e863287f3cbad0a5,PhagesDB: the actinobacteriophage database,"The Actinobacteriophage Database (PhagesDB) is a comprehensive, interactive, database-backed website that collects and shares information related to the discovery, characterization and genomics of viruses that infect Actinobacterial hosts. To date, more than 8000 bacteriophages-including over 1600 with sequenced genomes-have been entered into the database. PhagesDB plays a crucial role in organizing the discoveries of phage biologists around the world-including students in the SEA-PHAGES program-and has been cited in over 50 peer-reviewed articles.


Availability and Implementation
http://phagesdb.org/.


Contact
gfh@pitt.edu.","
          784-786
        ",2017.0,http://www.myers.com/tags/tags/mainregister.html,Computer Science
1684,3a19b36d7f41ba20a507dee2585fd56fa019167a,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,,17-23,2007.0,https://russell.com/main/wp-content/categorylogin.php,Biology
1685,d1de096375c58d18e167b0f2324f0288750b7411,NGA-West2 Database,"The NGA-West2 project database expands on its predecessor to include worldwide ground motion data recorded from shallow crustal earthquakes in active tectonic regimes post-2000 and a set of small-to-moderate-magnitude earthquakes in California between 1998 and 2011. The database includes 21,336 (mostly) three-component records from 599 events. The parameter space covered by the database is M 3.0 to M 7.9, closest distance of 0.05 to 1,533 km, and site time-averaged shear-wave velocity in the top 30 m of VS30 = 94 m/s to 2,100 m/s (although data becomes sparse for distances >400 km and VS30 > 1,200 m/s or <150 m/s). The database includes uniformly processed time series and response spectral ordinates for 111 periods ranging from 0.01 s to 20 s at 11 damping ratios. Ground motions and metadata for source, path, and site conditions were subject to quality checks by ground motion prediction equation developers and topical working groups.",1005 - 989,2014.0,http://watts.com/blogprivacy.php,Geology
1686,5fec3cda0d994c217c737c10b0b7e56d4574edca,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",D503 - D509,2013.0,https://www.rhodes-osborne.org/explore/list/categoryabout.htm,Medicine
1687,b43ffeb049adb791a82521eaaf83c367d651da4c,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",D816 - D823,2012.0,https://cooper.net/tagsregister.php,Biology
1688,cf4dc5ff06fc8c17b05b399a98c164c60a834e09,Systemic Banking Crises: A New Database,"This paper presents a new database on the timing of systemic banking crises and policy responses to resolve them. The database covers the universe of systemic banking crises for the period 1970-2007, with detailed data on crisis containment and resolution policies for 42 crisis episodes, and also includes data on the timing of currency crises and sovereign debt crises. The database extends and builds on the Caprio, Klingebiel, Laeven, and Noguera (2005) banking crisis database, and is the most complete and detailed database on banking crises to date.",29-117,2008.0,http://silva.com/blog/tag/postsmain.jsp,Business
1689,d5837fd1cf0d51e547553c3612928de880a08589,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.","
          D91-4
        ",2004.0,http://mason.biz/wp-contentsearch.html,Medicine
1690,1fcf323ff79c46b401e7a3d8510f50da17073cb8,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",D590 - D598,2005.0,https://hester-anderson.com/list/search/explorehomepage.html,Medicine
1691,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",D691 - D697,2010.0,https://cruz-jackson.com/blog/app/searchpost.html,Biology
1692,7def002796277facffe02aa09e3a1bb101ec0785,Access path selection in a relational database management system,"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.",23-34,1979.0,https://www.trujillo.com/explore/tagsregister.html,Computer Science
1693,f780775b85986a2bf1d5849c56f621cb99efef3c,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",D81 - D89,2015.0,https://reed.com/explore/mainsearch.php,Computer Science
1694,4bba202eee798202e214cbc6bfca6a78e3f26342,Energy-Aware Database Management Systems,"Annual energy cost of data centers have reached one billion US dollars in the United States alone. In a typical data center, a significant portion of the computing resources (thus energy) is dedicated to running the back end database service. Therefore, making databases energy-aware is of high economic and sustainability significance. For that purpose, we have launched a research project that aims at the design and implementation of an energy-aware DBMS that enables significant energy conservation (as compared to traditional DBMSs) within a bounded performance penalty. This white paper encapsulates some of the critical ideas we explored and verified in designing such a DBMS.",69-112,2011.0,https://ford.biz/tags/category/tagsprivacy.htm,Technology
1695,677872c4051ee37a13161ecb8efa32fcbadc7a80,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",D227 - D233,2009.0,https://www.stewart.com/tags/main/tagprivacy.htm,Medicine
1696,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",69-133,2016.0,https://levine.com/list/tagscategory.htm,Physics
1697,3a60678ad2b862fa7c27b11f04c93c010cc6c430,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",42-55,2012.0,http://www.wheeler.net/postslogin.html,Psychology
1698,fb1d51c023da037dac928d7991de1b77ade22728,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",747-751,2005.0,https://vargas.com/appprivacy.jsp,Chemistry
1699,a7a581f7f052570fa099dcb887b9934617760791,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",69-113,2013.0,http://richardson-conner.com/app/search/exploreregister.htm,Medicine
1700,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.","
          22-8
        ",2001.0,https://www.page-stuart.com/appfaq.htm,Biology
1701,6dc7b1da99e30592b54f2148c85e8725563011f7,Storing and querying ordered XML using a relational database system,"XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.",204-215,2002.0,https://bennett.com/search/search/explorehome.asp,Computer Science
1702,928e4178fca984b1d49da75c5120e05cff7a3fa9,Development of a 2001 National land-cover database for the United States,"Multi-Resolution Land Characterization 2001 (MRLC 2001) is a second-generation Federal consortium designed to create an updated pool of nation-wide Landsat 5 and 7 imagery and derive a second-generation National Land Cover Database (NLCD 2001). The objectives of this multi-layer, multi-source database are two fold: first, to provide consistent land cover for all 50 States, and second, to provide a data framework which allows flexibility in developing and applying each independent data component to a wide variety of other applications. Components in the database include the following: (1) normalized imagery for three time periods per path/row, (2) ancillary data, including a 30 m Digital Elevation Model (DEM) derived into slope, aspect and slope position, (3) perpixel estimates of percent imperviousness and percent tree canopy (4) 29 classes of land cover data derived from the imagery, ancillary data, and derivatives, (5) classification rules, confidence estimates, and metadata from the land cover classification. This database is now being developed using a Mapping Zone approach, with 66 Zones in the continental United States and 23 Zones in Alaska. Results from three initial mapping Zones show single-pixel land cover accuracies ranging from 73 to 77 percent, imperviousness accuracies ranging from 83 to 91 percent, tree canopy accuracies ranging from 78 to 93 percent, and an estimated 50 percent increase in mapping efficiency over previous methods. The database has now entered the production phase and is being created using extensive partnering in the Federal government with planned completion by 2006.",829-840,2004.0,http://ashley-lee.com/main/appprivacy.htm,Geography
1703,6b092ee6d04b4fa76a0452cf0696086a7c04644d,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.","
          303-5
        ",2002.0,https://wolf.com/app/mainregister.php,Biology
1704,f86808d344d7884296c222f0cd6892abe411c636,The Ribosomal Database Project.,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services, and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (rdp.life.uiuc.edu), electronic mail (server/rdp.life.uiuc.edu) and gopher (rdpgopher.life.uiuc.edu). The electronic mail server also provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for chimeric nature of newly sequenced rRNAs, and automated alignment.","
          3485-7
        ",1994.0,https://www.moore-jones.com/tags/posts/categoryabout.php,Computer Science
1705,60add4035c2b6d4b9888cb1028c31e6bed793d60,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",D764 - D770,2013.0,http://evans.com/blogterms.htm,Biology
1706,fd2d9588818d4bd6d5cc59a26642c3d3c05ab915,IT’IS Database for Thermal and Electromagnetic Parameters of Biological Tissues,,98-146,2012.0,https://owens.com/searchfaq.php,Materials Science
1707,b62628ac06bbac998a3ab825324a41a11bc3a988,XM2VTSDB: The Extended M2VTS Database,"Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10",43-130,1999.0,http://clark.com/wp-contentlogin.php,Computer Science
1708,10b40befe5942e997f88ab40bac1acd931147893,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",D581 - D591,2013.0,http://montgomery.com/tagsterms.php,Medicine
1709,99a4acc4de2097d5ed6c4ec257f284c9dc86b3b8,A major upgrade of the VALD database,"Vienna atomic line database (VALD) is a collection of critically evaluated laboratory parameters for individual atomic transitions, complemented by theoretical calculations. VALD is actively used by astronomers for stellar spectroscopic studies—model atmosphere calculations, atmospheric parameter determinations, abundance analysis etc. The two first VALD releases contained parameters for atomic transitions only. In a major upgrade of VALD—VALD3, publically available from spring 2014, atomic data was complemented with parameters of molecular lines. The diatomic molecules C2, CH, CN, CO, OH, MgH, SiH, TiO are now included. For each transition VALD provides species name, wavelength, energy, quantum number J and Landé-factor of the lower and upper levels, radiative, Stark and van der Waals damping factors and a full description of electronic configurarion and term information of both levels. Compared to the previous versions we have revised and verify all of the existing data and added new measurements and calculations for transitions in the range between 20 Å and 200 microns. All transitions were complemented with term designations in a consistent way and electron configurations when available. All data were checked for consistency: listed wavelength versus Ritz, selection rules etc. A new bibliographic system keeps track of literature references for each parameter in a given transition throughout the merging process so that every selected data entry can be traced to the original source. The query language and the extraction tools can now handle various units, vacuum and air wavelengths. In the upgrade process we had an intensive interaction with data producers, which was very helpful for improving the quality of the VALD content.",93-134,2015.0,http://www.lucas-walker.biz/search/wp-contentfaq.html,Physics
1710,6d703e6eb1c72241720bafdb42b46b70c3bdd16e,MRC Psycholinguistic Database,,51-106,2001.0,http://oneill-turner.net/list/tagterms.html,Psychology
1711,5d0b1c9e2e24b38489c8021261217b023e42a652,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",D370 - D376,2011.0,http://skinner.com/listlogin.php,Medicine
1712,d57ca29d73272e139c04f118d5c3107dfb964596,Survey of graph database models,"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.",1:1-1:39,2008.0,https://www.buck-carney.com/apphome.asp,Computer Science
1713,e0867d523f610b32267fa7ec35a510936b8b595f,DISFA: A Spontaneous Facial Action Intensity Database,"Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.",151-160,2013.0,https://novak.com/explorepost.html,Computer Science
1714,d594c19d91c55a4e7f341fd1d45fc30c5fac0b1a,Atlantic Hurricane Database Uncertainty and Presentation of a New Database Format,"Abstract“Best tracks” are National Hurricane Center (NHC) poststorm analyses of the intensity, central pressure, position, and size of Atlantic and eastern North Pacific basin tropical and subtropical cyclones. This paper estimates the uncertainty (average error) for Atlantic basin best track parameters through a survey of the NHC Hurricane Specialists who maintain and update the Atlantic hurricane database. A comparison is then made with a survey conducted over a decade ago to qualitatively assess changes in the uncertainties. Finally, the implications of the uncertainty estimates for NHC analysis and forecast products as well as for the prediction goals of the Hurricane Forecast Improvement Program are discussed.",3576-3592,2013.0,http://robertson.org/search/tags/tagprivacy.html,Environmental Science
1715,36698b7863c7d64cf3fb9d17d0f4df1ff6d286dc,Measuring Financial Inclusion: The Global Findex Database,"This paper provides the first analysis of the Global Financial Inclusion (Global Findex) Database, a new set of indicators that measure how adults in 148 economies save, borrow, make payments, and manage risk. The data show that 50 percent of adults worldwide have an account at a formal financial institution, though account penetration varies widely across regions, income groups and individual characteristics. In addition, 22 percent of adults report having saved at a formal financial institution in the past 12 months, and 9 percent report having taken out a new loan from a bank, credit union or microfinance institution in the past year. Although half of adults around the world remain unbanked, at least 35 percent of them report barriers to account use that might be addressed by public policy. Among the most commonly reported barriers are high cost, physical distance, and lack of proper documentation, though there are significant differences across regions and individual characteristics.",72-122,2012.0,http://cohen.com/main/search/exploreauthor.htm,Business
1716,6787a08978444d97d6860359ba24b5a735d492e2,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",97-141,2015.0,http://lewis-galloway.com/posts/listhomepage.htm,Medicine
1717,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",D836 - D842,2017.0,https://www.williams-fowler.info/search/tags/blogabout.html,Computer Science
1718,ad9a8e6346a1259022abf526670eaf592270e59f,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",D723 - D729,2016.0,https://www.contreras.info/main/apphomepage.php,Biology
1719,7d25222fcf62dae782ae6b18fae0b33d9f7923fe,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",77-104,2003.0,https://conner-jones.com/blog/categories/appprivacy.jsp,Biology
1720,24019050c30b7e5bf1be28e48b8cb5278c4286fd,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",5437-5440,2013.0,http://www.contreras.com/tag/app/wp-contenthomepage.asp,Computer Science
1721,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.","
          38-41
        ",2002.0,http://www.anderson.net/tags/tagabout.html,Biology
1722,a094ea0a295db1b4d86d504cd885fde5d3396c75,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.","
          173-4
        ",2001.0,https://wheeler.info/list/wp-contentauthor.jsp,Biology
1723,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",D198 - D201,2006.0,http://zavala.biz/search/categories/postsfaq.php,Computer Science
1724,ef12383f516840ec1ec998cd5921dfc6e197c9b2,PPDB: The Paraphrase Database,"We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",758-764,2013.0,https://www.garrett-sharp.com/list/tagsauthor.asp,Computer Science
1725,cc589c499dcf323fe4a143bbef0074c3e31f9b60,A 3D facial expression database for facial behavior research,"Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation",211-216,2006.0,https://www.hickman.com/postscategory.html,Computer Science
1726,0afa75ad56cc8ca3cfa176f89443e9a70e09434c,Systemic Banking Crises Database: An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",23-103,2012.0,https://www.ortiz-powell.biz/main/listhome.html,Business
1727,a057d7736e3ae4675054a104a1301dba2ff8dbba,The Pfam protein families database,"Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allowing Pfam domain definitions to be closer to those found in structure databases. Pfam is available on the web in the UK (http://www.sanger.ac.uk/Software/Pfam/), the USA (http://pfam.wustl.edu/), France (http://pfam.jouy.inra.fr/) and Sweden (http://Pfam.cgb.ki.se/).","
          D138-41
        ",2004.0,http://www.jones.com/searchprivacy.asp,Computer Science
1728,e3bc4caca9a5115c61281acb99ab9b978edd6387,The sketchy database,"We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.",1 - 12,2016.0,http://www.mack-anderson.com/main/categoriesauthor.jsp,Computer Science
1729,3962ddcab496d4a2a7196dcef370c87f58a3133d,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",D136 - D143,2011.0,http://www.horton.biz/tagabout.html,Biology
1730,a0e5802cf66257d0412de878682dc9caccca0719,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.","
          439-41
        ",2003.0,https://davis.org/main/app/mainhome.php,Biology
1731,c2b381b24aabf237394059fed7920cd6fd0e67b8,Database Mining: A Performance Perspective,"The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >",914-925,1993.0,http://williams-moore.org/tags/mainlogin.html,Computer Science
1732,dc85ca80fb3d75fe63106f631a2f7cd251d2851e,Identification of protein coding regions by database similarity search,,266-272,1993.0,https://www.carter.info/categoriesprivacy.html,Medicine
1733,788b43b7c62b497cf69b31544c6f81c6f4856d42,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",D222 - D230,2013.0,http://www.johnson-haney.info/appfaq.php,Computer Science
1734,0c13dc68a9b91e94dc23676ec878fa9f1794b866,"The Reticular Chemistry Structure Resource (RCSR) database of, and symbols for, crystal nets.","During the past decade, interest has grown tremendously in the design and synthesis of crystalline materials constructed from molecular clusters linked by extended groups of atoms. Most notable are metal-organic frameworks (MOFs), in which polyatomic inorganic metal-containing clusters are joined by polytopic linkers. (Although these materials are sometimes referred to as coordination polymers, we prefer to differentiate them, because MOFs are based on strong linkages that yield robust frameworks.) The realization that MOFs could be designed and synthesized in a rational way from molecular building blocks led to the emergence of a discipline that we call reticular chemistry. MOFs can be represented as a special kind of graph called a periodic net. Such descriptions date back to the earliest crystallographic studies but have become much more common recently because thousands of new structures and hundreds of underlying nets have been reported. In the simplest cases (e.g., the structure of diamond), the atoms in the crystal become the vertices of the net, and bonds are the links (edges) that connect them. In the case of MOFs, polyatomic groups act as the vertices and edges of the net. Because of the explosive growth in this area, a need has arisen for a universal system of nomenclature, classification, identification, and retrieval of these topological structures. We have developed a system of symbols for the identification of three periodic nets of interest, and this system is now in wide use. In this Account, we explain the underlying methodology of assigning symbols and describe the Reticular Chemistry Structure Resource (RCSR), in which about 1600 such nets are collected and illustrated in a database that can be searched by symbol, name, keywords, and attributes. The resource also contains searchable data for polyhedra and layers. The database entries come from systematic enumerations or from known chemical compounds or both. In the latter case, references to occurrences are provided. We describe some crystallographic, topological, and other attributes of nets and explain how they are reported in the database. We also describe how the database can be used as a tool for the design and structural analysis of new materials. Associated with each net is a natural tiling, which is a natural partition of space into space-filling tiles. The database allows export of data that can be used to analyze and illustrate such tilings.","
          1782-9
        ",2008.0,https://robbins.com/explore/blog/postsregister.htm,Chemistry
1735,987a42cc5a8d7c8536e7e5a308b1ba6aa15d454f,The UMIST database for astrochemistry 2012,"We present the fifth release of the UMIST Database for Astrochemistry (UDfA). The new reaction network contains 6173 gas-phase reactions, involving 467 species, 47 of which are new to this release. We have updated rate coefficients across all reaction types. We have included 1171 new anion reactions and updated and reviewed all photorates. In addition to the usual reaction network, we also now include, for download, state-specific deuterated rate coefficients, deuterium exchange reactions and a list of surface binding energies for many neutral species. Where possible, we have referenced the original source of all new and existing data. We have tested the main reaction network using a dark cloud model and a carbon-rich circumstellar envelope model. We present and briefly discuss the results of these models.",77-144,2012.0,https://www.massey.com/explore/wp-content/listsearch.php,Physics
1736,a175453ceff401c67ce503750418e66936331b06,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",D760 - D765,2006.0,https://www.gray.com/tagsabout.html,Computer Science
1737,b02d2cf90e5b06e3278e23e7984c29b0307c5ef3,Principles of database and knowledge- base systems,,99-106,1989.0,http://www.thompson-short.com/mainhome.htm,Computer Science
1738,ffc8c09a4cb5a174166ac16d19e646bc150c2c31,Phone Recognition on the TIMIT Database,",",86-143,2012.0,http://gibson.org/exploreabout.html,Technology
1739,187f31d400d711e25fd3cddd195f0af730729912,Cloud-Screening and Quality Control Algorithms for the AERONET Database,,337-349,2000.0,https://www.lee.net/tags/search/wp-contentprivacy.html,Environmental Science
1740,854b86751d0d02bf205c6fb4bf2f2d13804430ee,"Human genetic variation database, a reference database of genetic variations in the Japanese population",,547 - 553,2016.0,https://www.palmer.com/blog/categories/wp-contentprivacy.htm,Medicine
1741,6827419c860e6b9c825a1138d48633ecef00d4d5,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",19-110,2016.0,http://henderson.com/search/tagsabout.asp,Computer Science
1742,0a4365f6c30d40ca7610e9afac6c339f3c72224c,Validation of the national health insurance research database with ischemic stroke cases in Taiwan,The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke.,95-103,2011.0,https://rose-hill.com/mainregister.html,Medicine
1743,1e9e0538ae54be22a515430199dab0befb9e29cb,Database: The Journal of Biological Databases and Curation,"Evolution provides the unifying framework with which to understand biology. The coherent investigation of genic and genomic data often requires comparative genomics analyses based on whole-genome alignments, sets of homologous genes and other relevant datasets in order to evaluate and answer evolutionary-related questions. However, the complexity and computational requirements of producing such data are substantial: this has led to only a small number of reference resources that are used for most comparative analyses. The Ensembl comparative genomics resources are one such reference set that facilitates comprehensive and reproducible analysis of chordate genome data. Ensembl computes pairwise and multiple whole-genome alignments from which large-scale synteny, per-base conservation scores and constrained elements are obtained. Gene alignments are used to define Ensembl Protein Families, GeneTrees and homologies for both protein-coding and non-coding RNA genes. These resources are updated frequently and have a consistent informatics infrastructure and data presentation across all supported species. Specialized web-based visualizations are also available including synteny displays, collapsible gene tree plots, a gene family locator and different alignment views. The Ensembl comparative genomics infrastructure is extensively reused for the analysis of non-vertebrate species by other projects including Ensembl Genomes and Gramene and much of the information here is relevant to these projects. The consistency of the annotation across species and the focus on vertebrates makes Ensembl an ideal system to perform and support vertebrate comparative genomic analyses. We use robust software and pipelines to produce reference comparative data and make it freely available.Database URL: http://www.ensembl.org.",29-112,2016.0,http://smith-hoffman.org/wp-contentlogin.php,Computer Science
1744,bcc73dd05b7b7a6616c41df428e3624375c95e56,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",66-117,2016.0,https://lopez.com/app/tag/categoryhome.jsp,Biology
1745,7bbe0235f583b27f96c0f288043876f86795d6c2,AVA: A large-scale database for aesthetic visual analysis,"With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.",2408-2415,2012.0,https://arias-palmer.net/category/categories/explorepost.asp,Computer Science
1746,0b9182d502e62fb7e1ebd7e01de7523005d35677,The notions of consistency and predicate locks in a database system,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.",624-633,1976.0,https://www.brewer.com/tags/tag/mainauthor.htm,Computer Science
1747,0ec2a62edef364f4bef1e1b265d3d7869bb4444a,Database Systems: The Complete Book,"From the Publisher: 
This introduction to database systems offers a readable comprehensive approach with engaging, real-world examplesusers will learn how to successfully plan a database application before building it. The first half of the book provides in-depth coverage of databases from the point of view of the database designer, user, and application programmer, while the second half of the book provides in-depth coverage of databases from the point of view of the DBMS implementor. The first half of the book focuses on database design, database use, and implementation of database applications and database management systemsit covers the latest database standards SQL:1999, SQL/PSM, SQL/CLI, JDBC, ODL, and XML, with broader coverage of SQL than most other books. The second half of the book focuses on storage structures, query processing, and transaction managementit covers the main techniques in these areas with broader coverage of query optimization than most other books, along with advanced topics including multidimensional and bitmap indexes, distributed transactions, and information integration techniques. A professional reference for database designers, users, and application programmers.",31-121,2001.0,http://hughes.com/categorycategory.jsp,Computer Science
1748,148c4770b5fc2f841179f4c4f800f41c2171841a,A New Database on the Structure and Development of the Financial Sector,"This article introduces a new database of indicators of financial structure and financial development across countries and over time. The database is unique in that it combines a wide variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, introducing indicators of the size and activity of nonbank financial institutions, and constructing measures of the size of bond and primary equity markets. This article introduces a new database, the first to provide comprehensive measures of the development, structure, and performance of the financial sector. This database is the first to define and construct indicators of the size and activity of nonbank financial intermediaries, such as insurance companies, pension funds, and non-deposit money banks. It is also the first to include indicators of the size of primary equity markets and primary and secondary bond markets. In constructing the database, authors carefully deflate measures and match stock and flow variables.",597-605,2000.0,https://wood.com/main/searchlogin.html,Economics
1749,ff2218b349f89026ffaaccdf807228fa497c04bd,THE DIGITAL DATABASE FOR SCREENING MAMMOGRAPHY,,64-131,2007.0,http://www.harper.com/blog/categorieshomepage.html,Computer Science
1750,49b60b92201710d095684b6df1b1d93f92122dd0,MINT: the Molecular INTeraction database,"The Molecular INTeraction database (MINT, ) aims at storing, in a structured format, information about molecular interactions (MIs) by extracting experimental details from work published in peer-reviewed journals. At present the MINT team focuses the curation work on physical interactions between proteins. Genetic or computationally inferred interactions are not included in the database. Over the past four years MINT has undergone extensive revision. The new version of MINT is based on a completely remodeled database structure, which offers more efficient data exploration and analysis, and is characterized by entries with a richer annotation. Over the past few years the number of curated physical interactions has soared to over 95 000. The whole dataset can be freely accessed online in both interactive and batch modes through web-based interfaces and an FTP server. MINT now includes, as an integrated addition, HomoMINT, a database of interactions between human proteins inferred from experiments with ortholog proteins in model organisms ().",D572 - D574,2006.0,http://www.merritt.info/categoriesprivacy.asp,Medicine
1751,7bdafc53f96f0282c17b082a41b67d76cb24d957,miRTarBase: a database curates experimentally validated microRNA–target interactions,"MicroRNAs (miRNAs), i.e. small non-coding RNA molecules (∼22 nt), can bind to one or more target sites on a gene transcript to negatively regulate protein expression, subsequently controlling many cellular mechanisms. A current and curated collection of miRNA–target interactions (MTIs) with experimental support is essential to thoroughly elucidating miRNA functions under different conditions and in different species. As a database, miRTarBase has accumulated more than 3500 MTIs by manually surveying pertinent literature after data mining of the text systematically to filter research articles related to functional studies of miRNAs. Generally, the collected MTIs are validated experimentally by reporter assays, western blot, or microarray experiments with overexpression or knockdown of miRNAs. miRTarBase curates 3576 experimentally verified MTIs between 657 miRNAs and 2297 target genes among 17 species. miRTarBase contains the largest amount of validated MTIs by comparing with other similar, previously developed databases. The MTIs collected in the miRTarBase can also provide a large amount of positive samples to develop computational methods capable of identifying miRNA–target interactions. miRTarBase is now available on http://miRTarBase.mbc.nctu.edu.tw/, and is updated frequently by continuously surveying research articles.",D163 - D169,2010.0,http://www.morton-bell.com/exploresearch.php,Biology
1752,c6804d16e1f4e3bde4d535384a699e65479634b6,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",D262 - D267,2012.0,https://www.mullins-johnson.com/app/mainauthor.asp,Biology
1753,018b4d4b8cafce6624bc07a7339f53628dca5207,"The antiSMASH database, a comprehensive database of microbial secondary metabolite biosynthetic gene clusters","Secondary metabolites produced by microorganisms are the main source of bioactive compounds that are in use as antimicrobial and anticancer drugs, fungicides, herbicides and pesticides. In the last decade, the increasing availability of microbial genomes has established genome mining as a very important method for the identification of their biosynthetic gene clusters (BGCs). One of the most popular tools for this task is antiSMASH. However, so far, antiSMASH is limited to de novo computing results for user-submitted genomes and only partially connects these with BGCs from other organisms. Therefore, we developed the antiSMASH database, a simple but highly useful new resource to browse antiSMASH-annotated BGCs in the currently 3907 bacterial genomes in the database and perform advanced search queries combining multiple search criteria. antiSMASH-DB is available at http://antismash-db.secondarymetabolites.org/.",D555 - D559,2016.0,http://www.mcdonald.com/category/posts/apphomepage.asp,Computer Science
1754,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",68-148,2015.0,https://herrera.info/search/wp-content/blogsearch.html,Biology
1755,529124bc3ff7fa655729c49bf10aedaddd49787e,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",94-138,2013.0,http://thompson-nichols.net/taghomepage.asp,Computer Science
1756,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",D325 - D328,2004.0,http://martin-miller.biz/mainindex.htm,Computer Science
1757,66c0e24d97786f0382ce9f7acde37de9a349537c,The Reptile Database,,38-133,1995.0,https://www.parsons-ward.com/postsindex.php,Biology
1758,0d3e8b3a4bb5ffc8bc0527c443a99eb1438956a0,A face antispoofing database with diverse attacks,"Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.",26-31,2012.0,http://www.young-peters.biz/app/appindex.php,Computer Science
1759,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",D64 - D69,2012.0,http://www.pena-carter.com/exploreprivacy.php,Computer Science
1760,8df1903824290707e09ee754f3c40d90b088d65b,World Database on Protected Areas (WDPA),,55-146,2013.0,https://lucas-cunningham.com/posts/search/listprivacy.htm,Geography
1761,afef030037e621538ac53e18d23a1a4660df79d8,Harmonized World Soil Database (version 1.2),"METIS-ID: 167825 The Harmonized World Soil Database is a 30 arc-second raster database with over 15000 different soil mapping units that combines existing regional and national updates of soil information worldwide (SOTER, ESD, Soil Map of China, ISRIC-WISE) with the information contained within the 1:5 000 000 scale FAO-UNESCO Soil Map of the World (FAO, 1971-1981). The resulting raster database consists of ... 21600 rows and 43200 columns, which are linked to harmonized soil property data. The use of a standardized structure allows for the linkage of the attribute data with the raster map to display or query the composition in terms of soil units and the characterization of selected soil parameters (organic Carbon, pH, water storage capacity, soil depth, cation exchange capacity of the soil and the clay fraction, total exchangeable nutrients, lime and gypsum contents, sodium exchange percentage, salinity, textural class and granulometry). Reliability of the information contained in the database is variable: the parts of the database that still make use of the Soil Map of the World such as North America, Australia, West Africa and South Asia are considered less reliable, while most of the areas covered by SOTER databases are considered to have the highest reliability (Southern Africa, Latin America and the Caribbean, Central and Eastern Europe). Further expansion and update of the HWSD is foreseen for the near future, notably with the excellent databases held in the USA (Natural Resources Conservation Service US General Soil Map, STATSGO), Canada (Agriculture and AgriFood Canada: The National Soil Database NSDB), and Australia (CSIRO, ACLEP, Nnatural Heritage Trust and National Land and Water Resources Audit: ASRIS), and with the recently released SOTER database for Central Africa (FAO/ISRIC/Univ. Gent, 2007)",98-129,2008.0,http://www.marsh-cole.com/blog/tags/appcategory.htm,Environmental Science
1762,04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59,The IAM-database: an English sentence database for offline handwriting recognition,,39-46,2002.0,http://www.orozco.com/blog/categories/appsearch.jsp,Computer Science
1763,00899cac0e2e770fe6b28deac002eceb8c3c4bea,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",66-105,2010.0,https://www.brooks-miller.com/app/tags/blogcategory.html,Medicine
1764,0d2072d81d03247949126e87d6201788cb646526,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.","
          235-8
        ",2002.0,http://henry-jenkins.com/blog/blog/wp-contentabout.html,Biology
1765,e8ddd8c820dba886f618f0e84ce38ecd0b967b39,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",D767 - D772,2008.0,https://miller.net/tag/posts/wp-contentsearch.htm,Computer Science
1766,607ca163b2635f9992e773d1b1d07d39d5d33f4e,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,1,2003.0,https://stokes.com/search/list/wp-contentprivacy.htm,Medicine
1767,c80b987fe2d52214772f435417cb6666f60613d2,An Introduction to Database Systems,"From the Publisher: 
For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. ""Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.",34-108,1975.0,http://www.petty.com/wp-content/mainprivacy.html,Computer Science
1768,2bb7426e6ecdab0f120c89f6a324cf0c2a7266d4,BrainWeb: Online Interface to a 3D MRI Simulated Brain Database,"Introduction: The increased importance of automated computer techniques for anatomical brain mapping from MR images and quantitative brain image analysis methods leads to an increased need for validation and evaluation of the effect of image acquisition parameters on performance of these procedures. Validation of analysis techniques of in-vivo acquired images is complicated due to the lack of reference data (“ground truth”). Also, optimal selection of the MR imaging parameters is difficult due to the large parameter space. BrainWeb makes available to the neuroimaging community, online on WWW, a set of realistic simulated brain MR image volumes (Simulated Brain Database, SBD) that allows the above issues to be examined in a controlled, systematic way.",78-121,1997.0,http://roberson-bryant.com/category/categorieslogin.asp,Computer Science
1769,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",D525 - D531,2009.0,https://gould-white.biz/mainmain.jsp,Medicine
1770,f707571329e1aa7180a1a289b1aa250eabdc8618,InterPro in 2011: new developments in the family and domain prediction database,"InterPro (http://www.ebi.ac.uk/interpro/) is a database that integrates diverse information about protein families, domains and functional sites, and makes it freely available to the public via Web-based interfaces and services. Central to the database are diagnostic models, known as signatures, against which protein sequences can be searched to determine their potential function. InterPro has utility in the large-scale analysis of whole genomes and meta-genomes, as well as in characterizing individual protein sequences. Herein we give an overview of new developments in the database and its associated software since 2009, including updates to database content, curation processes and Web and programmatic interfaces.",D306 - D312,2011.0,https://anderson-harper.com/blog/explore/searchmain.htm,Medicine
1771,756686211e7d253ac7abb8cb9e290141880f0889,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",e6 - e6,2013.0,http://www.hughes-levy.com/app/categorieshome.php,Biology
1772,334511feb95634c91d06359fd497d01fc60767f7,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.","
          239-41
        ",2001.0,https://www.smith-gentry.com/categorymain.php,Biology
1773,2129930aa4803610ea25860e770cbc73440acbf4,Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",75-119,1991.0,http://www.murphy.com/wp-content/search/categoryhomepage.html,Biology
1774,acd36b17c486957cebacc3ad68bd83ed417bf9cc,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",75-134,2002.0,http://www.jones.org/listauthor.html,Biology
1775,93c5bb23406778eb421a3dfa5978a231e022792a,INbreast: toward a full-field digital mammographic database.,,"
          236-48
        ",2012.0,http://vaughn-parks.com/list/explorehomepage.jsp,Medicine
1776,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",D98 - D104,2008.0,http://www.meyer.com/blog/wp-contentmain.html,Medicine
1777,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,A gene expression database for the molecular pharmacology of cancer,,236-244,2000.0,https://www.mcpherson.com/main/app/mainindex.html,Biology
1778,aec58dd1363d679e9e1d7917d74c0bcde504b65f,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.","
          91-101; discussion 101-3, 119-28, 244-52
        ",2002.0,https://alexander.com/category/categoryhomepage.jsp,Computer Science
1779,b7340682cd94e4a40df2823cee68ff166be93f86,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",D793 - D800,2012.0,http://church.biz/main/list/searchprivacy.html,Computer Science
1780,c378df148d9e42376e4f47888b04a1f679d2e25f,XCOM : Photon Cross Sections Database,,71-137,2005.0,http://parks-alvarez.org/category/list/exploremain.html,Materials Science
1781,d53bcbac7ea19173e95d3bd855b998fab765737d,WordNet: An Electronic Lexical Database,,65-127,1998.0,https://yu.com/searchmain.html,Computer Science
1782,a6e72ff479fb58f0b714f07b0292c612dfe4ff05,Principles Of Database And Knowledge-Base Systems,"This book goes into the details of database conception and use, it tells you everything on relational databases. from theory to the actual used algorithms.",18-138,1979.0,https://thomas.org/wp-content/tagsabout.htm,Computer Science
1783,30ae1edd4e6f13a28f87ec150c407e6820c7da60,PROGgeneV2: enhancements on the existing database,,68-145,2014.0,http://www.walker.org/tag/categories/maincategory.php,Medicine
1784,3c2d95624ede725cc629cfe63affb57237f009f7,A New Database of Financial Reforms,,281-302,2008.0,http://daniel.com/tags/mainpost.htm,Business
1785,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",D192 - D196,2004.0,http://jones.biz/wp-content/explore/wp-contentfaq.htm,Biology
1786,ab99f2afd0f09ee707540022f0895a09fa1107f1,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.","
          28-33
        ",2003.0,http://www.miller-bell.com/categorylogin.html,Medicine
1787,03cea6194b4d402f83f48382cbaf52b369d3d700,Physical Database Design for Relational Databases,,2108-2114,2018.0,https://www.roberts.com/posts/main/listauthor.htm,Computer Science
1788,42a77b1006675dc954495459ba3bf9c194258c04,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.","
          248-50
        ",2003.0,http://www.johnson.org/explore/postsfaq.asp,Computer Science
1789,a5edce377759894482464a133cb9ec6791709eb2,Database System Concepts,"From the Publisher: 
This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.",23-131,1980.0,https://cortez.com/category/list/mainterms.php,Computer Science
1790,6dd37b57f3391b438fa588f98a1c2067365ae5ca,TID2008 – A database for evaluation of full-reference visual quality assessment metrics,"— In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.",30-45,2004.0,http://espinoza-blake.com/app/search/blogauthor.html,Computer Science
1791,c64e51ab702ecdfc65281eb06fae8722809a0756,A lifespan database of adult facial stimuli,,630-633,2004.0,http://www.johnson.com/postsprivacy.jsp,Medicine
1792,e606ccf581b507149e4bdaba972ab58682eef57b,The SIMBAD astronomical database. The CDS reference database for astronomical objects,"Simbad is the reference database for identification and bibliography of astronomical objects. It contains identifications, “basic data”, bibliography, and selected observational measurements for several million astronomical objects.  Simbad is developed and maintained by CDS, Strasbourg. Building the database contents is achieved with the help of several contributing institutes. Scanning the bibliography is the result of the collaboration of CDS with bibliographers in Observatoire de Paris (DASGAL), Institut d'Astrophysique de Paris, and Observatoire de Bordeaux. When selecting catalogues and tables for inclusion, priority is given to optimal multi-wavelength coverage of the database, and to support of research developments linked to large projects. In parallel, the systematic scanning of the bibliography reflects the diversity and general trends of astronomical research. A WWW interface to Simbad is available at: http://simbad.u-strasbg.fr/Simbad.",9-22,2000.0,http://hughes.info/blogfaq.html,Physics
1793,1dd0140d51e870a713340ae30734c8438b03d1a3,Unit selection in a concatenative speech synthesis system using a large speech database,"One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",373-376 vol. 1,1996.0,https://www.romero.info/searchhome.htm,Computer Science
1794,1bdc29257650e2bed7d11a9a4afb4eeea0bb1296,"The PROSITE database, its status in 1997","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.","
          217-21
        ",1997.0,https://www.ellis.net/wp-contentregister.html,Computer Science
1795,81e89f25baed869a690ffc6f93cd0306c58efe14,The Mammographic Image Analysis Society digital mammogram database,"A clamp or grip for heavy duty work with twisted wire cables and the like, such as in marine and industrial uses and especially where reasonably easy application of the cable grip to the cable is important and undue bending moments on the heavy cables are to be avoided. The feature of a removable jaw is coupled with dual link bar structure for the jaws without sacrificing strength and with a considerable reduction in overall weight of the clamp as compared to presently available equipment, this being accomplished in part by elimination of a frame as such and providing the principal jaw with a slotted stabilizing arm having a sliding connection with a unique hanger bar, which latter is designed to be connected to the lift hook of a crane or the like.",51-120,1994.0,https://www.bryant.org/explore/wp-content/categoriesregister.html,Computer Science
1796,20bdfd777432f7eab7ab2cc146297df1db654090,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",D857 - D861,2011.0,http://fitzgerald-fowler.net/tag/list/blogsearch.htm,Biology
1797,5c814bc6b49f21a1b84cbc3d4dc662a24165baee,Spanner: Google's globally-distributed database,"Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It provides strong transactional semantics, consistent replication, and high performance reads and writes for a variety of Google's applications. I'll discuss the design and implementation of Spanner, as well as some of the lessons we have learned along the way. I'll also discuss some open challenges that we still see in building scalable distributed storage systems.",9:1,2013.0,http://www.cain.com/listhome.htm,Computer Science
1798,f1d6cbac2f1ad7443c60c035e1c819bef402c5c7,Systemic Banking Crises Database II,,307-361,2013.0,https://www.clark.com/list/wp-contentsearch.jsp,Economics
1799,2055c63fd081abf321ad0ff61987df112f8871c4,Extending the database relational model to capture more meaning,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. (1) the search for meaningful units that are as small as possible—atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",397 - 434,1979.0,https://gardner.biz/listpost.php,Computer Science
1800,aef87d005e8e3d58f0a0577a7be4d55a10c2d5b3,MODOMICS: a database of RNA modification pathways. 2008 update,"MODOMICS, a database devoted to the systems biology of RNA modification, has been subjected to substantial improvements. It provides comprehensive information on the chemical structure of modified nucleosides, pathways of their biosynthesis, sequences of RNAs containing these modifications and RNA-modifying enzymes. MODOMICS also provides cross-references to other databases and to literature. In addition to the previously available manually curated tRNA sequences from a few model organisms, we have now included additional tRNAs and rRNAs, and all RNAs with 3D structures in the Nucleic Acid Database, in which modified nucleosides are present. In total, 3460 modified bases in RNA sequences of different organisms have been annotated. New RNA-modifying enzymes have been also added. The current collection of enzymes includes mainly proteins for the model organisms Escherichia coli and Saccharomyces cerevisiae, and is currently being expanded to include proteins from other organisms, in particular Archaea and Homo sapiens. For enzymes with known structures, links are provided to the corresponding Protein Data Bank entries, while for many others homology models have been created. Many new options for database searching and querying have been included. MODOMICS can be accessed at http://genesilico.pl/modomics.",D118 - D121,2008.0,http://www.acosta.com/tagsregister.php,Biology
1801,107e98602c1be84b1654d6a1b241b7c97a94c71f,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",D1241 - D1244,2011.0,https://webb-bolton.info/category/postscategory.html,Computer Science
1802,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",D539 - D543,2008.0,http://robinson.com/postscategory.php,Computer Science
1803,8162d4f3bfce2055c9a53c267af66103c3bfd167,Survey on NoSQL database,"With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.",363-366,2011.0,https://tapia-hampton.net/search/categoryindex.htm,Computer Science
1804,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",D145 - D149,2005.0,https://www.clarke-oconnor.net/blog/wp-contentpost.html,Biology
1805,ef50e6878d6addcbd5d1ca96e08eef51b9ddec9e,Reference database of Raman spectra of biological molecules,"Raman spectra of biological materials are very complex, because they consist of signals from all molecules present in cells. In order to obtain chemical information from these spectra, it is necessary to know the Raman patterns of the possible components of a cell. In this paper, we present a collection of Raman spectra of biomolecules that can serve as references for the interpretation of Raman spectra of biological materials. We included the most important components present in a cell: (1) DNA and RNA bases (adenine, cytosine, guanine, thymine and uracil), (2) amino acids (glycine, L-alanine, L-valine, L-serine, L-glutamic acid, L-arginine, L-phenylalanine, L-tyrosine, L-tryptophan, L-histidine, L-proline), (3) fatty acids and fats (lauric acid, myristic acid, palmitic acid, stearic acid, 12-methyltetradecanoic acid, 13-methylmyristic acid, 14-methylpentadecanoic acid, 14-methylhexadecanoic acid, 15-methylpalmitic acid, oleic acid, vaccenic acid, glycerol, triolein, trilinolein, trilinolenin), (4) saccharides (β-D-glucose, lactose, cellulose, D-(+)-dextrose, D-(+)-trehalose, amylose, amylopectine, D-(+)-mannose, D-(+)-fucose, D-(−)-arabinose, D-(+)-xylose, D-(−)-fructose, D-(+)-galactosamine, N-acetyl-D-glucosamine, chitin), (5) primary metabolites (citric acid, succinic acid, fumarate, malic acid, pyruvate, phosphoenolpyruvate, coenzyme A, acetyl coenzyme A, acetoacetate, D-fructose-6-phosphate) and (6) others (β-carotene, ascorbic acid, riboflavin, glutathione). Examples of Raman spectra of bacteria and fungal spores are shown, together with band assignments to the reference products. Copyright © 2007 John Wiley & Sons, Ltd.",1133-1147,2007.0,http://hogan.info/wp-contentfaq.html,Chemistry
1806,2a75f34663a60ab1b04a0049ed1d14335129e908,Web-based database for facial expression analysis,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.",5 pp.-,2005.0,http://www.barrett-newman.org/explorecategory.html,Computer Science
1807,baf03356bf3403f0fc111e1b348a77a01ef48899,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",D459 - D471,2013.0,http://www.jones-solomon.info/searchhome.jsp,Computer Science
1808,015525f864ccaf28efbdaed46029598441121a9e,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",15-130,2003.0,http://weber-hood.com/list/appfaq.asp,Engineering
1809,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",D613 - D619,2009.0,https://www.perez-lewis.net/wp-contentauthor.htm,Computer Science
1810,8445ff6a398279d3179bbedae01bab7c034c31ff,The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings,"Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.",3591-3591,2013.0,http://white.com/blog/mainabout.php,Computer Science
1811,b7e3bec7efda0946a73d8cfa06550ef2b1a2b2bd,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,,355 - 358,2004.0,https://www.perez.biz/tags/main/listabout.asp,Medicine
1812,5c86baa92ece7425ceb09232dddd9538c224ce5e,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",100-134,2010.0,https://www.stephenson-archer.com/wp-content/mainindex.jsp,Biology
1813,9c1411c9ebc6260edc5798c9339e189e759b2168,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/","
          1635-8
        ",2005.0,https://www.curtis.com/tags/main/blogfaq.php,Computer Science
1814,71e9a23138a5d7c35b28bd98fd616c81719b1b7a,"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison","Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as ‘Big Data’ that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can’t be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.",22-113,2013.0,http://www.olsen.com/tag/appprivacy.html,Computer Science
1815,85331cdfdae93103b0c86bcb12bb1b47158ced08,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",D251 - D258,2013.0,http://www.robinson.com/listregister.jsp,Computer Science
1816,b0c5efdf2f90322784283290a052797eb073b554,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",WDS229-WDS234,2013.0,https://gomez.com/categorymain.html,Computer Science
1817,20f5782a5fed99979ae406849d7d11bd59314996,dbEST — database for “expressed sequence tags”,,332-333,1993.0,http://www.martin-blanchard.com/listprivacy.htm,Biology
1818,d6cff906315e29b61afcf14bf7e6fe40f4d74ea5,A large-scale hierarchical image database,,76-109,2009.0,https://www.santiago-miller.com/search/tag/categoryabout.jsp,Computer Science
1819,5ab169aed6e76c20621a23c411f651aac423efe3,"Principles of Database and Knowledge-Base Systems, Volume II",,"I-XI, 1-631",1988.0,http://abbott.com/tag/explore/mainhomepage.asp,Computer Science
1820,f322e882eec09709f7f7c2d7824722509b79f5e9,Principles of Database Systems,"A large part is a description of relations, their algebra and calculus, and the query languages that have been designed using these concepts. There are explanations of how the theory can be used to design good systems. A description of the optimization of queries in relation-based query languages is provided, and a chapter is devoted to the recently developed protocols for guaranteeing consistency in databases that are operated on by many processes concurrently",92-105,1980.0,http://www.miles.com/categoryauthor.html,Computer Science
1821,d769ca2ac7b7057df74fdf9d4d0de91f1b07917d,ARDB—Antibiotic Resistance Genes Database,"The treatment of infections is increasingly compromised by the ability of bacteria to develop resistance to antibiotics through mutations or through the acquisition of resistance genes. Antibiotic resistance genes also have the potential to be used for bio-terror purposes through genetically modified organisms. In order to facilitate the identification and characterization of these genes, we have created a manually curated database—the Antibiotic Resistance Genes Database (ARDB)—unifying most of the publicly available information on antibiotic resistance. Each gene and resistance type is annotated with rich information, including resistance profile, mechanism of action, ontology, COG and CDD annotations, as well as external links to sequence and protein databases. Our database also supports sequence similarity searches and implements an initial version of a tool for characterizing common mutations that confer antibiotic resistance. The information we provide can be used as compendium of antibiotic resistance factors as well as to identify the resistance genes of newly sequenced genes, genomes, or metagenomes. Currently, ARDB contains resistance information for 13 293 genes, 377 types, 257 antibiotics, 632 genomes, 933 species and 124 genera. ARDB is available at http://ardb.cbcb.umd.edu/.",D443 - D447,2008.0,http://www.morris.com/categories/blogcategory.htm,Computer Science
1822,151ec57b35f6a7431fdce934a57ae15451079d85,NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",D975 - D979,2013.0,https://lopez.info/tags/blogindex.html,Medicine
1823,11fdda41735869a5962b698e9d4fc6524ee96d4c,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",726 - 729,2009.0,http://www.dixon.com/list/listhomepage.php,Medicine
1824,2485c98aa44131d1a2f7d1355b1e372f2bb148ad,The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.",149-161,2008.0,http://www.padilla-johnson.com/searchfaq.php,Computer Science
1825,9ebe338e49e63ff97348aca0db521ac3ff01bcef,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",D205 - D210,2008.0,http://white.net/posts/blogindex.html,Medicine
1826,6348c3490aa17188229decfd09b67e5169ce1cfc,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",1757 - 1764,2008.0,https://garcia-reed.org/appcategory.php,Computer Science
1827,69114cb58049c3b06ce012cf41d493d2e947b6c8,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",52-121,2014.0,https://anderson.net/main/category/categoriessearch.php,Computer Science
1828,bef5bdc4d49c9da09d125f4d86b15509ebff52cd,Parallel database systems: the future of high performance database systems,,85-98,1992.0,http://lopez-rose.com/searchmain.jsp,Computer Science
1829,ff89306dcc77b387f01718b497df0116c87c260d,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",D1013 - D1017,2008.0,http://fowler.com/list/list/mainmain.html,Computer Science
1830,0bd83f3dc2bf04bc592ae05112adf30882db1196,The Human Gene Mutation Database: 2008 update,,13 - 13,2009.0,http://www.walker.com/tags/searchauthor.php,Medicine
1831,fefea2b9ed93a0c3163432c52a67cf34efa868f7,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",D1140 - D1146,2013.0,https://lopez-walker.org/list/wp-content/tagsfaq.htm,Biology
1832,462098bcc81f5d8b8a067aa0b8988adba0eef91f,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",D750 - D753,2009.0,http://www.snyder.com/categorylogin.html,Computer Science
1833,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.","
          623-5
        ",2006.0,https://www.nelson.biz/tags/categorieshome.php,Medicine
1834,13bc458634865e23ed8ecd54473a34e705f7da10,THE HITRAN MOLECULAR DATABASE: EDITIONS OF 1991 AND 1992,,469-507,1992.0,https://herrera-chapman.net/app/category/listlogin.htm,Physics
1835,d03cab6781985ad3f62aec52048a7e15ee2dee61,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",D527 - D532,2006.0,https://williams-pierce.com/category/list/tagsfaq.php,Biology
1836,e72c49c059ac9926cc7e25d18971300f7ec2feef,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",287-317,1983.0,https://evans.net/list/mainhome.html,Computer Science
1837,fd05e825c5d076b17626996a78bdff5e7752549d,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",D344 - D350,2007.0,http://singh.info/categories/categoryindex.htm,Computer Science
1838,1f159d534f2e150577718b92d3ccfd3e23b7e889,Constructing a research database of social and environmental reporting by UK companies,"Responds to the widely‐reported methodological problems which have arisen in research into corporate social and environmental reporting. Reports on an attempt to build a database of UK company social and environmental disclosure. The motivation behind the database is an attempt to provide, first, a data set which both refines and develops earlier attempts to capture and interpret such disclosures; second, a data set covering several years to permit longitudinal analysis; and third, a public database for accounting researchers who wish to pursue, in a systematic and comparable way, more focused hypotheses about social and environmental reporting behaviour. Explains the motivation for, the background to, and process of establishing such a database and attempts to expose the difficulties met and the assumptions made in establishing the structure of the data capture. The resultant database has already proved useful to other UK researchers. Aims to help researchers in other countries to develop their own metho...",78-101,1995.0,https://powers.com/search/list/searchlogin.html,Business
1839,eda424538bab229d38f03a97d3ed1731e2a2c871,"Semantic database modeling: survey, applications, and research issues","Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages.
This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.",201-260,1987.0,http://www.burns.com/explore/categories/listindex.html,Computer Science
1840,100f4767f087e858976013b7117f38d26bb32a66,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.","
          1426-36
        ",1995.0,http://www.nichols.com/blogsearch.htm,Chemistry
1841,f82d09363e9a657749d14e97e9d6d1410331b436,Database Management Systems,"Database Management Systems Introduction Data Models Relational Model Entity-Relationship (E/R) Model Key-Value Model Entity-Relationship (E/R) Model Relationships Subclasses Constraints Converting ER to Relational Model Functional Dependencies (FD) FD Properties Closure Algorithm Superkeys & Minimal Basis Schema Normalization Decomposition Lossless-Join & Chase Algorithm Dependency Preserving Boyce-Codd Normal Form (BCNF) Third Normal Form (3NF) Relational Algebra Basic Operators Derived Operators Extended RA Syntax Data Storage Buffer Manager File Organization Page Organization Record Format Index Structures Indexing Basics Hash Tables B+ Trees Bitmaps & Bitslices Supporting SQL Operators External Sorting Selection Operator Projection Operator Join Operator Aggregation Operation Query Optimizer Annotated RA Trees Optimization Plans Cost Estimation Transaction Management Definition of Transactions The ""ACID"" Principle Write-Ahead Logging (WAL) Transaction Concurrency",25-113,2010.0,https://delgado.com/tags/mainregister.html,Computer Science
1842,240faf3bbdea0673f5bd6e3668e4de1de905ceee,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",D161 - D166,2009.0,https://www.allison.com/blog/apphome.php,Computer Science
1843,311c0501c68f8cbe0d2e3a161de1ab12d49cb9ce,Physical Properties of Ionic Liquids: Database and Evaluation,"A comprehensive database on physical properties of ionic liquids (ILs), which was collected from 109 kinds of literature sources in the period from 1984 through 2004, has been presented. There are 1680 pieces of data on the physical properties for 588 available ILs, from which 276 kinds of cations and 55 kinds of anions were extracted. In terms of the collected database, the structure-property relationship was evaluated. The correlation of melting points of two most common systems, disubstituted imidazolium tetrafluoroborate and disubstituted imidazolium hexafluorophosphate, was carried out using a quantitative structure-property relationship method.",1475-1517,2006.0,https://wilson-chang.com/taglogin.htm,Chemistry
1844,c9fc7bf985fde7246f6139809cc7b019fd1ae007,The Forest Inventory and Analysis Database: Database Description and Users Manual Version 4.0 for Phase 2,"This document is based on previous documentation of the nationally standardized Forest Inventory and Analysis database (Hansen and others 1992; Woudenberg and Farrenkopf 1995; Miles and others 2001). Documentation of the structure of the Forest Inventory and Analysis database (FIADB) for Phase 2 data, as well as codes and definitions, is provided. Examples for producing population level estimates are also presented. This database provides a consistent framework for storing forest inventory data across all ownerships for the entire United States. These data are available to the public.",67-109,2012.0,https://ballard.com/categories/mainindex.php,Computer Science
1845,269a6271fa98bbdc2d456bae7fb419a77c88dc70,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",D508 - D516,2012.0,https://middleton.com/tagsregister.php,Computer Science
1846,7b472238215ac399e119ef152c0ff93f2df1c8e6,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",D136 - D140,2008.0,http://www.warren.com/tagprivacy.asp,Computer Science
1847,a03d8f591bc3c2dbdecbd9d515e0469953a3f7ef,The NCBI dbGaP database of genotypes and phenotypes,,1181-1186,2007.0,https://leonard.org/listmain.htm,Biology
1848,7dabd56ccd524f78f0eda5073dc358f28893a45d,The CIPIC HRTF database,"This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the UC Davis CIPIC Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the HRTFs are reported.",99-102,2001.0,https://www.brewer.com/mainhome.html,Computer Science
1849,28e702e1a352854cf0748b9a6a9ad6679b1d4e83,Progressive skyline computation in database systems,"The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",41-82,2005.0,https://vargas.com/main/categories/tagsauthor.html,Computer Science
1850,b791d488eef45ef79da812f7569fc2cc83196aa5,EuroWordNet: A multilingual database with lexical semantic networks,,27-121,1998.0,https://patterson.com/blog/categorymain.asp,Computer Science
1851,977fe5853db16e320917a43fb00f334456625a1e,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.","
          289-91
        ",2000.0,http://www.evans.com/categorieslogin.php,Biology
1852,a0883d134b5abb7928483eb0859832a66a51fbf9,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",D227 - D230,2005.0,http://martin.com/search/listregister.htm,Biology
1853,695557ab15e44bee66d532d52b81a37decd87d70,The Object-Oriented Database System Manifesto,,223-240,1994.0,http://www.edwards-delacruz.com/blog/tags/categoriesmain.html,Computer Science
1854,bf9e27a62e100e46c5060c7ea79a0d97ce6c1a79,An atomic and molecular database for analysis of submillimetre line observations,"Atomic and molecular data for the transitions of a number of astrophysically interesting species are summarized, in- cluding energy levels, statistical weights, Einstein A-coefficients and collisional rate coefficients. Available collisional data from quantum chemical calculations and experiments are extrapolated to higher energies (up to E/k ∼ 1000 K). These data, which are made publically available through the WWW at http://www.strw.leidenuniv.nl/∼moldata, are essential input for non-LTE line radiative transfer programs. An online version of a computer program for performing statistical equilibrium calcu- lations is also made available as part of the database. Comparisons of calculated emission lines using different sets of collisional rate coefficients are presented. This database should form an important tool in analyzing observations from current and future (sub)millimetre and infrared telescopes.",369-379,2004.0,http://love.biz/tag/search/categorysearch.htm,Physics
1855,c3b16176728c7f785802f84df5aacffbc82ad431,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",105-133,1977.0,https://wilson.net/searchfaq.html,Mathematics
1856,2acf7e58f0a526b957be2099c10aab693f795973,Bosphorus Database for 3D Face Analysis,,47-56,2008.0,http://www.johnson.info/list/blog/searchabout.htm,Psychology
1857,db45667093e4fa4f95bc402c10b460052119717f,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",D744 - D748,2013.0,http://www.miller.com/tag/tagshome.jsp,Computer Science
1858,c2eb8cbfa71ead3e30d08fa5f2712a51950c6a40,"Principles of database and knowledge-base systems, Vol. I",,99-140,1988.0,https://www.henderson.biz/category/categories/wp-contentsearch.html,Computer Science
1859,782d8e30d599e1499555268b5b97c4a86b6bc25b,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",D29 - D33,2004.0,http://short.com/category/wp-content/appterms.htm,Medicine
1860,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",W525 - W530,2012.0,http://www.nelson.com/tag/search/postsindex.html,Biology
1861,0758a501039f9e2dfb7607507f9734155c52c7fc,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",D1104 - D1114,2012.0,http://www.taylor.net/app/mainsearch.php,Biology
1862,aa7b1246e367b5a1154bdc877558a8a4a8474f96,The MRC Psycholinguistic Database,"This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",497 - 505,1981.0,http://www.wright.info/tagshome.asp,Computer Science
1863,47e7dc1724b5a3c12154c134f898c58ca4e9c49c,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/","
          73-9
        ",1998.0,http://www.rivas.com/category/app/categorieshomepage.htm,Biology
1864,8a61d0e598d1a1d47fa4f744081cd39255a3f508,TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening In Silico,"Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.",85-142,2011.0,http://www.le.com/categoriesabout.html,Medicine
1865,755ef09cc0f7593b792482edc5bf799138243acf,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,,468-477,2011.0,https://www.pearson-moon.biz/search/tagindex.asp,Psychology
1866,ff9186e43abd68e55fbcb9ba992944c7497bacab,Repbase update: a database and an electronic journal of repetitive elements.,,"
          418-20
        ",2000.0,http://www.brown-obrien.com/listpost.jsp,Biology
1867,999db6b11c1fe6377118081c84f79f6ae6b4262d,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",D343 - D350,2011.0,https://www.brewer.biz/wp-contentmain.html,Computer Science
1868,638f10c6cc396907b98424621f6420a4287d342f,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.","
          181-4
        ",2001.0,https://www.marquez-cunningham.org/mainmain.htm,Medicine
1869,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.","
          37-40
        ",2001.0,http://www.brewer.org/app/wp-contentlogin.asp,Biology
1870,5f47123f5d86019c79c89f75ef6b44a60039f347,SCface – surveillance cameras face database,,863-879,2011.0,http://moore-martinez.com/tag/blogauthor.html,Computer Science
1871,d70c182a71aea05a145391b24d6bc3cdeede32a5,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",65-112,2001.0,http://jackson-grant.biz/wp-content/explore/exploreindex.php,Mathematics
1872,6c26791be6a51844f2784cd402876b18f110c5e4,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.","
          D452-5
        ",2004.0,https://www.stewart-stone.com/app/blogindex.asp,Computer Science
1873,df0708235e6c40899f7c9c14dff25ea8b86fdd19,The ClinicalTrials.gov results database--update and key issues.,"BACKGROUND
The ClinicalTrials.gov trial registry was expanded in 2008 to include a database for reporting summary results. We summarize the structure and contents of the results database, provide an update of relevant policies, and show how the data can be used to gain insight into the state of clinical research.


METHODS
We analyzed ClinicalTrials.gov data that were publicly available between September 2009 and September 2010.


RESULTS
As of September 27, 2010, ClinicalTrials.gov received approximately 330 new and 2000 revised registrations each week, along with 30 new and 80 revised results submissions. We characterized the 79,413 registry and 2178 results of trial records available as of September 2010. From a sample cohort of results records, 78 of 150 (52%) had associated publications within 2 years after posting. Of results records available publicly, 20% reported more than two primary outcome measures and 5% reported more than five. Of a sample of 100 registry record outcome measures, 61% lacked specificity in describing the metric used in the planned analysis. In a sample of 700 results records, the mean number of different analysis populations per study group was 2.5 (median, 1; range, 1 to 25). Of these trials, 24% reported results for 90% or less of their participants.


CONCLUSIONS
ClinicalTrials.gov provides access to study results not otherwise available to the public. Although the database allows examination of various aspects of ongoing and completed clinical trials, its ultimate usefulness depends on the research community to submit accurate, informative data.","
          852-60
        ",2011.0,http://www.williams.com/tags/category/postssearch.php,Medicine
1874,633888a9e6ac257c3e1e3d480525231c1627dc8d,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",D195 - D201,2010.0,https://kirby.com/tags/categories/postsindex.php,Computer Science
1875,7a350beede1b8eda39ce22bca62732bcd6677ebd,Concurrency Control in Distributed Database Systems,"In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each subproblem and show how to combine these techniques into algorithms for solving the entire concurrency control problem. Such algorithms are called ""concurrency control methods."" We describe 48 principal methods, including all practical algorithms that have appeared m the literature plus several new ones. We concentrate on the structure and correctness of concurrency control algorithms. Issues of performance are given only secondary treatment.",185-221,1986.0,http://www.cummings-burke.biz/wp-content/wp-contentprivacy.html,Computer Science
1876,61076194ec631a89daa30edbcc90bc7be37804cc,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",D492 - D496,2009.0,https://www.russell.com/explore/wp-content/blogfaq.jsp,Biology
1877,fa3c3fb3db6d54105c7990b6fd3ef41f3aff439d,Human immunodeficiency virus reverse transcriptase and protease sequence database,"The HIV reverse transcriptase and protease sequence database is an on-line relational database that catalogues evolutionary and drug-related sequence variation in the human immunodeficiency virus (HIV) reverse transcriptase (RT) and protease enzymes, the molecular targets of antiretroviral therapy (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences, including submissions to GenBank, sequences published in journal articles and sequences of HIV isolates from persons participating in clinical trials. Sequences are linked to data about the source of the sequence, the antiretroviral drug treatment history of the person from whom the sequence was obtained and the results of in vitro drug susceptibility testing. Sequence data on two new molecular targets of HIV drug therapy--gp41 (cell fusion) and integrase--will be added to the database in 2003.","
          298-303
        ",2003.0,https://www.lucas-jones.com/tagpost.php,Medicine
1878,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,"The PROSITE database, its status in 1999","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.","
          215-9
        ",1999.0,https://waters.net/wp-content/categories/postspost.asp,Biology
1879,4d130f394bd16320ac41112eebd3af74a129c6be,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",82-148,2012.0,https://wilcox.com/wp-contentcategory.html,Physics
1880,b0bf5eb499c483b94efd57135cf9572f2bb4bb8f,Systemic Banking Crises Database; An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",1,2012.0,https://sherman.com/listcategory.asp,Economics
1881,250718af678dedb2938775f9ca6fb8601749778a,Social Conflict in Africa: A New Database,"We describe the Social Conflict in Africa Database (SCAD), a new event dataset for conducting research and analysis on various forms of social and political unrest in Africa. SCAD contains information on over 7,200 instances of protests, riots, strikes, government repression, communal violence, and other forms of unrest for 47 African countries from 1990–2010. SCAD includes information on event dates, actors and targets, lethality, georeferenced location information, and other conflict attributes. This article gives an overview of the data collection process, presents descriptive statistics and trends across the continent, and compares SCAD to the widely used Banks event data. We believe that SCAD will be a useful resource for scholars across multiple disciplines as well as for the policy community.",503 - 511,2012.0,http://johnson-mason.com/app/appindex.jsp,Political Science
1882,7fbb81ee7df0f48b3e55649cf41bcc506d19b314,The American Mineralogist crystal structure database,"A database has been constructed that contains all the crystal structures previously published in the American Mineralogist. The database is called “The American Mineralogist Crystal Structure Database” and is freely accessible from the websites of the Mineralogical Society of America at http://www.minsocam.org/MSA/Crystal_Database.html and the University of Arizona. In addition to the database, a suite of interactive software is provided that can be used to view and manipulate the crystal structures and compute different properties of a crystal such as geometry, diffraction patterns, and procrystal electron densities. The database is set up so that the data can be easily incorporated into other software packages. Included at the website is an evolving set of guides to instruct the user and help with classroom education. parameters; (5) incorporating comments from either the original authors or ourselves when changes are made to the originally published data. Each record in the database consists of a bibliographic reference, cell parameters, symmetry, atomic positions, displacement parameters, and site occupancies. An example of a data set is provided in Figure 1. The first part of each data set contains identifying information, bibliography and notes, while the second part contains the crystallographic parameters. The first line of a data file contains an identifier, such as the name of the mineral or formula of the chemical species. The next line(s) contain the names of the authors, each separated by a comma. This is followed by the journal reference, title of the paper, and additional notes. The crystallographic data begins with a listing of the cell parameters and space group. If the data is given with respect to a non-standard space group origin then an asterisk precedes the space group symbol and the next line contains the translation vector from the standard origin. The 1952 edition of the International Tables for X-ray Crystallography are used to define the standard origin. The rest of the data set is a fixed-formatted listing of the atoms, their positional and displacement parameters, and occupancies. A header is provided that defines rightjustified columns. The name of each atom identifies the occupying elements, with additional identifiers added when appropriate. For instance, “Oco” identifies a particular oxygen atom in the albite structure. Some data sets report a crystallographic site occupied by molecular species rather than elemental, such as OH, water or methane. In most of these cases the atom name is denoted by molecular formula. For example, “CH4” denotes methane, and “Wat” denotes water. The displacement factors are tabulated in one of two formats, U’s or b’s",247-250,2003.0,http://www.hernandez.com/main/list/apppost.asp,Chemistry
1883,351bbaa6d0b597175a17f59f822c8e0d1fdebe03,Defining and cataloging exoplanets: the exoplanet.eu database,"We describe an online database for extrasolar planetary-mass candidates, which is updated regularly as new data are available. We first discuss criteria for inclusion of objects in the catalog: “definition” of a planet and several aspects of the confidence level of planet candidates. We are led to point out the contradiction between the sharpness of criteria for belonging to a catalog and the fuzziness of the confidence level for an object to be a planet. We then describe the different tables of extrasolar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: histograms of planet and host star data, cross-correlations between these parameters, and some Virtual Observatory services. Future evolutions of the database are presented.",79-90,2011.0,http://patterson.com/mainmain.asp,Physics
1884,cd7763d7c118bc875ea34b30b52d0d95257b1418,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,"
          238-41
        ",1996.0,https://www.white-may.net/search/searchsearch.asp,Biology
1885,a6859f695e6b2bd967df7cdb8becf8c9465b472a,The 'Dresden Image Database' for benchmarking digital image forensics,"This paper introduces and documents a novel image database specifically built for the purpose of development and bench-marking of camera-based digital forensic techniques. More than 14,000 images of various indoor and outdoor scenes have been acquired under controlled and thus widely comparable conditions from altogether 73 digital cameras. The cameras were drawn from only 25 different models to ensure that device-specific and model-specific characteristics can be disentangled and studied separately, as validated with results in this paper. In addition, auxiliary images for the estimation of device-specific sensor noise pattern were collected for each camera. Another subset of images to study model-specific JPEG compression algorithms has been compiled for each model. The 'Dresden Image Database' will be made freely available for scientific purposes when this accompanying paper is presented. The database is intended to become a useful resource for researchers and forensic investigators. Using a standard database as a benchmark not only makes results more comparable and reproducible, but it is also more economical and avoids potential copyright and privacy issues that go along with self-sampled benchmark sets from public photo communities on the Internet.",1584-1590,2010.0,http://www.evans.net/appregister.asp,Computer Science
1886,dd79f74b9f5537ceecd097563a20546dd60937f6,The VizieR database of astronomical catalogues,"VizieR is a database grouping in an homoge- neous way thousands of astronomical catalogues gath- ered for decades by the Centre de Donn ees de Strasbourg (CDS) and participating institutes. The history and cur- rent status of this large collection is briefly presented, and the way these catalogues are being standardized to t in the VizieR system is described. The architecture of the database is then presented, with emphasis on the man- agement of links and of accesses to very large catalogues. Several query interfaces are currently available, making use of the ASU protocol, for browsing purposes or for use by other data processing systems such as visualisa- tion tools.",23-32,2000.0,http://munoz.info/categorysearch.html,Physics
1887,48671641597e73e7e2ba3067a029d7e632fc5e59,An Overview of the Global Historical Climatology Network Temperature Database,"Abstract The Global Historical Climatology Network version 2 temperature database was released in May 1997. This century-scale dataset consists of monthly surface observations from ∼7000 stations from around the world. This archive breaks considerable new ground in the field of global climate databases. The enhancements include 1) data for additional stations to improve regional-scale analyses, particularly in previously data-sparse areas; 2) the addition of maximum–minimum temperature data to provide climate information not available in mean temperature data alone; 3) detailed assessments of data quality to increase the confidence in research results; 4) rigorous and objective homogeneity adjustments to decrease the effect of nonclimatic factors on the time series; 5) detailed metadata (e.g., population, vegetation, topography) that allow more detailed analyses to be conducted; and 6) an infrastructure for updating the archive at regular intervals so that current climatic conditions can constantly be put...",2837-2849,1997.0,http://hall.com/category/appfaq.asp,Physics
1888,798e312dd67798024da74f9a8f92946af88c7cd4,Comparative study of retinal vessel segmentation methods on a new publicly available database,"In this work we compare the performance of a number of vessel segmentation algorithms on a newly constructed retinal vessel image database. Retinal vessel segmentation is important for the detection of numerous eye diseases and plays an important role in automatic retinal disease screening systems. A large number of methods for retinal vessel segmentation have been published, yet an evaluation of these methods on a common database of screening images has not been performed. To compare the performance of retinal vessel segmentation methods we have constructed a large database of retinal images. The database contains forty images in which the vessel trees have been manually segmented. For twenty of those forty images a second independent manual segmentation is available. This allows for a comparison between the performance of automatic methods and the performance of a human observer. The database is available to the research community. Interested researchers are encouraged to upload their segmentation results to our website (http://www.isi.uu.nl/Research/Databases). The performance of five different algorithms has been compared. Four of these methods have been implemented as described in the literature. The fifth pixel classification based method was developed specifically for the segmentation of retinal vessels and is the only supervised method in this test. We define the segmentation accuracy with respect to our gold standard as the performance measure. Results show that the pixel classification method performs best, but the second observer still performs significantly better.",91-145,2004.0,https://adams.com/wp-content/categories/mainfaq.html,Computer Science
1889,062cea54e5d58ee41aea607cbf2ba0cf457aa4e7,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",1-10,2007.0,https://www.navarro.org/posts/posts/searchpost.php,Computer Science
1890,a7352bf3b88df27f0d0faa9d7ee7198a8b304c1d,Development and use of a database of hydraulic properties of European soils,,169-185,1999.0,http://stone-ramirez.com/explore/wp-contentpost.htm,Environmental Science
1891,73f072aac4f44b860ab8eefd428573dfad3d44fc,MIPS: a database for genomes and protein sequences.,"The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de).","
          31-4
        ",2002.0,https://www.villegas.net/explore/list/tagterms.jsp,Medicine
1892,97dcab33aa0f1b8c98eec95e52e13596f3fb890d,The ecoinvent Database: Overview and Methodological Framework (7 pp),,3-9,2005.0,https://thomas-baldwin.net/posts/posts/appmain.php,Engineering
1893,816b0957fa05347951a1b37c29e21b67d9257c91,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,"
          304-5
        ",2000.0,http://www.king.com/tags/searchregister.php,Biology
1894,0d6b4182d465f70e359e30372e550121a0fc94b0,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/","
          257-9
        ",1998.0,https://myers-sutton.com/app/posts/blogauthor.php,Medicine
1895,c4907ef7d044ad71cc8b292c8b1e146987422ec7,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",D1234 - D1240,2012.0,https://watson-franklin.info/explore/tagsauthor.htm,Computer Science
1896,fe134631e96a8937b1cc93952e895d882c536655,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",D15 - D18,2010.0,https://www.bell.com/categoriesindex.php,Computer Science
1897,2c9b060388b88841cf8095cd9efcbfeb805357f6,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",D400 - D412,2011.0,http://torres.com/tag/tags/wp-contenthomepage.html,Computer Science
1898,9f37bd6500bcc8ed946c0fd3dc9deb6334c24c12,CHIANTI—AN ATOMIC DATABASE FOR EMISSION LINES. XII. VERSION 7 OF THE DATABASE,"The CHIANTI spectral code consists of an atomic database and a suite of computer programs to calculate the optically thin spectrum of astrophysical objects and carry out spectroscopic plasma diagnostics. The database includes atomic energy levels, wavelengths, radiative transition probabilities, collision excitation rate coefficients, and ionization and recombination rate coefficients, as well as data to calculate free–free, free–bound, and two-photon continuum emission. Version 7 has been released, which includes several new ions, significant updates to existing ions, as well as Chianti-Py, the implementation of CHIANTI software in the Python programming language. All data and programs are freely available at http://www.chiantidatabase.org, while the Python interface to CHIANTI can be found at http://chiantipy.sourceforge.net.",91-105,2012.0,http://browning.com/taghomepage.jsp,Physics
1899,711d5205c29fd772e22521cc4f9150db2f338d8e,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",96,2000.0,http://www.peters-woodard.net/categories/searchcategory.htm,Physics
1900,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",D841 - D846,2011.0,http://www.jones.info/tags/searchauthor.php,Medicine
1901,251c272eef27fed72dd4e4e07c202f20e6dbd55a,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.","
          171-3
        ",1999.0,https://www.jackson.com/search/searchprivacy.htm,Biology
1902,5c06b60a6940df55271fe5917848abf7ab3ca706,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.","
          109-11
        ",1997.0,https://thomas.info/main/category/appmain.htm,Computer Science
1903,0d74d07ee64ae04e01a1893d798a93680a9211c8,Conceptual Database Design: An Entity-Relationship Approach,I. CONCEPTUAL DATABASE DESIGN. 1. An Introduction to Database Design. 2. Data Modeling Concepts. 3. Methodologies for Conceptual Design. 4. View Design. 5. View Integration. 6. Improving the Quality of a Database Schema. 7. Schema Documentation and Maintenance. II. FUNCTIONAL ANALYSIS FOR DATABASE DESIGN. 1. Functional Analysis Using the Dataflow Model. 2. Joint Data and Functional Analysis. 3. Case Study. III. LOGICAL DESIGN AND DESIGN TOOLS. 1. High-Level Logical Design Using the Entity-Relationship Model. 2. Logical Design for the Relational Model. 3. Logical Design for the Network Model. 4. Logical Design for the Hierarchical Model. 5. Database Design Tools. Index. 0805302441T04062001,15-108,1991.0,https://www.garcia.com/tags/tags/categoryhomepage.php,Computer Science
1904,83a500fcc7cd98db063b73461277ac885c8fe7c3,Towards Sensor Database Systems,,3-14,2001.0,https://cruz.com/app/category/tagscategory.html,Computer Science
1905,440819897d051bdb57182fd2a61777c7a8b710b7,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",D637 - D640,2007.0,https://www.white.com/tag/categories/categoriesmain.html,Medicine
1906,49578a040f3346f81759ac40cc174cd12cb40045,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",D102 - D106,2007.0,http://www.james.com/blog/blog/postscategory.php,Medicine
1907,17891bdbfec1950f7c361db96dca043cfbf54769,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.","
          371-3
        ",2003.0,http://www.sanchez-clark.org/blogprivacy.htm,Computer Science
1908,09d73eecceb080eb1f7cea71d7df1411c712baf6,The National Land Cover Database,,24-111,2012.0,https://hicks.com/postshomepage.jsp,Environmental Science
1909,5db8051dd2ee99996484bf1c48795c5ca13e04c5,XCOM: Photon Cross Section Database (version 1.2),,69-104,1999.0,http://www.bryant.com/wp-content/explore/postshomepage.html,Materials Science
1910,8b683b12f9efc1d8bdf330182a0afd7c14369ce1,The Comparative Toxicogenomics Database: update 2011,"The Comparative Toxicogenomics Database (CTD) is a public resource that promotes understanding about the interaction of environmental chemicals with gene products, and their effects on human health. Biocurators at CTD manually curate a triad of chemical–gene, chemical–disease and gene–disease relationships from the literature. These core data are then integrated to construct chemical–gene–disease networks and to predict many novel relationships using different types of associated data. Since 2009, we dramatically increased the content of CTD to 1.4 million chemical–gene–disease data points and added many features, statistical analyses and analytical tools, including GeneComps and ChemComps (to find comparable genes and chemicals that share toxicogenomic profiles), enriched Gene Ontology terms associated with chemicals, statistically ranked chemical–disease inferences, Venn diagram tools to discover overlapping and unique attributes of any set of chemicals, genes or disease, and enhanced gene pathway data content, among other features. Together, this wealth of expanded chemical–gene–disease data continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases. CTD is freely available at http://ctd.mdibl.org.",D1067 - D1072,2010.0,https://www.cisneros.net/explore/tags/mainregister.jsp,Biology
1911,ad0494f2a46b4272530a79f274bab28ae6269acc,"Database Systems: A Practical Approach to Design, Implementation and Management","This best-selling text introduces the theory behind databases in a concise yet comprehensive manner, providing database design methodology that can be used by both technical and non-technical readers. The methodology for relational Database Management Systems is presented in simple, step-by-step instructions in conjunction with a realistic worked example using three explicit phasesconceptual, logical, and physical database design. Background: Introduction to Databases; Database Environment; Database Architectures and the Web. The Relational Model and Languages: The Relational model; Relational Algebra and Relational Calculus; SQL: Data Manipulation; SQL: Data Definition; Query-By-Example (QBE). Database Analysis and Design: Database System Lifecycle; Database Analysis and the DreamHome Case Study; EntityRelationship Modeling; Enhanced EntityRelationship Modeling; Normalization; Advanced Normalization. Methodology: MethodologyConceptual Database Design; MethodologyLogical Database Design for Relational Model; MethodologyPhysical Database Design for Relational Databases; MethodologyMonitoring and Tuning the Operational System. Selected Database Issues: Security and Administration; Professional, Legal, and Ethical Issues; Transaction Management; Query Processing. Distributed DBMSs and Replication: Distributed DBMSsConcepts and Design; Distributed DBMSsAdvanced Concepts; Replication and Mobile Databases. Object DBMSs: Object-Oriented DBMSsConcepts and Design; Object-Oriented DBMSsStandards and Languages; Object-Relational DBMSs. Web and DBMSs: Web Technology and DBMSs; Semistructured Data and XML. Business Intelligence Technologies: Data Warehousing Concepts; Data Warehousing Design; OLAP; Data Mining. Appendices: Users' Requirements Specification for DreamHome Case Study; Other Case Studies; Alternative Data Modeling Notations; Summary of the Database Design Methodology for Relational Databases; Introduction to PyrrhoA Liteweight RDBMS. Web Appendices: File Organization and Storage Structures; When Is a DBMS Relational?; Commercial DBMSs: Access and Oracle; Programmatic SQL; Estimating Disk Space Requirements; Introduction to Object-Orientation; Example Web Scripts. This book is ideal for readers interested in database management or database design.",89-118,1998.0,http://rodriguez.com/posts/categorieshome.jsp,Computer Science
1912,40691c4ba3c2eb44849273eb92dca66da6b634ad,NIST Atomic Spectra Database (version 2.0),,47-127,1999.0,http://tucker.com/categoriesmain.html,Materials Science
1913,9d79185d82c03c30778f3635bfbdcf605330f41b,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",D854 - D862,2009.0,http://www.duran.org/list/appcategory.htm,Computer Science
1914,4deebd0fdcec477a780e950dc0299beb872ea350,"BRENDA , the enzyme database : updates and major new developments","BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83 000 different enzymes from 9800 different organisms, classi®ed in ~4200 EC numbers. BRENDA includes biochemical and molecular information on classi®cation and nomenclature, reaction and speci®city, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from ~46 000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with >50% of all data updated in 2002 or in the ®rst half of 2003, the development of a new EC-tree browser, a taxonomy-tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information ®elds and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni-koeln.de.",77-121,2003.0,https://hawkins.com/main/category/mainlogin.html,Technology
1915,7463a0b934ac40a353773840485bb56d35fbbb66,Database on medicinal plants used in Ayurveda,,87-107,2000.0,https://www.fox.com/blog/bloghome.htm,Medicine
1916,17e6076b6761788684434d1e14e85e8877fc0146,LandScan: A Global Population Database for Estimating Populations at Risk,"The LandScan Global Population Project produced a world-wide 1998 population database at a 30-by 30-second resolution for estimating ambient populations at risk. Best available census counts were distributed to cells based on probability coefficients which, in turn, were based on road proximity, slope, land cover, and nighttime lights, LandScan 1998 has been completed for the entire world. Verification and validation (V&V) studies were conducted routinely for all regions and more extensively for Israel, Germany, and the southwestern United States. Geographic information systems (GIS) were essential for conflation of diverse input variables, computation of probability coefficients, allocation of population to cells, and reconciliation of cell totals with aggregate (usually province) control totals. Remote sensing was an essential source of two input variables-land cover and nighttime lights-and one ancillary database-high-resolution panchromatic imagery-used in V&V of the population model and resulting LandScan database.",849-858,2000.0,https://jones.com/searchregister.html,Geography
1917,2033531aeaf7d0da158cdaacae9b208407bd4a1c,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).","
          368-9
        ",1999.0,http://wolf-ayers.biz/blog/posts/wp-contentauthor.html,Computer Science
1918,5bf9cebe3658cfbf7f67c0a2680c8233509aa5e4,UCI Repository of Machine Learning Database,,27-101,1998.0,https://richmond.org/category/main/explorelogin.asp,Computer Science
1919,5524cacaae93810945f1b21e77f565f6c8bdcdef,Relational Cloud: a Database Service for the cloud,"This paper introduces a new transactional “database-as-a-service” (DBaaS) called Relational Cloud. A DBaaS promises to move much of the operational burden of provisioning, configuration, scaling, performance tuning, backup, privacy, and access control from the database users to the service operator, offering lower overall costs to users. Early DBaaS efforts include Amazon RDS and Microsoft SQL Azure, which are promising in terms of establishing the market need for such a service, but which do not address three important challenges: efficient multi-tenancy, elastic scalability, and database privacy. We argue that these three challenges must be overcome before outsourcing database software and management becomes attractive to many users, and cost-effective for service providers. The key technical features of Relational Cloud include: (1) a workload-aware approach to multi-tenancy that identifies the workloads that can be co-located on a database server, achieving higher consolidation and better performance than existing approaches; (2) the use of a graph-based data partitioning algorithm to achieve near-linear elastic scale-out even for complex transactional workloads; and (3) an adjustable security scheme that enables SQL queries to run over encrypted data, including ordering operations, aggregates, and joins. An underlying theme in the design of the components of Relational Cloud is the notion of workload awareness: by monitoring query patterns and data accesses, the system obtains information useful for various optimization and security functions, reducing the configuration effort for users and operators.",235-240,2011.0,http://roberts.org/tag/main/listhome.html,Computer Science
1920,fb2896bb515ad483260f2b937202d0e7289ddd16,SDUMLA-HMT: A Multimodal Biometric Database,,260-268,2011.0,https://scott-hall.com/tag/main/wp-contentpost.php,Computer Science
1921,369f62edea6e6ace6f68c7ebe9bdde046b9514ad,The asteroid lightcurve database,,134-146,2009.0,http://www.perez.net/wp-content/explore/appfaq.asp,Mathematics
1922,cc42c8dac7c3fb8cd522136f1c7c31ae45a3121f,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.","
          3453-9
        ",1998.0,http://cruz.org/listlogin.htm,Biology
1923,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",D623 - D631,2007.0,https://www.bell.biz/main/app/appregister.php,Medicine
1924,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",23 - 44,2008.0,http://flowers-combs.net/categoriessearch.php,Computer Science
1925,9e9801ef47bff55074165440a17d5122520837a2,Concurrency Control and Recovery in Database Systems,"This book is an introduction to the design and implementation of concurrency control and recovery mechanisms for transaction management in centralized and distributed database systems. Concurrency control and recovery have become increasingly important as businesses rely more and more heavily on their on-line data processing activities. For high performance, the system must maximize concurrency by multiprogramming transactions. But this can lead to interference between queries and updates, which concurrency control mechanisms must avoid. In addition, a satisfactory recovery system is necessary to ensure that inevitable transaction and database system failures do not corrupt the database.",47-127,1987.0,https://www.may.com/app/categoryhomepage.php,Computer Science
1926,a047888622c576ccf06a7708ae18a5d9ec5f09fd,Lexique 2 : A new French lexical database,,516-524,2004.0,http://www.hernandez.com/categoriesmain.php,Computer Science
1927,075082cfbedfe3161d15354b31859ca59dfbeafb,A global database of soil respiration data,"Abstract. Soil respiration – RS, the flux of CO2 from the soil to the atmosphere – is probably the least well constrained component of the terrestrial carbon cycle. Here we introduce the SRDB database, a near-universal compendium of published RS data, and make it available to the scientific community both as a traditional static archive and as a dynamic community database that may be updated over time by interested users. The database encompasses all published studies that report one of the following data measured in the field (not laboratory): annual RS, mean seasonal RS, a seasonal or annual partitioning of RS into its sources fluxes, RS temperature response (Q10), or RS at 10 °C. Its orientation is thus to seasonal and annual fluxes, not shorter-term or chamber-specific measurements. To date, data from 818 studies have been entered into the database, constituting 3379 records. The data span the measurement years 1961–2007 and are dominated by temperate, well-drained forests. We briefly examine some aspects of the SRDB data – its climate space coverage, mean annual RS fluxes and their correlation with other carbon fluxes, RS variability, temperature sensitivities, and the partitioning of RS source flux – and suggest some potential lines of research that could be explored using these data. The SRDB database is available online in a permanent archive as well as via a project-hosting repository; the latter source leverages open-source software technologies to encourage wider participation in the database's future development. Ultimately, we hope that the updating of, and corrections to, the SRDB will become a shared project, managed by the users of these data in the scientific community.",1915-1926,2010.0,http://www.lopez.info/categorysearch.htm,Environmental Science
1928,369e98d934881e9cfd464a73e56011cb807ab104,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",,95-98,2001.0,http://www.walker-fernandez.com/wp-content/wp-content/wp-contentterms.php,Physics
1929,353988f8d56224b9d46aa34059e499c638dcbc2e,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",746-749,1996.0,https://cannon-hernandez.biz/tags/tagslogin.php,Engineering
1930,2a92c98a943ae21360d52730f1f2117c7edb2ecb,The Regulation and Supervision of Banks around the World: A New Database,"International consultants on bank regulation, and supervision for developing countries, often base their advice on how their home country does things, for lack of information on practice in other countries. Recommendations for reform have tended to be shaped by bias rather than facts. To better inform advice about bank regulation, and supervision, and to lower the marginal cost of empirical research, the authors present, and discuss a new, and comprehensive database on the regulation, and supervision of banks in a hundred and seven countries. The data, based on surveys sent to national bank regulatory, supervisory authorities, are now available to researchers, and policymakers around the world. The data cover such aspects of banking as entry requirements, ownership restrictions, capital requirements, activity restrictions, external auditing requirements, characteristics of deposit insurance schemes, loan classification and provisioning requirements, accounting and disclosure requirements, troubled bank resolution actions, and (uniquely) the quality of supervisory personnel, and their actions. The database permits users to learn how banks are currently regulated, and supervised, and about bank structures, and deposit insurance schemes, for a broad cross-section of countries. In addition to describing the data, the authors show how variables ay be grouped, and aggregated. They also show some simple correlations among selected variables. In a comparison paper (""Bank regulation and supervision: What works best"") studying the relationship between differences in bank regulation and supervision, and bank performance and stability, they conclude that: 1) Countries with policies that promote private monitoring of banks, have better bank performance, and more stability. Countries with more generous deposit insurance schemes tend to have poorer bank performance, and more bank fragility. 2) Diversification of income streams, and loan portfolios - by not restricting bank activities - also tends to improve performance, and stability. (This works best when an active securities market exists). Countries in which banks are encouraged to diversify their portfolios, domestically and internationally, suffer fewer crisis.",183 - 240,2001.0,http://dennis.com/search/wp-content/mainauthor.html,Business
1931,b4ea6e57966ffdab58ec410e085acc1232064303,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",D284 - D288,2004.0,http://sims.com/mainindex.html,Biology
1932,794c048acd3d2d35e3248161729fcf142b4966c6,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",D240 - D244,2007.0,http://www.orozco-williams.com/categories/search/searchabout.htm,Medicine
1933,54f4ef17f6b7a315260e001af85fc8b6fc1d2a7b,The UCSC Genome Browser Database: update 2009,"The UCSC Genome Browser Database (GBD, http://genome.ucsc.edu) is a publicly available collection of genome assembly sequence data and integrated annotations for a large number of organisms, including extensive comparative-genomic resources. In the past year, 13 new genome assemblies have been added, including two important primate species, orangutan and marmoset, bringing the total to 46 assemblies for 24 different vertebrates and 39 assemblies for 22 different invertebrate animals. The GBD datasets may be viewed graphically with the UCSC Genome Browser, which uses a coordinate-based display system allowing users to juxtapose a wide variety of data. These data include all mRNAs from GenBank mapped to all organisms, RefSeq alignments, gene predictions, regulatory elements, gene expression data, repeats, SNPs and other variation data, as well as pairwise and multiple-genome alignments. A variety of other bioinformatics tools are also provided, including BLAT, the Table Browser, the Gene Sorter, the Proteome Browser, VisiGene and Genome Graphs.",D755 - D761,2008.0,http://valentine.com/tags/categories/exploreabout.php,Biology
1934,523a87607f06f7ed56a0506bdb4671f76244264a,Introducing the Global Terrorism Database,"Compared to most types of criminal violence, terrorism poses special data collection challenges. In response, there has been growing interest in open source terrorist event data bases. One of the major problems with these data bases in the past is that they have been limited to international events—those involving a national or group of nationals from one country attacking targets physically located in another country. Past research shows that domestic incidents greatly outnumber international incidents. In this paper we describe a previously unavailable open source data base that includes some 70,000 domestic and international incidents since 1970. We began the Global Terrorism Database (GTD) by computerizing data originally collected by the Pinkerton Global Intelligence Service (PGIS). Following computerization, our research team has been working for the past two years to validate and extend the data to real time. In this paper, we describe our data collection efforts, the strengths and weaknesses of open source data in general and the GTD in particular, and provide descriptive statistics on the contents of this new resource.",181 - 204,2007.0,https://www.jones.org/categories/posts/blogcategory.php,Computer Science
1935,a5881da1c592ea11d24f90992f5b210beaa3ea73,Gigascope: a stream database for network applications,"We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.",647-651,2003.0,http://www.fox-weber.org/main/app/exploreauthor.html,Computer Science
1936,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",D334 - D337,2004.0,http://www.matthews-weaver.info/app/listhomepage.html,Medicine
1937,78597d4989a7f7f892067637dd3271e60569b087,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",D786 - D793,2006.0,http://www.perez.com/tagscategory.php,Biology
1938,9aa8a679e3401f1bbf805b738095bf2ed52e08e7,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,15-121,2009.0,http://avery.net/listterms.html,Biology
1939,ed431581f9537896d26b7c8d9935dce9ee73871d,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",D33 - D37,2011.0,http://hart.com/tags/explore/appcategory.htm,Computer Science
1940,48fa4530c0eddf525b273e222753c978606243f7,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.","
          D590-2
        ",2004.0,http://www.cobb.com/tag/explore/tagslogin.html,Computer Science
1941,978a4276e0874a590d34aef84e6238ce21f0539e,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",D476 - D480,2004.0,https://www.patel-hayes.com/posts/tags/exploreterms.htm,Biology
1942,a5375b684c8e6640246df2eaec5f59b2ef94242b,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.","
          383-7
        ",2003.0,http://www.herring.biz/wp-content/poststerms.htm,Biology
1943,e13a71f639bed2c1c739b206a6a053e8f18651a6,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",D689 - D691,2005.0,https://parker.com/main/postsmain.htm,Medicine
1944,714e6dc60dba65431ecae63310da52462defd9c9,The Object Database Standard: ODMG-93,,15-124,1993.0,http://neal.com/tag/postssearch.php,Computer Science
1945,e2a5593f587970018b9e826e5d876125b374b801,Integrating compression and execution in column-oriented database systems,"Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",56-145,2006.0,https://www.hall.com/explore/categoryregister.htm,Computer Science
1946,03094c68e9333dbfb17426d22d4e61748d92e414,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",D698 - D704,2010.0,http://www.castillo-simpson.biz/mainauthor.html,Biology
1947,a68f8e1f7b9f4144049537c766be3faec5b54786,The Exoplanet Orbit Database,"We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties. This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from radial velocity and transit measurements as reported in the literature. We have also compiled fundamental transit parameters, stellar parameters, and the method used for the planets discovery. This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles. The database is available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the data can be plotted and explored through the Exoplanet Data Explorer plotter. We use the Data Explorer to generate publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution: We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct semimajor-axis distribution from apparently singleton systems.",412 - 422,2010.0,http://www.moore-wright.com/mainindex.html,Physics
1948,2515e0eb321c558629678b0c34447b40725ed989,The BioGRID Interaction Database,,1-1,2011.0,http://www.ruiz-johnson.org/searchterms.html,Technology
1949,f472655c370e4b3209f35a2834e01fb4e77ade9b,NoSQL databases: a step to database scalability in web environment,"The paper is focused on so called NoSQL databases. In context of cloud computing, architectures and basic features of these databases are studied, particularly their horizontal scalability and concurrency model, that is mostly weaker than ACID transactions in relational SQL-like database systems. Some characteristics like a data model and querying capabilities are discussed in more detail. The paper also contains an overview of some representatives of NoSQL databases.",69-82,2011.0,https://www.aguilar.info/wp-content/wp-contentprivacy.htm,Computer Science
1950,51f3fbc8b948dd93ed6d1e27e320141d0507603d,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",65-130,2012.0,http://moody.com/list/exploreregister.jsp,Chemistry
1951,4a30343f3230dddd96fd6f79547fef9407262dbf,A comparison of a graph database and a relational database: a data provenance perspective,"Relational databases have been around for many decades and are the database technology of choice for most traditional data-intensive storage and retrieval applications. Retrievals are usually accomplished using SQL, a declarative query language. Relational database systems are generally efficient unless the data contains many relationships requiring joins of large tables. Recently there has been much interest in data stores that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's BigTable and Facebook's Cassandra. This paper reports on a comparison of one such NoSQL graph database called Neo4j with a common relational database system, MySQL, for use as the underlying technology in the development of a software system to record and query data provenance information.",42,2010.0,http://www.zimmerman.com/search/postsprivacy.html,Computer Science
1952,0b115b72214b501923852e6278b20401fee27f85,Database Repairing and Consistent Query Answering,"Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely, a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers. The consistent data in an inconsistent database is usually characterized as the data that persists across all the database instances that are consistent and minimally differ from the inconsistent instance. Those are the so-called repairs of the database. In particular, the consistent answers to a query posed to the inconsistent database are those answers that can be simultaneously obtained from all the database repairs. As expected, the notion of repair requires an adequate notion of distance that allows for the comparison of databases with respect to how much they differ from the inconsistent instance. On this basis, the minimality condition on repairs can be properly formulated. In this monograph we present and discuss these fundamental concepts, different repair semantics, algorithms for computing consistent answers to queries, and also complexity-theoretic results related to the computation of repairs and doing consistent query answering. Table of Contents: Introduction / The Notions of Repair and Consistent Answer / Tractable CQA and Query Rewriting / Logically Specifying Repairs / Decision Problems in CQA: Complexity and Algorithms / Repairs and Data Cleaning",92-101,2011.0,https://www.walker.org/search/search/categoryauthor.htm,Computer Science
1953,e7ab23d011e5183db78cfea48e303210f6e57e2e,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",631-653,1979.0,https://morrison.com/app/exploresearch.html,Computer Science
1954,f86cefe05621180a48856c9f23a55bf587d7476b,Principles of Database Systems,,81-148,2004.0,https://greer.biz/app/searchindex.jsp,Computer Science
1955,b71ac5caa3a4335c311122cbacada6b17a199060,Saccharomyces Genome Database.,,"
          329-46
        ",2002.0,https://taylor.com/wp-contentlogin.html,Biology
1956,58efda5a28e5791adfde9ef6e330caf7b89349c6,Providing database as a service,"We explore a novel paradigm for data management in which a third party service provider hosts ""database as a service"", providing its customers with seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, a data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by ""database as a service"" are the additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. The paper is meant as a challenge for the database community to explore a rich set of research issues that arise in developing such a service.",29-38,2002.0,http://www.williams.com/posts/tagcategory.htm,Computer Science
1957,f836da820f53f5bbc890647ecbf00e1031f200c7,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",75-123,2004.0,https://www.ewing.com/tags/posts/tagsauthor.html,Biology
1958,67f8831944ecc502ce74c761bd7ae0d929b5e2f8,An Electronic Lexical Database,"""Natural language processing is essential for dealing efficiently with the large quantities of text now available online: fact extraction and summarization, automated indexing and text categorization, and machine translation. Another essential function is helping the user with query formulation through synonym relationships between words and hierarchical and other relationships between concepts. WordNet supports both of these functions and thus deserves careful study by the digital library community."" By Dagobert Soergel ds52@umail.umd.edu",71-105,1998.0,http://byrd-rose.com/exploreregister.php,Business
1959,fd6c52fe253f972a54102f43f7d6ee9827eeafd0,A high-resolution 3D dynamic facial expression database,"Face information processing relies on the quality of data resource. From the data modality point of view, a face database can be 2D or 3D, and static or dynamic. From the task point of view, the data can be used for research of computer based automatic face recognition, face expression recognition, face detection, or cognitive and psychological investigation. With the advancement of 3D imaging technologies, 3D dynamic facial sequences (called 4D data) have been used for face information analysis. In this paper, we focus on the modality of 3D dynamic data for the task of facial expression recognition. We present a newly created high-resolution 3D dynamic facial expression database, which is made available to the scientific research community. The database contains 606 3D facial expression sequences captured from 101 subjects of various ethnic backgrounds. The database has been validated through our facial expression recognition experiment using an HMM based 3D spatio-temporal facial descriptor. It is expected that such a database shall be used to facilitate the facial expression analysis from a static 3D space to a dynamic 3D space, with a goal of scrutinizing facial behavior at a higher level of detail in a real 3D spatio-temporal domain.",1-6,2008.0,https://www.yang.com/categories/appauthor.php,Computer Science
1960,dacc0018d0a0c45d93599751e53c91f88fcd45b8,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).","
          315-8
        ",2003.0,https://mendez-williams.com/search/blogterms.asp,Biology
1961,97232c7bba5bef3ac970bf82966a8ea97cb4fa14,WHO global database on child growth and malnutrition,"ii The designations employed and the presentation of material do not imply the expression of any opinion whatsoever on the part of the World Health Organization concerning the legal status of any country, territory or area, its authorities, its current or former official name or the delimitation of its frontiers or boundaries. We are guilty of many errors and many faults, but our worst crime is abandoning the children, neglecting the foundation of life. Many of the things we need can wait. The child cannot. Right now is the time his bones are being formed, his blood is being made and his senses are being developed. To him we cannot answer "" Tomorrow "". His name is "" Today "". We dedicate this work to the world's children in the hope that it will alert decision-makers to how much remains to be done to ensure children's healthy growth and development. "" "" WHO/NUT/97.4 iv Acknowledgements The Programme of Nutrition appreciates the strong support from numerous individuals, institutions, governments, and nongovernmental and international organizations, without whose continual collaboration this compilation would not have been possible. A special note of gratitude is due to all those who provided standardized information and reanalyses of original data sets to conform to the database requirements. Thanks to such international cooperation in keeping the Global Database up-to-date, the Programme of Nutrition is able to present this vast compilation of data on worldwide patterns and trends in child growth and malnutrition. SD Standard deviation WHO World Health Organization Z-score (or SD-score) The deviation of an individual's value from the median value of a reference population, divided by the standard deviation of the reference population.",67-139,1997.0,https://www.perez.com/main/category/listhomepage.php,Medicine
1962,6ecb2f55ae787363712adf6e7ba6c2812d3f0b32,miRBase: the microRNA sequence database.,,"
          129-38
        ",2006.0,https://herrera.info/categoriesterms.jsp,Computer Science
1963,1337f14678d80f22df094a3a9ad09a695d5f86ee,THE EXTRAGALACTIC DISTANCE DATABASE,"A database can be accessed on the Web at http://edd.ifa.hawaii.edu that was developed to promote access to information related to galaxy distances. The database has three functional components. First, tables from many literature sources have been gathered and enhanced with links through a distinct galaxy naming convention. Second, comparisons of results both at the levels of parameters and of techniques have begun and are continuing, leading to increasing homogeneity and consistency of distance measurements. Third, new material is presented arising from ongoing observational programs at the University of Hawaii 2.2 m telescope, radio telescopes at Green Bank, Arecibo, and Parkes and with the Hubble Space Telescope. This new observational material is made available in tandem with related material drawn from archives and passed through common analysis pipelines.",323 - 331,2009.0,http://singleton.com/explore/main/postsprivacy.jsp,Physics
1964,86dc58305d4cb2766eb2746bbe578e9a58a6f238,Carbohydrate-active enzymes : an integrated database approach,,74-145,1999.0,http://www.andrews-stokes.net/explore/list/categoriesprivacy.htm,Chemistry
1965,78b20577738126cb80ed80ad9d1ef96ed813a14a,System R: relational approach to database management,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.
This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",97-137,1976.0,http://www.nelson.com/app/explore/postsauthor.php,Computer Science
1966,c7a77164a8ede4f536c2779f32aeb6bf98eff766,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,,"
          751-9
        ",1992.0,http://www.davis-nielsen.com/categoryregister.php,Medicine
1967,c9e151ba8e59422320013d64307a17a94e018a98,Scopus database: a review,,1 - 1,2006.0,http://www.brown.net/categories/blog/appauthor.jsp,Medicine
1968,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",D253 - D260,2010.0,https://www.moore-watts.org/wp-contenthome.htm,Biology
1969,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",15-129,2009.0,http://hawkins.info/posts/exploreauthor.asp,Geography
1970,ecb74f4c908455f446ac455437f81e0e9dd9a319,Introduction to Temporal Database Research,"A wide range of database applications manage time-varying data. In contrast, existing database technology provides little support for managing such data. The research area of temporal databases aims to change this state of affairs by characterizing the semantics of temporal data and providing expressive and efficient ways to model, store, and query temporal data. This chapter offers a brief introduction to temporal database research. It concisely introduces fundamental temporal database concepts, surveys state-of-the-art solutions to challenging aspects of temporal data management, and also offers a look into the future of temporal database research.",51-110,,http://www.torres.com/search/searchlogin.jsp,Technology
1971,a4379a159ff5a4540dd81f00b3a3fd1adc44e399,iRefIndex: A consolidated protein interaction database with provenance,,405 - 405,2008.0,https://www.hood-trevino.biz/posts/categories/categorylogin.php,Computer Science
1972,f1af714b92372c8e606485a3982eab2f16772ad8,The MUG facial expression database,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,1-4,2010.0,https://moyer-clayton.com/main/tagspost.asp,Computer Science
1973,b26281050ffcdf52ac24fe2b4d21482fba78deaa,The EPILEPSIAE database: An extensive electroencephalography database of epilepsy patients,"From the very beginning the seizure prediction community faced problems concerning evaluation, standardization, and reproducibility of its studies. One of the main reasons for these shortcomings was the lack of access to high‐quality long‐term electroencephalography (EEG) data. In this article we present the EPILEPSIAE database, which was made publicly available in 2012. We illustrate its content and scope. The EPILEPSIAE database provides long‐term EEG recordings of 275 patients as well as extensive metadata and standardized annotation of the data sets. It will adhere to the current standards in the field of prediction and facilitate reproducibility and comparison of those studies. Beyond seizure prediction, it may also be of considerable benefit for studies focusing on seizure detection, basic neurophysiology, and other fields.",47-126,2012.0,http://mccall.com/blog/search/categoriessearch.php,Medicine
1974,ef47742e72bd64fb1ae5359cd6d5dd6dfad34dc8,Implementation techniques for main memory database systems,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations",1-8,1984.0,http://pearson.org/categorieslogin.html,Computer Science
1975,88b98d7b20ede342fe471b2889ace70d082e4db7,Query by humming: musical information retrieval in an audio database,"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.",231-236,1995.0,https://www.nelson.com/explorehome.htm,Computer Science
1976,d5ae5a965ac5ad79128082d7d1edf9d7ab1d840b,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",16 - 26,2003.0,http://www.young.com/app/search/poststerms.php,Biology
1977,8bbb5e29bc76675ae3b73185c17e9077742742ee,A Historical Public Debt Database,"This paper describes the compilation of the first truly comprehensive database on gross government debt-to-GDP ratios, covering nearly the entire IMF membership (174 countries) and spanning an exceptionally long time period. The database was constructed by bringing together a number of other datasets and information from original sources. For the most recent years, the data are linked to the IMF World Economic Outlook (WEO) database to facilitate regular updates. The paper discusses the evolution of debt-to-GDP ratios across country groups for several decades, episodes of debt spikes and reversals, and a pattern of negative correlation between debt and growth.",38-111,2010.0,https://barr.com/app/search/categoryregister.html,Economics
1978,e9a1699735aff36cdd1fa385165426dd18b0d9ec,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",D224 - D228,2007.0,https://www.stevens-schmidt.biz/blog/mainlogin.html,Medicine
1979,b069e125442995787119db3bfa71dff5d965f3aa,MCYT baseline corpus: a bimodal biometric database,"The current need for large multimodal databases to evaluate automatic biometric recognition systems has motivated the development of the MCYT bimodal database. The main purpose has been to consider a large scale population, with statistical significance, in a real multimodal procedure, and including several sources of variability that can be found in real environments. The acquisition process, contents and availability of the single-session baseline corpus are fully described. Some experiments showing consistency of data through the different acquisition sites and assessing data quality are also presented.",395-401,2003.0,https://petersen.info/list/categories/tagspost.html,Computer Science
1980,de31aa8e914c60189a425e174af223967e2722cc,"RWC Music Database: Popular, Classical and Jazz Music Databases","paper describes the design policy and specifications of the RWC Music Database , a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database(15 pieces), Classical Music Database(50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",82-117,2002.0,http://fields.info/list/list/tagsterms.html,Computer Science
1981,a5c4c2b5719eff7160334259b018809dc9c4ab4b,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.","
          1443-5
        ",1992.0,http://www.gonzalez.com/tags/posts/categoryprivacy.html,Biology
1982,64acb315b6129061c62bfabef2ac06d1a6fff95b,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.","
          D497-501
        ",2004.0,https://www.leach-george.com/categoriescategory.html,Biology
1983,5160fb34a6719bbf8d60743d0d27db0ed5df3d2a,Foundations of Preferences in Database Systems,,311-322,2002.0,http://www.hall.com/postsabout.html,Computer Science
1984,daeabbe2ac3aa90aabf10527090f548fc125e9e6,The PDBbind database: methodologies and updates.,"We have developed the PDBbind database to provide a comprehensive collection of binding affinities for the protein-ligand complexes in the Protein Data Bank (PDB). This paper gives a full description of the latest version, i.e., version 2003, which is an update to our recently reported work. Out of 23 790 entries in the PDB release No.107 (January 2004), 5897 entries were identified as protein-ligand complexes that meet our definition. Experimentally determined binding affinities (K(d), K(i), and IC(50)) for 1622 of these were retrieved from the references associated with these complexes. A total of 900 complexes were selected to form a ""refined set"", which is of particular value as a standard data set for docking and scoring studies. All of the final data, including binding affinity data, reference citations, and processed structural files, have been incorporated into the PDBbind database accessible on-line at http:// www.pdbbind.org/.","
          4111-9
        ",2005.0,http://king-keller.com/search/explore/poststerms.php,Chemistry
1985,c17ee327e563536f8adaf214eb6d3bde33b73dd6,Chabot: Retrieval from a Relational Database of Images,"Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >",40-48,1995.0,http://mccarthy.com/bloghomepage.htm,Computer Science
1986,a6089c7eca1d77dc199e462763ab13f99f85c663,Global wood density database,,31-124,2009.0,http://burgess.com/categoriesfaq.html,Computer Science
1987,584af7d56a7247b9ad5a399d99b1eb0a2c8f6586,Journal of Database Management,"INTRODUCTION Background Advances in wireless technology increase the number of mobile device users and give pace to the rapid development of e-commerce conducted with these devices. The new type of e-commerce transactions, conducted through mobile devices using wireless telecommunications networks and other wired e-commerce technologies, is called mobile commerce (increasingly known as mobile e-commerce or m-commerce). Due to the special characteristics and constraints of mobile devices and the wireless network, the emerging mobile commerce operates in an environment very different from e-commerce conducted over the wired Internet. In terms of business potential, mobile commerce promises many more alluring market opportunities than traditional e-commerce because of its inherent characteristics such as ubiquity, per-sonalization, flexibility, and dissemination. Mobile commerce will likely emerge as a major focus of the business world and telecommunication industry in the immediate future. For example, according to Guy Singh (2000), the global mobile commerce market is expected to be worth a staggering US$200 billion by 2004. The marriage of mobile devices and the Internet is, however, filled with challenges as well as opportunities. This paper presents an overview of mobile commerce development by looking at the enabling technologies, the impact of mobile commerce on business models, and the",39-107,,https://www.ortiz.info/posts/mainindex.php,Technology
1988,be9124db35e5f451f508e095fdc25727c067a9ef,UBIRIS: A Noisy Iris Image Database,,970-977,2005.0,http://kelly.com/search/wp-content/listauthor.html,Computer Science
1989,89e3fae32bf72b61834fc2ae60b1f8508e714e38,World Ocean Database,"The U.S. National Oceanic and Atmospheric Administration's (NOAA) World Ocean Database 2009, released in November as an update to the 2005 version, provides about 9.1 million temperature profiles and 3.5 million salinity reports, with some information dating as far back as 1800. The updated database includes scientific information about the oceans that can be sorted in various ways, including geographically or by year. 
 
“There is now more data about the global oceans than ever before,” according to Sydney Levitus, director of the World Data Center for Oceanography, part of NOAA's National Oceanographic Data Center. “Previous databases have shown the world ocean has warmed during the last 53 years, and it's crucial we have reliable, accurate monitoring of our oceans into the future,” he said. The database is a part of the Integrated Ocean Observing System and the Global Earth Observation System of Systems.",472-472,2009.0,http://watkins-raymond.net/explore/wp-content/blogprivacy.asp,Geology
1990,d4a8e93f004c86267eead89edecbd332518dbf21,Database description with SDM: a semantic database model,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.
SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",351-386,1981.0,https://www.stephens.com/searchmain.php,Computer Science
1991,4aa6aaeb14e5f881100c97cd5d06306f16ab80d0,Current Status of the Digital Database for Screening Mammography,,457-460,1998.0,http://www.castillo.com/wp-contentcategory.html,Medicine
1992,599cc88971d2f5b375ff23b6342f17855e01791c,The CMU Motion of Body (MoBo) Database,"In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database.",50-124,2001.0,https://green.org/bloglogin.php,Computer Science
1993,2fe0dea4a9a243ebeaae37fec9cbbaa28b5f72a7,The HITRAN database: 1986 edition.,"A description and summary of the latest edition of the AFGL HITRAN molecular absorption parameters database are presented. This new database combines the information for the seven principal atmospheric absorbers and twenty-one additional molecular species previously contained on the AFGL atmospheric absorption line parameter compilation and on the trace gas compilation. In addition to updating the parameters on earlier editions of the compilation, new parameters have been added to this edition such as the self-broadened halfwidth, the temperature dependence of the air-broadened halfwidth, and the transition probability. The database contains 348043 entries between 0 and 17,900 cm(-1). A FORTRAN program is now furnished to allow rapid access to the molecular transitions and for the creation of customized output. A separate file of molecular cross sections of eleven heavy molecular species, applicable for qualitative simulation of transmission and emission in the atmosphere, has also been provided.","
          4058-97
        ",1987.0,https://wang.org/categoryhome.htm,Computer Science
1994,74e10ff37b568e76c5166ce8b0eddf2abfdcbac9,A common database approach for OLTP and OLAP using an in-memory column database,"When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.",49-144,2009.0,https://nguyen.com/tags/listprivacy.htm,Computer Science
1995,5737a1f6fd8d928b88726ada916d7874afdfe0d7,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",491-500,2001.0,http://www.smith.net/explore/search/mainhome.htm,Computer Science
1996,a0d251f893e043175d0a6d0e12e6e166ff523255,Documentation Mocap Database HDM05,"Preface In the past two decades, motion capture (mocap) systems have been developed that allow to track and record human motions at high spatial and temporal resolutions. The resulting motion capture data is used to analyze human motions in fields such as sports sciences and biometrics (person identification), and to synthesize realistic motion sequences in data-driven computer animation. Such applications require efficient methods and tools for the automatic analysis, synthesis and classification of motion capture data, which constitutes an active research area with many yet unsolved problems. Even though there is a rapidly growing corpus of motion capture data, the academic research community still lacks publicly available motion data, as supplied by [4], that can be freely used for systematic research on motion analysis, synthesis, and classification. Furthermore, a common dataset of annotated and well-documented motion capture data would be extremely valuable to the research community in view of an objective comparison and evaluation of the achieved research results. It is the objective of our motion capture database HDM05 1 to supply free motion capture data for research purposes. HDM05 contains more than tree hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. Furthermore, HDM05 contains for each of roughly 70 motion classes 10 to 50 realizations executed by various actors amounting to roughly 1, 500 motion clips. In this documentation, we give a detailed description of our mocap database HDM05. In Sect. 1, we provide some general information on motion capture data including references to various application fields. A detailed description of the database structure of HDM05 as well as of the content of each mocap file can be found in Sect. 2. We also provide several MATLAB tools comprising a parser for ASF/AMC and C3D as well as visualization, renaming and cutting tools, which are described in Sect. 3. Finally, Sect. 4 summarizes some facts on the mocap file formats ASF/AMC and C3D as used in our database. We appreciate any comments and suggestions for improvement. 1 The motion capture data has been recorded at the Hochschule der Medien (HDM) in the year 2005 under the supervision of Bernhard Eberhardt.",80-138,2007.0,http://chen.com/categoriesauthor.html,Computer Science
1997,54a54371577545b529a8ab53b421d14a58b33ba6,Genetic association database,,87-87,2004.0,http://cruz-benson.info/tag/searchcategory.jsp,Biology
1998,666dfa8258914bf17970b20d2f7247c7c1468307,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",479-513,1983.0,http://www.davis-young.com/explore/tagfaq.html,Mathematics
1999,2c0ed016d2bc69854523aab6bcf86342692e31e2,The RDP (Ribosomal Database Project) continues,"The Ribosomal Database Project (RDP-II), previously described by Maidak et al., continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 7.1 (September 17, 1999) included more than 10 700 small subunit rRNA sequences. More than 850 type strain sequences were identified and added to the prokaryotic alignment, bringing the total number of type sequences to 3324 representing 2460 different species. Availability of an RDP-II mirror site in Japan is also near completion. RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/ ). Analysis services include rRNA probe checking, approx-i-mate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment length polymorphism (T-RFLP) experiments.","
          173-4
        ",2000.0,https://hopkins.net/categoriesregister.php,Biology
2000,1ee0abcb8f0afd74d602255d529d7c2a036a8f02,Graph Theory,,78-140,2016.0,https://www.sanders.biz/wp-content/categories/bloghome.htm,Technology
2001,69906b74ee369cd25522ec432ecbc601a77c9d87,Spectral graph theory,"Spectral graph theory is a vast and expanding area of combinatorics. We start these notes by introducing and motivating classical matrices associated with a graph, and then show how to derive combinatorial properties of a graph from the eigenvalues of these matrices. We then examine more modern results such as polynomial interlacing and high dimensional expanders",58-108,2019.0,https://harvey-garrison.net/blog/mainhomepage.php,Mathematics
2002,3aca80d2a6ec2014342c4abe6611d498c789f7fa,Graph Theory,Abstract,32,2018.0,https://golden-smith.com/listprivacy.php,Computer Science
2003,b07c157e7d40e06a4f2d486b16d5180d8b24acb9,Algebraic Graph Theory,,"I-XIX, 1-439",2001.0,http://www.cameron.com/main/searchindex.jsp,Mathematics
2004,0a8bcccd8acc7ec899b57d4ba76db4ee21295092,Extremal Graph Theory,,34-104,2021.0,http://www.davis.com/app/tag/blogauthor.php,Mathematics
2005,415224a9aff759f6972189df8b7761dfd6a81154,Introduction to graph theory,"In graph theory, the term graph refers to a set of vertices and a set of edges. A vertex can be used to represent any object. Graphs may contain undirected or directed edges. An undirected edge is a set of two vertices. A directed edge is an ordered pair of two vertices where the edge goes from the first vertex to the second vertex. Graphs that contain directed edges are called directed graphs or digraphs.",348 - 348,1973.0,https://www.fox.com/tags/searchabout.php,Computer Science
2006,6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876,Graph Theory with Applications,"When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will",74-123,1977.0,http://fields.com/app/categories/categoriesprivacy.htm,Computer Science
2007,75d82765de0900fed1a7a073415d8f7c625f79e8,Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.",59-134,1996.0,http://www.wallace.com/tagsindex.jsp,Computer Science
2008,f59dbb2e58a39e298f04690d7a71972cf2e480ac,Graph Theory,,18-122,2023.0,https://carlson-garcia.com/tags/tags/tagpost.htm,Technology
2009,c2f21a6b917286c7e904e0f168b53bbaa2bda4ba,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders.",72-121,2019.0,http://www.boone.com/posts/wp-content/tagprivacy.htm,Computer Science
2010,bea139a35ffe458d7aec576b5e651cd8ac80e4d2,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.",27-110,1995.0,http://www.fitzgerald.com/tagfaq.asp,Mathematics
2011,18f7d9eb515e3f93fafc887cf0080ea2dfe1f9fa,"Dean Lusher, Johan Koskinen, Garry Robins. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. New York: Cambridge University Press, 2012. 360 pp, $36.99 (pbk), ISBN: 9780521141383",,1367 - 1370,2021.0,http://www.bowman.org/blog/app/exploreregister.php,Sociology
2012,2e75dd6fb569ee917b6571d89afadd980af499c0,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.",91-136,2020.0,http://www.schneider.info/categories/search/blogprivacy.html,Medicine
2013,92b7248f2b6eb568d0c69a5e89bf33cfe225cd92,Revisiting the paper on “Applications of graph theory to enzyme kinetics and protein folding kinetics: steady and non-steady state systems”,About 30 years ago a very important paper on “Applications of graph theory to enzyme kinetics and protein folding kinetics: steady and non-steady state systems”,60-131,2020.0,https://www.roth.com/tags/app/categoryregister.php,Chemistry
2014,455416dd3b2b34786cda6ce86740a6357a787ff3,Graph Theory: A Comprehensive Survey about Graph Theory Applications in Computer Science and Social Networks,"Graph theory (GT) concepts are potentially applicable in the field of computer science (CS) for many purposes. The unique applications of GT in the CS field such as clustering of web documents, cryptography, and analyzing an algorithm’s execution, among others, are promising applications. Furthermore, GT concepts can be employed to electronic circuit simplifications and analysis. Recently, graphs have been extensively used in social networks (SNs) for many purposes related to modelling and analysis of the SN structures, SN operation modelling, SN user analysis, and many other related aspects. Considering the widespread applications of GT in SNs, this article comprehensively summarizes GT use in the SNs. The goal of this survey paper is twofold. First, we briefly discuss the potential applications of GT in the CS field along with practical examples. Second, we explain the GT uses in the SNs with sufficient concepts and examples to demonstrate the significance of graphs in SN modeling and analysis.",26-126,2020.0,https://www.stone-strickland.com/wp-contentlogin.jsp,Computer Science
2015,6acabf73828a3e137a0e26673726834fef99d04a,A Graph Theory-Based Modeling of Functional Brain Connectivity Based on EEG: A Systematic Review in the Context of Neuroergonomics,"Graph theory analysis, a mathematical approach, has been applied in brain connectivity studies to explore the organization of network patterns. The computation of graph theory metrics enables the characterization of the stationary behavior of electroencephalogram (EEG) signals that cannot be explained by simple linear methods. The main purpose of this study was to systematically review the graph theory applications for mapping the functional connectivity of the EEG data in neuroergonomics. Moreover, this article proposes a pipeline for constructing an unweighted functional brain network from EEG data using both source and sensor methods. Out of 57 articles, our results show that graph theory metrics used to characterize EEG data have attracted increasing attention since 2006, with the highest frequency of publications in 2018. Most studies have focused on cognitive tasks in comparison with motor tasks. The mean phase coherence method, based on the “phase-locking value,” was the most frequently used functional estimation technique in the reviewed studies. Furthermore, the unweighted functional brain network has received substantially more attention in the literature than the weighted network. The global clustering coefficient and characteristic path length were the most prevalent metrics for differentiating between global integration and local segregation, and the small-worldness property emerged as a compelling metric for the characterization of information processing. This review provides insight into the use of graph theory metrics to model functional brain connectivity in the context of neuroergonomics research.",155103-155135,2020.0,https://www.sanchez-allen.org/category/mainprivacy.asp,Computer Science
2016,73dd81990333e603469fbab07958fbb7e47d302f,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",111 - 121,2018.0,http://www.dixon.info/wp-content/list/tagshomepage.html,Medicine
2017,bc4aa2998973727ea84d91141a0175bbf1ff4923,Graph Theory,,93-108,2021.0,http://www.johnson.com/posts/tagregister.htm,Computer Science
2018,56a13467a3cfb7a9ec00b7f3ed5e953324225233,Graph Theory with Applications,,1-226,1978.0,https://williams.com/posts/categories/wp-contentmain.htm,Computer Science
2019,321fede71792a304d44d8143399309c5d88c73a1,Graph Theory and Its Applications,,81-132,2018.0,https://www.thomas-shaw.biz/categories/exploreabout.html,Technology
2020,d96d7c7dd979ad51b04fe06e32904bece29696fd,"Algebraic Graph Theory: Morphisms, Monoids and Matrices","This is a highly self-contained book about algebraic graph theory which iswritten with a view to keep the lively and unconventional atmosphere of a spoken text to communicate the enthusiasm the author feels about this subject. The focus is on homomorphisms and endomorphisms, matrices and eigenvalues. Graph models are extremely useful for almost all applications and applicators as they play an important role as structuring tools. They allow to model net structures -like roads, computers, telephones -instances of abstract data structures -likelists, stacks, trees -and functional or object oriented programming.",34-127,2019.0,https://adams-obrien.org/category/category/apphome.php,Mathematics
2021,ee4fd9cd27836870dd18eb2d81efac596a758fb1,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements.",57-132,2019.0,https://www.yates.com/categoryauthor.htm,Materials Science
2022,85f51e3d3173a30e4bcfa3640354f940303d7023,Graph theory approaches to functional network organization in brain disorders: A critique for a brave new small-world,"Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer’s disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field.",1 - 26,2018.0,http://reese.com/main/wp-contenthomepage.php,Medicine
2023,72744fb5963b168b4590e5eaa148c8d0e5eafe38,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks.",977-1005,2018.0,http://hill.com/list/blogprivacy.htm,Computer Science
2024,78793b5f9394379aab1c84c782bdbef819667d56,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",75-103,2020.0,https://www.wagner.net/categoriesabout.html,Computer Science
2025,6acfec1c7361e88dd96ce60676cfed4c82a6e40a,Graph Theory,"Book file PDF easily for everyone and every device. You can download and read online Graph Theory file PDF Book only if you are registered here. And also you can download or read online all Book PDF file that related with Graph Theory book. Happy reading Graph Theory Bookeveryone. Download file Free Book PDF Graph Theory at Complete PDF Library. This Book have some digital formats such us :paperbook, ebook, kindle, epub, fb2 and another formats. Here is The Complete PDF Book Library. It's free to register here to get Book file PDF Graph Theory.",73-126,2020.0,http://hughes.com/posts/blog/appauthor.php,Technology
2026,16b8b3a7840e9b61756db16097e2a8b1d18b8bf6,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",21-116,2020.0,https://swanson.com/list/app/tagauthor.htm,Computer Science
2027,756bd8b609f9754fee6ae9e9056f5face4386726,BRAPH: A graph theory software for the analysis of brain connectivity,"The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH – BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer’s disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson’s patients with mild cognitive impairment.",54-109,2017.0,https://garcia-estrada.com/search/blog/listmain.asp,Computer Science
2028,968ea337e15004a2ed3e1442d7f632a1214e2268,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
",36-136,2018.0,https://www.mcdonald.net/postslogin.jsp,Computer Science
2029,aec03b62900277709b10793a168058c5d05fd34f,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete.",177 - 187,2018.0,https://www.long-montgomery.info/categories/blogabout.htm,Mathematics
2030,6097094b745b265de8dad6eaf86f7d832e4e7695,Spectral Graph Theory and its Applications,"Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.",29-38,2007.0,https://www.mack.com/category/tagterms.html,Mathematics
2031,e1274867d404fd1dbf73a654c217ea5c0c32852c,Graph Theory and Brain Connectivity in Alzheimer’s Disease,This article presents a review of recent advances in neuroscience research in the specific area of brain connectivity as a potential biomarker of Alzheimer’s disease with a focus on the application of graph theory. The review will begin with a brief overview of connectivity and graph theory. Then resent advances in connectivity as a biomarker for Alzheimer’s disease will be presented and analyzed.,616 - 626,2017.0,http://peck.com/posts/postsprivacy.html,Psychology
2032,ecaa9d581d26cb23a759ca1310b0a5d8ce27b7b2,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have",27-139,2018.0,http://mcdonald.com/wp-contentauthor.asp,Technology
2033,f66b6c35b1820dfb39dfbda11fa4040913c1bb85,Graph Theory,,88-113,2020.0,https://moore.org/app/searchterms.html,Technology
2034,b9d26450520aa98e6ccb1b2e11a48fec29f273b9,Cytoscape.js: a graph theory library for visualisation and analysis,"Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org. Contact: gary.bader@utoronto.ca",309 - 311,2015.0,https://ford-mora.com/category/categoryregister.htm,Computer Science
2035,fc9051185b4879606ed006d00055940aee4da5e1,Modern Graph Theory,,56-119,2013.0,http://www.gutierrez.org/blog/posts/wp-contentmain.jsp,Computer Science
2036,0e63d3bea6a258017c43bb6c91f923839d260c8f,Graph Theory,,1-582,2008.0,https://harrington.biz/wp-content/tagprivacy.php,Computer Science
2037,12748e904f5025ca1757ce49d72cad3878e1be8f,Graph Theory-Based Pinning Synchronization of Stochastic Complex Dynamical Networks,"This paper is concerned with the adaptive pinning synchronization problem of stochastic complex dynamical networks (CDNs). Based on algebraic graph theory and Lyapunov theory, pinning controller design conditions are derived, and the rigorous convergence analysis of synchronization errors in the probability sense is also conducted. Compared with the existing results, the topology structures of stochastic CDN are allowed to be unknown due to the use of graph theory. In particular, it is shown that the selection of nodes for pinning depends on the unknown lower bounds of coupling strengths. Finally, an example on a Chua’s circuit network is given to validate the effectiveness of the theoretical results.",427-437,2017.0,http://www.warren.net/tags/blogsearch.htm,Medicine
2038,2f1214615605b298733c008ffbd8a79051992473,Fuzzy Graph Theory,,20-127,2018.0,http://chavez.org/listlogin.php,Computer Science
2039,f8dab78b4ddd64a83bbc62cb153161a033dc06d1,Study of biological networks using graph theory,,1212 - 1219,2017.0,https://www.young.com/main/tag/categoriesindex.html,Medicine
2040,74eb4d6abf1d0236be338c1bd5ee59a498b961b1,Band connectivity for topological quantum chemistry: Band structures as a graph theory problem,"The conventional theory of solids is well suited to describing band structures locally near isolated points in momentum space, but struggles to capture the full, global picture necessary for understanding topological phenomena. In part of a recent paper [B. Bradlyn et al., Nature 547, 298 (2017)], we have introduced the way to overcome this difficulty by formulating the problem of sewing together many disconnected local ""k-dot-p"" band structures across the Brillouin zone in terms of graph theory. In the current manuscript we give the details of our full theoretical construction. We show that crystal symmetries strongly constrain the allowed connectivities of energy bands, and we employ graph-theoretic techniques such as graph connectivity to enumerate all the solutions to these constraints. The tools of graph theory allow us to identify disconnected groups of bands in these solutions, and so identify topologically distinct insulating phases.",035138,2017.0,https://cameron.com/wp-contentabout.asp,Mathematics
2041,7d08932d11d5cf03ac2725250e83ec68f5a0f878,Graph Theory 1736 1936,"Thank you very much for downloading graph theory 1736 1936. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this graph theory 1736 1936, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their laptop. graph theory 1736 1936 is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the graph theory 1736 1936 is universally compatible with any devices to read.",47-115,2016.0,https://knox-mays.com/list/blog/listhomepage.php,Computer Science
2042,abb780f85a1a27919e461efdcc2a77cf1ab46c8b,Applying Graph Theory in Ecological Research,"Graph theory can be applied to ecological questions in many ways, and more insights can be gained by expanding the range of graph theoretical concepts applied to a specific system. But how do you know which methods might be used? And what do you do with the graph once it has been obtained? This book provides a broad introduction to the application of graph theory in different ecological systems, providing practical guidance for researchers in ecology and related fields. Readers are guided through the creation of an appropriate graph for the system being studied, including the application of spatial, spatio-temporal, and more abstract structural process graphs. Simple figures accompany the explanations to add clarity, and a broad range of ecological phenomena from many ecological systems are covered. This is the ideal book for graduate students and researchers looking to apply graph theoretical methods in their work.",81-128,2017.0,https://randolph-porter.com/posts/searchauthor.php,Mathematics
2043,73be8ad32db73d4f4f50c5dd96d71bdfb02ea9bb,Algorithmic graph theory and perfect graphs,,207-208,1986.0,https://kelly.com/main/list/postsregister.htm,Mathematics
2044,ea84995e21bf19d8deecc1c88d0b47be83ddb267,Topics in Chromatic Graph Theory: Colouring random graphs,"How many colours are typically necessary to colour a graph? We survey a number of perspectives on this natural question, which is central to random graph theory and to probabilistic and extremal combinatorics. It has stimulated a vibrant area of research, with a rich history extending back through more than half a century. Erdős and Rényi [36] asked a form of this question in a celebrated early paper on random graphs in 1960. Let Gn,m be a graph chosen uniformly at random from the set of graphs with vertex-set [n] = {1, 2, . . . , n} and m edges. In this probabilistic model, we cannot rule out the possibility that Gn,m is, for example, the disjoint union of one large clique and some isolated vertices, or perhaps one Turán graph (a balanced complete multipartite graph) and some isolated vertices. The resulting range is large: in the former situation the chromatic number could be about √ 2m, while in the latter it could be 2 if m ≤ n2/4. These outcomes are unlikely, however, and we are interested in the most probable ones. To state the question properly, we say that an event An (which here always describes a property of a random graph on the vertex-set [n]) holds asymptotically almost surely (a.a.s.) if the probability that An holds satisfies P(An)→ 1 as n→∞. Erdős and Rényi asked the following question. Suppose that m ∼ cn for some c ≥ 12 . Is there a positive integer function f = fc(n) for which χ(Gn,m) = f a.a.s., and if so how large is it?",23-119,2015.0,http://riley.com/posts/blog/categoriesauthor.php,Mathematics
2045,36e7356a12554d8af87094a9ca245b49b4f4a5c4,Basic Graph Theory,,1-163,2017.0,https://www.jackson.biz/category/category/apphomepage.asp,Computer Science
2046,8e8152d46c8ff1070805096c214df7f389c57b80,Wavelets on Graphs via Spectral Graph Theory,,77-119,2009.0,https://graham.org/exploreregister.html,Computer Science
2047,583c47a08841b662c661973951af40045cec3088,"Network science and the human brain: Using graph theory to understand the brain and one of its hubs, the amygdala, in health and disease","Over the past 15 years, the emerging field of network science has revealed the key features of brain networks, which include small‐world topology, the presence of highly connected hubs, and hierarchical modularity. The value of network studies of the brain is underscored by the range of network alterations that have been identified in neurological and psychiatric disorders, including epilepsy, depression, Alzheimer's disease, schizophrenia, and many others. Here we briefly summarize the concepts of graph theory that are used to quantify network properties and describe common experimental approaches for analysis of brain networks of structural and functional connectivity. These range from tract tracing to functional magnetic resonance imaging, diffusion tensor imaging, electroencephalography, and magnetoencephalography. We then summarize the major findings from the application of graph theory to nervous systems ranging from Caenorhabditis elegans to more complex primate brains, including man. Focusing, then, on studies involving the amygdala, a brain region that has attracted intense interest as a center for emotional processing, fear, and motivation, we discuss the features of the amygdala in brain networks for fear conditioning and emotional perception. Finally, to highlight the utility of graph theory for studying dysfunction of the amygdala in mental illness, we review data with regard to changes in the hub properties of the amygdala in brain networks of patients with depression. We suggest that network studies of the human brain may serve to focus attention on regions and connections that act as principal drivers and controllers of brain function in health and disease.†Published 2016",94-144,2016.0,http://barnes.com/tag/blogauthor.php,Psychology
2048,f6c5a2a6cccae1db5f0a753946a2e89d7d22aa5d,"Handbook of graph theory, combinatorial optimization, and algorithms","Basic Concepts and Algorithms Basic Concepts in Graph Theory and Algorithms Subramanian Arumugam and Krishnaiyan ""KT"" Thulasiraman Basic Graph Algorithms Krishnaiyan ""KT"" Thulasiraman Depth-First Search and Applications Krishnaiyan ""KT"" Thulasiraman Flows in Networks Maximum Flow Problem F. Zeynep Sargut, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Minimum Cost Flow Problem Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Multi-Commodity Flows Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Algebraic Graph Theory Graphs and Vector Spaces Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Incidence, Cut, and Circuit Matrices of a Graph Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Matrix and Signal Flow Graphs Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Spectrum and the Laplacian Spectrum of a Graph R. Balakrishnan Resistance Networks, Random Walks, and Network Theorems Krishnaiyan ""KT"" Thulasiraman and Mamta Yadav Structural Graph Theory Connectivity Subramanian Arumugam and Karam Ebadi Connectivity Algorithms Krishnaiyan ""KT"" Thulasiraman Graph Connectivity Augmentation Andras Frank and Tibor Jordan Matchings Michael D. Plummer Matching Algorithms Krishnaiyan ""KT"" Thulasiraman Stable Marriage Problem Shuichi Miyazaki Domination in Graphs Subramanian Arumugam and M. Sundarakannan Graph Colorings Subramanian Arumugam and K. Raja Chandrasekar Planar Graphs Planarity and Duality Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Edge Addition Planarity Testing Algorithm John M. Boyer Planarity Testing Based on PC-Trees Wen-Lian Hsu Graph Drawing Md. Saidur Rahman and Takao Nishizeki Interconnection Networks Introduction to Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Cayley Graphs S. Lakshmivarahan, Lavanya Sivakumar, and S.K. Dhall Graph Embedding and Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Special Graphs Program Graphs Krishnaiyan ""KT"" Thulasiraman Perfect Graphs Chinh T. Hoang and R. Sritharan Tree-Structured Graphs Andreas Brandstadt and Feodor F. Dragan Partitioning Graph and Hypergraph Partitioning Sachin B. Patkar and H. Narayanan Matroids Matroids H. Narayanan and Sachin B. Patkar Hybrid Analysis and Combinatorial Optimization H. Narayanan Probabilistic Methods, Random Graph Models, and Randomized Algorithms Probabilistic Arguments in Combinatorics C.R. Subramanian Random Models and Analyses for Chemical Graphs Daniel Pascua, Tina M. Kouri, and Dinesh P. Mehta Randomized Graph Algorithms: Techniques and Analysis Surender Baswana and Sandeep Sen Coping with NP-Completeness General Techniques for Combinatorial Approximation Sartaj Sahni epsilon-Approximation Schemes for the Constrained Shortest Path Problem Krishnaiyan ""KT"" Thulasiraman Constrained Shortest Path Problem: Lagrangian Relaxation-Based Algorithmic Approaches Ying Xiao and Krishnaiyan ""KT"" Thulasiraman Algorithms for Finding Disjoint Paths with QoS Constraints Alex Sprintson and Ariel Orda Set-Cover Approximation Neal E. Young Approximation Schemes for Fractional Multicommodity Flow Problems George Karakostas Approximation Algorithms for Connectivity Problems Ramakrishna Thurimella Rectilinear Steiner Minimum Trees Tao Huang and Evangeline F.Y. Young Fixed-Parameter Algorithms and Complexity Venkatesh Raman and Saket Saurabh",60-144,2016.0,https://www.nelson-donaldson.com/tag/maincategory.html,Mathematics
2049,37c0809e246e593d524fa8a918eaee661124c21f,An Introduction to Bipolar Single Valued Neutrosophic Graph Theory,"In this paper, we first define the concept of bipolar single neutrosophic graphs as the generalization of bipolar fuzzy graphs, N-graphs, intuitionistic fuzzy graph, single valued neutrosophic graphs and bipolar intuitionistic fuzzy graphs.",184 - 191,2016.0,https://www.luna-roberts.com/list/tagsindex.html,Mathematics
2050,d66c93134b8345d76c6d4d25cab52abb160fee67,A Brief Introduction to Spectral Graph Theory,"Spectral graph theory starts by associating matrices to graphs, notably, the adjacency matrix and the laplacian matrix. The general theme is then, firstly, to compute or estimate the eigenvalues of such matrices, and secondly, to relate the eigenvalues to structural properties of graphs. As it turns out, the spectral perspective is a powerful tool. Some of its loveliest applications concern facts that are, in principle, purely graph-theoretic or combinatorial. To give just one example, spectral ideas are a key ingredient in the proof of the so-called Friendship Theorem: if, in a group of people, any two persons have exactly one common friend, then there is a person who is everybody’s friend. This text is an introduction to spectral graph theory, but it could also be seen as an invitation to algebraic graph theory. On the one hand, there is, of course, the linear algebra that underlies the spectral ideas in graph theory. On the other hand, most of our examples are graphs of algebraic origin. The two recurring sources are Cayley graphs of groups, and graphs built out of finite fields. In the study of such graphs, some further algebraic ingredients (e.g., characters) naturally come up. The table of contents gives, as it should, a good glimpse of where is this text going. Very broadly, the first half is devoted to graphs, finite fields, and how they come together. This part is meant as an appealing and meaningful motivation. It provides a context that frames and fuels much of the second, spectral, half. Most sections have one or two exercises. Their position within the text is a hint. The exercises are optional, in the sense that virtually nothing in the main body depends on them. But the exercises are often of the non-trivial variety, and they should enhance the text in an interesting way. The hope is that the reader will enjoy them. We assume a basic familiarity with linear algebra, finite fields, and groups, but not necessarily with graph theory. This, again, betrays our algebraic perspective. This text is based on a course I taught in Göttingen, in the Fall of 2015. I would like to thank Jerome Baum for his help with some of the drawings. The present version is preliminary, and comments are welcome (email: bogdan.nica@gmail.com).",100-124,2016.0,http://cervantes.com/search/postsregister.html,Mathematics
2051,24987e91a046a71de3cf1017cf6eb907a8f7a444,Support Vector Machine Classification of Major Depressive Disorder Using Diffusion-Weighted Neuroimaging and Graph Theory,"Recently, there has been considerable interest in understanding brain networks in major depressive disorder (MDD). Neural pathways can be tracked in the living brain using diffusion-weighted imaging (DWI); graph theory can then be used to study properties of the resulting fiber networks. To date, global abnormalities have not been reported in tractography-based graph metrics in MDD, so we used a machine learning approach based on “support vector machines” to differentiate depressed from healthy individuals based on multiple brain network properties. We also assessed how important specific graph metrics were for this differentiation. Finally, we conducted a local graph analysis to identify abnormal connectivity at specific nodes of the network. We were able to classify depression using whole-brain graph metrics. Small-worldness was the most useful graph metric for classification. The right pars orbitalis, right inferior parietal cortex, and left rostral anterior cingulate all showed abnormal network connectivity in MDD. This is the first use of structural global graph metrics to classify depressed individuals. These findings highlight the importance of future research to understand network properties in depression across imaging modalities, improve classification results, and relate network alterations to psychiatric symptoms, medication, and comorbidities.",53-130,2015.0,https://santiago.com/wp-contentfaq.php,Medicine
2052,cfc104f686b2190f64db03c8933d8f2a7fff5a5d,Molecular Orbital Calculations Using Chemical Graph Theory,"molecular orbital calculations using chemical graph theory is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the molecular orbital calculations using chemical graph theory is universally compatible with any devices to read.",82-115,2016.0,https://www.jacobson-tapia.com/tags/categoriesregister.asp,Computer Science
2053,e9b90c75c5c99353a43cbf5e083334a59e50cca8,Rational exponents in extremal graph theory,"Given a family of graphs $\mathcal{H}$, the extremal number $\textrm{ex}(n, \mathcal{H})$ is the largest $m$ for which there exists a graph with $n$ vertices and $m$ edges containing no graph from the family $\mathcal{H}$ as a subgraph. We show that for every rational number $r$ between $1$ and $2$, there is a family of graphs $\mathcal{H}_r$ such that $\textrm{ex}(n, \mathcal{H}_r) = \Theta(n^r)$. This solves a longstanding problem in the area of extremal graph theory.",42-124,2015.0,http://hart.net/wp-content/main/blogpost.php,Mathematics
2054,788eab78c4c6e998fc6812b7d7af7ce6505af2e4,Graph Theory And Sparse Matrix Computation,"Thank you for downloading graph theory and sparse matrix computation. As you may know, people have look numerous times for their chosen readings like this graph theory and sparse matrix computation, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their laptop. graph theory and sparse matrix computation is available in our book collection an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the graph theory and sparse matrix computation is universally compatible with any devices to read.",16-111,2016.0,https://rice.com/categories/tag/listterms.htm,Computer Science
2055,e3acbca01b107b43b04f73499fbce2eeadc5970a,Neutrosophic Graphs: A New Dimension to Graph Theory,"In this book authors for the first time have made a through study of neutrosophic graphs. This study reveals that these neutrosophic graphs give a new dimension to graph theory. The important feature of this book is it contains over 200 neutrosophic graphs to provide better understanding of this concepts. Further these graphs happen to behave in a unique way inmost cases, for even the edge colouring problem is different from the classical one. Several directions and dimensions in graph theory are obtained from this study.",87-106,2015.0,http://salazar.com/wp-contentpost.html,Mathematics
2056,bba08b50d33448a3b65b4baf7c0df3c1eca8b10d,Evolutionary graph theory revisited: when is an evolutionary process equivalent to the Moran process?,"Evolution in finite populations is often modelled using the classical Moran process. Over the last 10 years, this methodology has been extended to structured populations using evolutionary graph theory. An important question in any such population is whether a rare mutant has a higher or lower chance of fixating (the fixation probability) than the Moran probability, i.e. that from the original Moran model, which represents an unstructured population. As evolutionary graph theory has developed, different ways of considering the interactions between individuals through a graph and an associated matrix of weights have been considered, as have a number of important dynamics. In this paper, we revisit the original paper on evolutionary graph theory in light of these extensions to consider these developments in an integrated way. In particular, we find general criteria for when an evolutionary graph with general weights satisfies the Moran probability for the set of six common evolutionary dynamics.",66-129,2015.0,http://nelson.com/search/blogregister.htm,Mathematics
2057,b255694c69768b03bf854a72f3b0520befb0c509,The Fascinating World of Graph Theory,"The fascinating world of graph theory goes back several centuries and revolves around the study of graphsmathematical structures showing relations between objects. With applications in biology, computer science, transportation science, and other areas, graph theory encompasses some of the most beautiful formulas in mathematicsand some of its most famous problems. For example, what is the shortest route for a traveling salesman seeking to visit a number of cities in one trip? What is the least number of colors needed to fill in any map so that neighboring regions are always colored differently? Requiring readers to have a math background only up to high school algebra, this book explores the questions and puzzles that have been studied, and often solved, through graph theory. In doing so, the book looks at graph theorys development and the vibrant individuals responsible for the fields growth. Introducing graph theorys fundamental concepts, the authors explore a diverse plethora of classic problems such as the Lights Out Puzzle, the Minimum Spanning Tree Problem, the Knigsberg Bridge Problem, the Chinese Postman Problem, a Knights Tour, and the Road Coloring Problem. They present every type of graph imaginable, such as bipartite graphs, Eulerian graphs, the Petersen graph, and trees. Each chapter contains math exercises and problems for readers to savor. An eye-opening journey into the world of graphs, this book offers exciting problem-solving possibilities for mathematics and beyond.",86-122,2015.0,http://www.johnson.biz/categories/postsregister.html,Computer Science
2058,5b2d23a567b117ffe32dad3907a473e328127ff2,Modern Graph Theory,,"I-XIII, 1-394",2002.0,https://www.lane-dillon.net/postslogin.htm,Computer Science
2059,9893595f6950ca1340055da05fd2fae2b7b4dfd3,Three conjectures in extremal spectral graph theory,,137-161,2016.0,http://king.com/explorepost.html,Mathematics
2060,b19fd39320634d3421191e0614b32f809d8b0866,A Survey on some Applications of Graph Theory in Cryptography,"Abstract Graph theory is rapidly moving into the main stream of research because of its applications in diverse fields such as biochemistry (genomics), coding theory, communication networks and their security etc. In particular researchers are exploring the concepts of graph theory that can be used in different areas of Cryptography. In this paper a review of the works carried out in the field of Cryptography which use the concepts of Graph Theory, is given. Some of the Cryptographic Algorithms based on general graph theory concepts, Extremal Graph Theory and Expander Graphs are analyzed.",209 - 217,2015.0,https://www.haas-mosley.com/categories/tags/categoriesmain.html,Mathematics
2061,d53aa6487575762c1a14addf273ea271fef24d29,Algorithmic graph theory and perfect graphs,,45-128,1980.0,http://jackson-king.net/tagslogin.php,Mathematics
2062,088e404fb84c0dc1a5c77724705e04b7872b1e01,Graph theory,,24-109,2018.0,http://bryant.biz/categories/categorieslogin.php,Technology
2063,4fe29b8330dc11095cdda0f2913b781e0a2262de,n-Nucleotide circular codes in graph theory,"The circular code theory proposes that genes are constituted of two trinucleotide codes: the classical genetic code with 61 trinucleotides for coding the 20 amino acids (except the three stop codons {TAA,TAG,TGA}) and a circular code based on 20 trinucleotides for retrieving, maintaining and synchronizing the reading frame. It relies on two main results: the identification of a maximal C3 self-complementary trinucleotide circular code X in genes of bacteria, eukaryotes, plasmids and viruses (Michel 2015 J. Theor. Biol. 380, 156–177. (doi:10.1016/j.jtbi.2015.04.009); Arquès & Michel 1996 J. Theor. Biol. 182, 45–58. (doi:10.1006/jtbi.1996.0142)) and the finding of X circular code motifs in tRNAs and rRNAs, in particular in the ribosome decoding centre (Michel 2012 Comput. Biol. Chem. 37, 24–37. (doi:10.1016/j.compbiolchem.2011.10.002); El Soufi & Michel 2014 Comput. Biol. Chem. 52, 9–17. (doi:10.1016/j.compbiolchem.2014.08.001)). The univerally conserved nucleotides A1492 and A1493 and the conserved nucleotide G530 are included in X circular code motifs. Recently, dinucleotide circular codes were also investigated (Michel & Pirillo 2013 ISRN Biomath. 2013, 538631. (doi:10.1155/2013/538631); Fimmel et al. 2015 J. Theor. Biol. 386, 159–165. (doi:10.1016/j.jtbi.2015.08.034)). As the genetic motifs of different lengths are ubiquitous in genes and genomes, we introduce a new approach based on graph theory to study in full generality n-nucleotide circular codes X, i.e. of length 2 (dinucleotide), 3 (trinucleotide), 4 (tetranucleotide), etc. Indeed, we prove that an n-nucleotide code X is circular if and only if the corresponding graph is acyclic. Moreover, the maximal length of a path in corresponds to the window of nucleotides in a sequence for detecting the correct reading frame. Finally, the graph theory of tournaments is applied to the study of dinucleotide circular codes. It has full equivalence between the combinatorics theory (Michel & Pirillo 2013 ISRN Biomath. 2013, 538631. (doi:10.1155/2013/538631)) and the group theory (Fimmel et al. 2015 J. Theor. Biol. 386, 159–165. (doi:10.1016/j.jtbi.2015.08.034)) of dinucleotide circular codes while its mathematical approach is simpler.",55-103,2016.0,http://www.hurley-lester.biz/blog/taghomepage.htm,Mathematics
2064,9455977ce159fdc496cda7128ec35e03615baf43,Graph theory in the geosciences,,147-160,2015.0,https://www.james.com/appabout.asp,Technology
2065,23d4cba08cc13f3e9b9442762dc8ad745c865b65,An Introduction To The Theory Of Graph Spectra,"an introduction to the theory of graph spectra is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to the theory of graph spectra is universally compatible with any devices to read.",91-138,2016.0,https://taylor.org/main/wp-contentpost.html,Computer Science
2066,9699f9186a38107ff5dfaae9d44dd1796cbe7ac7,Water Network Sectorization Based on Graph Theory and Energy Performance Indices,"AbstractThis paper proposes a new methodology for the optimal design of water network sectorization, which is an essential technique for improving the management and security of multiple-source water supply systems. In particular, the network sectorization problem under consideration concerns the definition of isolated district meter areas, each of which is supplied by its own source (or sources) and is completely disconnected from the rest of the water system through boundary valves or permanent pipe sectioning. The proposed methodology uses graph theory principles and a heuristic procedure based on minimizing the amount of dissipated power in the water network. The procedure has been tested on two existing water distribution networks (WDNs) (in Parete, Italy and San Luis Rio Colorado, Mexico) using different performance indices. The simulation results, which confirmed the effectiveness of the proposed methodology, surpass empirical trial-and-error approaches and offer water utilities a tool for the desi...",620-629,2014.0,http://www.dougherty.info/category/explore/categoryindex.html,Engineering
2067,59cdf849049627e4c30f3bd866e3a7e03e893251,Complex brain networks: graph theoretical analysis of structural and functional systems,,186-198,2009.0,https://flores-stewart.biz/posts/app/blogterms.htm,Psychology
2068,89ce4cf020e64bb4c7820b550ed9895525d31872,Functional connectivity and graph theory in preclinical Alzheimer's disease,,757-768,2014.0,http://www.williams.com/main/mainlogin.htm,Medicine
2069,d4334742a490ff8ef9830daa8b3cde35abd5fc84,Encryption Algorithm Using Graph Theory,"In the recent years, with the increase of using Internet and other new telecommunication technologies, cryptography has become a key area to research and improve in order to transfer data securely between two or more entities, especially when the data transferred classified as a critical or important data .Even there are many encryption algorithms exist, the need of new non-standard encryption algorithms raise to prevent any traditional opportunity to sniff dat a. The proposed algorithm represents a new encryption algorithm to encrypt and decrypt data securely with the benefits of graph theory properties, the new symmetric encryption algorithm use the concepts of cycle graph, complete graph and minimum spanning tree to generate a complexcipher text using a shared key.",2519-2527,2014.0,https://www.anderson-scott.com/list/tagsterms.asp,Computer Science
2070,61290e15e45007ce55554d65ef364508390663e1,Graph Theory and Cyber Security,"One of the most important fields in discrete mathematics is graph theory. Graph theory is discrete structures, consisting of vertices and edges that connect these vertices. Problems in almost every conceivable discipline can be solved using graph models. The field graph theory started its journey from the problem of Konigsberg Bridges in 1735. This paper is a guide for the applied mathematician who would like to know more about network security, cryptography and cyber security based of graph theory. The paper gives a brief overview of the subject and the applications of graph theory in computer security, and provides pointers to key research and recent survey papers in the area.",90-96,2014.0,https://waters-cook.org/categories/apphome.html,Computer Science
2071,1f69420a7a8f7b78a7edc74036c89586f09ef517,Graph theory-recent developments of its application in geomorphology,,130-146,2014.0,http://www.baldwin.com/search/explorehomepage.jsp,Geology
2072,b131ac160af2c3ef91aff47f6578067183ca4c4b,"Descriptive Complexity, Canonisation, and Definable Graph Structure Theory","Descriptive complexity theory establishes a connection between the computational complexity of algorithmic problems (the computational resources required to solve the problems) and their descriptive complexity (the language resources required to describe the problems). This ground-breaking book approaches descriptive complexity from the angle of modern structural graph theory, specifically graph minor theory. It develops a ‘definable structure theory’ concerned with the logical definability of graph-theoretic concepts such as tree decompositions and embeddings. The first part starts with an introduction to the background, from logic, complexity, and graph theory, and develops the theory up to first applications in descriptive complexity theory and graph isomorphism testing. It may serve as the basis for a graduate-level course. The second part is more advanced and mainly devoted to the proof of a single, previously unpublished theorem: properties of graphs with excluded minors are decidable in polynomial time if, and only if, they are definable in fixed-point logic with counting.",71-123,2017.0,http://www.patterson.net/searchcategory.asp,Computer Science
2073,3278d34e0eb3d58d36cc2300a4396664a6c950e5,Chemical Graph Theory,This chapter on chemical graph theory forms part of the natural science and processes section of the handbook,1538-1558,2013.0,https://www.vega.net/posts/app/listcategory.htm,Mathematics
2074,19c0d004bd0e42a6449d8b7717cbda4431a67e65,Principal Neighbourhood Aggregation for Graph Nets,"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",59-127,2020.0,http://white.com/tags/explore/searchcategory.html,Computer Science
2075,e4715a13f6364b1c81e64f247651c3d9e80b6808,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",5171-5181,2018.0,http://romero-peterson.net/list/list/categoriespost.htm,Computer Science
2076,3bf89f38b238dd2d4ca4870e6b1fb28dbd136c84,"Handbook of Graph Theory, Second Edition","In the ten years since the publication of the best-selling first edition, more than 1,000 graph theory papers have been published each year. Reflecting these advances, Handbook of Graph Theory, Second Edition provides comprehensive coverage of the main topics in pure and applied graph theory. This second editionover 400 pages longer than its predecessorincorporates 14 new sections. Each chapter includes lists of essential definitions and facts, accompanied by examples, tables, remarks, and, in some cases, conjectures and open problems. A bibliography at the end of each chapter provides an extensive guide to the research literature and pointers to monographs. In addition, a glossary is included in each chapter as well as at the end of each section. This edition also contains notes regarding terminology and notation. With 34 new contributors, this handbook is the most comprehensive single-source guide to graph theory. It emphasizes quick accessibility to topics for non-experts and enables easy cross-referencing among chapters.",36-110,2013.0,http://salas.com/mainauthor.php,Computer Science
2077,a23407b19100acba66fcfc2803d251a3c829e9e3,Applying Graph theory to the Internet of Things,"In the Internet of Things (IoT), we all are ``things''. Graph theory, a branch of discrete mathematics, has been proven to be useful and powerful in understanding complex networks in history. By means of graph theory, we define new concepts and terminology, and explore the definition of IoT, and then show that IoT is the union of a topological network, a data-functional network and a domi-functional network.",2354-2361,2013.0,http://shannon-morales.com/tags/posts/tagmain.asp,Computer Science
2078,4d89d0da3bb1c8bc2ef36c7641abd41149def20b,Graph theory and molecular orbitals. Total φ-electron energy of alternant hydrocarbons,,535-538,1972.0,https://www.wilson-luna.com/blog/tag/mainhomepage.htm,Chemistry
2079,bfacd964ba59bcc6b87b251fa3b30df90736c315,Fuzzy Graph Theory: A Survey,"A fuzzy graph (f-graph) is a pair G : ( σ, �) where σ is a fuzzy subset of a set S andis a fuzzy relation on σ. A fuzzy graph H : ( τ, υ) is called a partial fuzzy subgraph of G : ( σ, �) if τ (u) ≤ σ(u) for every u and υ (u, v) ≤ �(u, v) for every u and v . In particular we call a partial fuzzy subgraph H : ( τ, υ) a fuzzy subgraph of G : ( σ, � ) if τ (u) = σ(u) for every u in τ * and υ (u, v) = �(u, v) for every arc (u, v) in υ*. A connected f-graph G : ( σ, �) is a fuzzy tree(f-tree) if it has a fuzzy spannin g subgraph F : (σ, υ), which is a tree, where for all arcs (x, y) not i n F there exists a path from x to y in F whose strength is more than �(x, y). A path P of length n is a sequence of disti nct nodes u0, u 1, ..., u n such that �(u i1 , u i) > 0, i = 1, 2, ..., n and the degree of membershi p of a weakest arc is defined as its strength. If u 0 = u n and n ≥ 3, then P is called a cycle and a cycle P is called a fuzzy cycle(f-cycle) if it cont ains more than one weakest arc . The strength of connectedness between two nodes x and y is defined as the maximum of the strengths of all paths between x and y and is denot ed by CONN G(x, y). An x y path P is called a strongest x y",54-136,2013.0,http://white-leon.com/tags/categoriesindex.htm,Mathematics
2080,6b5d3dbfe9ed31517b657ea9072064ec1a024b4b,Introduction to Graph Theory and Algebraic Graph Theory,,15-35,2013.0,http://miller.com/list/app/blogprivacy.php,Mathematics
2081,fbe0aeb6d1aaca06c84c985ce63b7c061569dd99,Comparing Brain Networks of Different Size and Connectivity Density Using Graph Theory,"Graph theory is a valuable framework to study the organization of functional and anatomical connections in the brain. Its use for comparing network topologies, however, is not without difficulties. Graph measures may be influenced by the number of nodes (N) and the average degree (k) of the network. The explicit form of that influence depends on the type of network topology, which is usually unknown for experimental data. Direct comparisons of graph measures between empirical networks with different N and/or k can therefore yield spurious results. We list benefits and pitfalls of various approaches that intend to overcome these difficulties. We discuss the initial graph definition of unweighted graphs via fixed thresholds, average degrees or edge densities, and the use of weighted graphs. For instance, choosing a threshold to fix N and k does eliminate size and density effects but may lead to modifications of the network by enforcing (ignoring) non-significant (significant) connections. Opposed to fixing N and k, graph measures are often normalized via random surrogates but, in fact, this may even increase the sensitivity to differences in N and k for the commonly used clustering coefficient and small-world index. To avoid such a bias we tried to estimate the N,k-dependence for empirical networks, which can serve to correct for size effects, if successful. We also add a number of methods used in social sciences that build on statistics of local network structures including exponential random graph models and motif counting. We show that none of the here-investigated methods allows for a reliable and fully unbiased comparison, but some perform better than others.",58-118,2010.0,http://www.odonnell.net/search/category/postsauthor.htm,Medicine
2082,3ba0fab49dc15f0d39c1ffa8f8f767b506e2918c,Graph Theory with Algorithms and its Applications,,71-146,2013.0,https://joyce.com/search/tags/apphome.html,Computer Science
2083,d089f5e5d0548df6066ff281f3918ed67ae742a0,Recent developments in graph Ramsey theory,"Given a graph $H$, the Ramsey number $r(H)$ is the smallest natural number $N$ such that any two-colouring of the edges of $K_N$ contains a monochromatic copy of $H$. The existence of these numbers has been known since 1930 but their quantitative behaviour is still not well understood. Even so, there has been a great deal of recent progress on the study of Ramsey numbers and their variants, spurred on by the many advances across extremal combinatorics. In this survey, we will describe some of this progress.",49-118,2015.0,https://webb.net/searchprivacy.htm,Mathematics
2084,d133cb102ad0f81e3fd17a7db090b28afc124c4a,Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.,"The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.","
          145301
        ",2017.0,https://www.nelson-montgomery.com/tag/applogin.html,Computer Science
2085,4ce9c20642dce5eb7930966053a1e3da4ef617f2,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős -- Renyi graph. We show that when the Erdős -- Renyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at this https URL.",99-112,2019.0,https://www.howard.com/search/explore/listprivacy.htm,Computer Science
2086,ab9accbaf61438bf518de0af0946f4bf12b7e158,"Network meta‐analysis, electrical networks and graph theory","Network meta‐analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph‐theoretical methods can be applied to network meta‐analysis. A meta‐analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta‐analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph‐theoretical methods that have been routinely applied to electrical networks also work well in network meta‐analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore–Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta‐analysis and is consistent with published results when applied to network meta‐analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi‐armed trials are addressed. Copyright © 2012 John Wiley & Sons, Ltd.",72-127,2012.0,http://gutierrez-collins.com/listhomepage.html,Computer Science
2087,b54b6e737d5ba0f1b2c128a90570652b9b2fc3f5,Extremal Graph Theory,"The basic statement of extremal graph theory is Mantel’s theorem, proved in 1907, which states that any graph on n vertices with no triangle contains at most n2/4 edges. This is clearly best possible, as one may partition the set of n vertices into two sets of size bn/2c and dn/2e and form the complete bipartite graph between them. This graph has no triangles and bn2/4c edges. As a warm-up, we will give a number of different proofs of this simple and fundamental theorem.",115-139,2011.0,http://www.dixon-wallace.info/tagsterms.html,Mathematics
2088,29af614088e9a2b387c2dc81539cf8420483c28a,A First Course in Graph Theory,"Because of its inherent simplicity, graph theory has a wide range of applications in engineering, and in physical sciences. It has of course uses in social sciences, in linguistics and in numerous other areas. In fact, a graph can be used to represent almost any physical situation involving discrete objects and the relationship among them. Now with the solutions to engineering and other problems becoming so complex leading to larger graphs, it is virtually difficult to analyze without the use of computers. This book is recommended in IIT Kharagpur, West Bengal for B.Tech Computer Science, NIT Arunachal Pradesh, NIT Nagaland, NIT Agartala, NIT Silchar, Gauhati University, Dibrugarh University, North Eastern Regional Institute of Management, Assam Engineering College, West Bengal Univerity of Technology (WBUT) for B.Tech, M.Tech Computer Science, University of Burdwan, West Bengal for B.Tech. Computer Science, Jadavpur University, West Bengal for M.Sc. Computer Science, Kalyani College of Engineering, West Bengal for B.Tech. Computer Science. Key Features: This book provides a rigorous yet informal treatment of graph theory with an emphasis on computational aspects of graph theory and graph-theoretic algorithms. Numerous applications to actual engineering problems are incorpo-rated with software design and optimization topics.",70-106,2012.0,https://rice.org/categoriessearch.php,Computer Science
2089,995f4c79933bf2219c26a634207449f255ae7b6e,Boundary-connectivity via graph theory,"We generalize theorems of Kesten and Deuschel-Pisztora about the connectedness of the exterior boundary of a connected subset of Zd, where “connectedness” and “boundary” are understood with respect to various graphs on the vertices of Zd. These theorems are widely used in statistical physics and related areas of probability. We provide simple and elementary proofs of their results. It turns out that the proper way of viewing these questions is graph theory instead of topology. Denote by Z the usual nearest-neighbor lattice on Z, i.e., two points of Z are adjacent if they differ only in one coordinate, by 1. Let Zd∗ be the graph on the same vertex set and edges between every two distinct points that differ in every coordinate by at most 1. We say that a set of vertices in Z is *-connected if it is connected in the graph Zd∗. In [DP] Deuschel and Pisztora prove that the part of the outer vertex boundary of a finite connected subgraph C in Zd∗ that is visible from infinity (the exterior boundary) is *-connected. Earlier, Kesten [K] showed that the set of points in the *-boundary of a connected subgraph C ⊂ Zd∗ that are Z-visible from infinity is connected in Z. Similar results were proved about the case when C is in an n× n box of Z [DP], or Zd∗ [H]. See the second paragraphs of Theorem 3 and Theorem 4 for the precise statements. We generalize these results about Z and Zd∗ to a very general family of pairs of graphs; see Lemma 2, Theorem 3 and Theorem 4. Our method also gives an elementary and short alternative to the original proofs for the cubic grid case. This approach seems to be efficient to treat possible other questions about the connectedness of boundaries. Although [K] mentions that some use of algebraic topology seems to be unavoidable, the greater generality (and simplicity) of our proof is a result of using purely graph-theoretic arguments. Also, it makes slight modifications of the results (such as considering boundaries in some subset of Z instead of boundaries in Z) straightforward, while previously one had to go through the original proofs and make significant modifications. In two dimensions, the use of some duality argument makes connectedness of boundaries more straightforward to prove. The lack of duality (that is, the correspondance that a cycle in one graph is a separating set in its dual) in higher Received by the editors March 25, 2010 and, in revised form, February 21, 2011; July 1, 2011; and July 5, 2011. 2010 Mathematics Subject Classification. Primary 05C10, 05C63; Secondary 20F65, 60K35. c ©2012 American Mathematical Society Reverts to public domain 28 years from publication",475-480,2012.0,http://www.blankenship-davis.com/categoryauthor.jsp,Physics
2090,5656c718164f6ab8950f13d78ccf5d3b9942df7f,Test-Retest Reliability of Graph Theory Measures of Structural Brain Connectivity,,"
          305-12
        ",2012.0,https://jenkins-huerta.com/tagsauthor.htm,Computer Science
2091,a51a630f1088baff85282993ebf14fb910ccf126,Using graph theory to analyze biological networks,,10 - 10,2011.0,http://www.hunter-phillips.net/list/listlogin.php,Computer Science
2092,6fa7236e8ff0d958b77164f72b4ce93df5c61caa,Graph Algorithms,,86-129,2018.0,https://www.martinez.net/bloglogin.htm,Computer Science
2093,f2ab3c4e6fae8e666cc68ba1b5672dfddea71123,Local Higher-Order Graph Clustering,"Local graph clustering methods aim to find a cluster of nodes by exploring a small region of the graph. These methods are attractive because they enable targeted clustering around a given seed node and are faster than traditional global graph clustering methods because their runtime does not depend on the size of the input graph. However, current local graph partitioning methods are not designed to account for the higher-order structures crucial to the network, nor can they effectively handle directed networks. Here we introduce a new class of local graph clustering methods that address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs. We develop the Motif-based Approximate Personalized PageRank (MAPPR) algorithm that finds clusters containing a seed node with minimal \emph{motif conductance}, a generalization of the conductance metric for network motifs. We generalize existing theory to prove the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif conductance). We also develop a theory of node neighborhoods for finding sets that have small motif conductance, and apply these results to the case of finding good seed nodes to use as input to the MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that our new framework MAPPR outperforms the current edge-based personalized PageRank methodology.",39-146,2017.0,http://www.berry.com/search/tagssearch.php,Computer Science
2094,cfa6e9e2331fca45f5c948a3477164fd14c956c9,"Graph Theory, 4th Edition",,"I-XVIII, 1-436",2012.0,http://levy-medina.com/category/tagcategory.php,Computer Science
2095,287650fad5a478581f0040ad064e6856c7c13af2,Topological Graph Theory,Introduction Voltage Graphs and Covering Spaces Surfaces and Graph Imbeddings Imbedded Voltage Graphs and Current Graphs Map Colorings The Genus of A Group References.,610-786,1987.0,https://www.ellis.com/appmain.htm,Computer Science
2096,8df5d1e909de14932e42b347adf35070f60dc9ba,"An $L^p$ theory of sparse graph convergence I: Limits, sparse random graph models, and power law distributions","We introduce and develop a theory of limits for sequences of sparse graphs based on $L^p$ graphons, which generalizes both the existing $L^\infty$ theory of dense graph limits and its extension by Bollob\'as and Riordan to sparse graphs without dense spots. In doing so, we replace the no dense spots hypothesis with weaker assumptions, which allow us to analyze graphs with power law degree distributions. This gives the first broadly applicable limit theory for sparse graphs with unbounded average degrees. In this paper, we lay the foundations of the $L^p$ theory of graphons, characterize convergence, and develop corresponding random graph models, while we prove the equivalence of several alternative metrics in a companion paper.",23-129,2014.0,https://www.thornton.com/tags/explore/wp-contentabout.php,Mathematics
2097,fc472f5017c6200e79b7540be07bc6b4798b4ef8,A review of evolutionary graph theory with applications to game theory,,"
          66-80
        ",2012.0,https://www.chavez.com/tags/explore/searchprivacy.html,Computer Science
2098,1f400c9732c13eae95be34a181c2b8042a774a39,Handbook of graph theory,"Introduction to Graphs Fundamentals of Graph Theory, Jonathan L. Gross and Jay Yellen Families of Graphs and Digraphs, Lowell W. Beineke History of Graph Theory, Robin J. Wilson Graph Representation Computer Representation of Graphs, Alfred V. Aho Graph Isomorphism, Brendan D. McKay The Reconstruction Problem, Josef Lauri Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Structural Graph Theory, Maria Chudnovsky Directed Graphs Basic Digraph Models and Properties, Jay Yellen Directed Acyclic Graphs, Stephen B. Maurer Tournaments, K.B. Reid Connectivity and Traversability Connectivity Properties and Structure, Camino Balbuena, Josep Fabrega, and Miguel Angel Fiol Eulerian Graphs, Herbert Fleischner Chinese Postman Problems, R. Gary Parker and Richard B. Borie DeBruijn Graphs and Sequences, A.K. Dewdney Hamiltonian Graphs, Ronald J. Gould Traveling Salesman Problems, Gregory Gutin Further Topics in Connectivity, Josep Fabrega and Miguel Angel Fiol Colorings and Related Topics Graph Coloring, Zsolt Tuza Further Topics in Graph Coloring, Zsolt Tuza Independence and Cliques, Gregory Gutin Factors and Factorization, Michael Plummer Applications to Timetabling, Edmund Burke, Dominique de Werra, and Jeffrey Kingston Graceful Labelings, Joseph A. Gallian Algebraic Graph Theory Automorphisms, Mark E. Watkins Cayley Graphs, Brian Alspach Enumeration, Paul K. Stockmeyer Graphs and Vector Spaces, Krishnaiyan ""KT"" Thulasiraman Spectral Graph Theory, Michael Doob Matroidal Methods in Graph Theory, James Oxley Topological Graph Theory Graphs on Surfaces, Tomaz Pisanski and Primoz Potocnik Minimum Genus and Maximum Genus, Jianer Chen Genus Distributions, Jonathan L. Gross Voltage Graphs, Jonathan L. Gross The Genus of a Group, Thomas W. Tucker Maps, Roman Nedela and Martin Skoviera Representativity, Dan Archdeacon Triangulations, Seiya Negami Graphs and Finite Geometries, Arthur T. White Crossing Numbers, R. Bruce Richter and Gelasio Salazar Analytic Graph Theory Extremal Graph Theory, Bela Bollobas and Vladimir Nikiforov Random Graphs, Nicholas Wormald Ramsey Graph Theory, Ralph J. Faudree The Probabilistic Method, Alan Frieze and Po-Shen Loh Graph Limits, Bojan Mohar Graphical Measurement Distance in Graphs, Gary Chartrand and Ping Zhang Domination in Graphs, Teresa W. Haynes and Michael A. Henning Tolerance Graphs, Martin Charles Golumbic Bandwidth, Robert C. Brigham Pursuit-Evasion Problems, Richard B. Borie, Sven Koenig, and Craig A. Tovey Graphs in Computer Science Searching, Harold N. Gabow Dynamic Graph Algorithms, Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano Drawings of Graphs, Emilio Di Giacomo, Giuseppe Liotta, and Roberto Tamassia Algorithms on Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Fuzzy Graphs, John N. Mordeson and D.S. Malik Expander Graphs, Mike Krebs and Anthony Shaheen Visibility Graphs, Alice M. Dean and Joan P. Hutchinson Networks and Flows Maximum Flows, Clifford Stein Minimum Cost Flows, Lisa Fleischer Matchings and Assignments, Jay Sethuraman and Douglas R. Shier Communication Networks Complex Networks, Anthony Bonato and Fan Chung Broadcasting and Gossiping, Hovhannes A. Harutyunyan, Arthur L. Liestman, Joseph G. Peters, and Dana Richards Communication Network Design Models, Prakash Mirchandani and David Simchi-Levi Network Science for Graph Theorists, David C. Arney and Steven B. Horton Natural Science and Processes Chemical Graph Theory, Ernesto Estrada and Danail Bonchev Ties between Graph Theory and Biology, Jacek Blazewicz, Marta Kasprzak, and Nikos Vlassis Index A Glossary appears at the end of each chapter.",1-1167,2007.0,https://www.hart-todd.info/searchhomepage.html,Computer Science
2099,3265e606639c89e64e5cac5e32dd3e9081b387be,Spectral Graph Theory,16.,100-133,2012.0,http://www.chang-davis.com/tags/list/categoryprivacy.php,Engineering
2100,3265e606639c89e64e5cac5e32dd3e9081b387be,Spectral Graph Theory,16.,89-131,2012.0,http://www.gray.com/tagindex.html,Engineering
2101,74b6089b46e3e12f74da7d5d981d787aa7fb8459,Active semi-supervised learning using sampling theory for graph signals,"We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method.",63-116,2014.0,http://www.johnson.com/categoriesabout.html,Computer Science
2102,36e3cf186ebc510800fb6e217cc8edae93fbf4f9,"An $L^{p}$ theory of sparse graph convergence II: LD convergence, quotients and right convergence","We extend the LpLp theory of sparse graph limits, which was introduced in a companion paper, by analyzing different notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs and sparse versions of WW-random graphs.",337-396,2014.0,http://peterson.com/posts/blogabout.htm,Mathematics
2103,7dbdb4209626fd92d2436a058663206216036e68,Elements of Information Theory,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.",25-124,2005.0,http://www.reid-thomas.com/list/list/categoriespost.html,Engineering
2104,69381b5efd97e7c55f51c2730caccab3d632d4d2,Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions",40-51,2007.0,https://www.gill-lucero.com/tagshome.jsp,Mathematics
2105,7f5ea861a57e14796f033fd0f5580dbc34ff88f2,Relational Pooling for Graph Representations,"This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.",55-140,2019.0,https://www.shaw-thornton.com/posts/categories/listhome.html,Computer Science
2106,f271c09c88e5b136d52d42d82f16e2e3f3a19642,Graph theory,,"I-V, 1-274",1969.0,https://www.smith-butler.biz/posts/wp-content/tagshome.asp,Computer Science
2107,39afec82e60d44f003cbd15e85d892b6fc5016e9,"Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices","The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective. Mathematics Subject Classification (2010). Primary 68Q25; Secondary 65F08.",2698-2722,2011.0,https://ortega.com/blogcategory.php,Mathematics
2108,f2a34394994089ee965dd9fa7a4bbc2446432770,Applications of Graph Theory and Network Science to Transit Network Design,"Abstract While the network nature of public transportation systems is well known, the study of their design from a topological/geometric perspective remains relatively limited. From the work of Euler in the 1750s to the discovery of scale-free networks in the late 1990s, the goal of this paper is to review the topical literature that applied concepts of graph theory and network science. After briefly introducing the origins of graph theory, we review early indicators developed to study transport networks, which notably includes the works of Garrison and Marble, and Kansky. Afterwards, we examine network indicators and characteristics developed to study transit systems specifically, in particular by reviewing the works of Vuchic and Musso. Subsequently, we introduce the concepts of small-worlds and scale-free networks from the emerging field network science, and review early applications to transit networks. Finally, we identify three challenges that will need to be addressed in the future. As transit systems are likely to grow in the world, the study of their network feature could be of substantial help to planners so as to better design the transit systems of tomorrow, but much work lies ahead.",495 - 519,2011.0,https://www.stout.org/wp-contenthome.htm,Computer Science
2109,2ced07a295025f46ae51efa0a61679815c0866c5,Some new results in extremal graph theory,"In recent years several classical results in extremal graph theory have been improved in a uniform way and their proofs have been simplified and streamlined. These results include a new Erd\H{o}s-Stone-Bollob\'as theorem, several stability theorems, several saturation results and bounds for the number of graphs with large forbidden subgraphs. Another recent trend is the expansion of spectral extremal graph theory, in which extremal properties of graphs are studied by means of eigenvalues of various matrices. One particular achievement in this area is the casting of the central results above in spectral terms, often with additional enhancement. In addition, new, specific spectral results were found that have no conventional analogs. All of the above material is scattered throughout various journals, and since it may be of some interest, the purpose of this survey is to present the best of these results in a uniform, structured setting, together with some discussions of the underpinning ideas.",25-103,2011.0,https://french-gutierrez.com/categories/searchsearch.html,Mathematics
2110,6b7fe2878d6c42079562f8adb57635dd2f2a2772,Review of graph theory: a problem oriented approach by Daniel Marcus,"Graph Theory: A Problem Oriented Approach is a textbook cum workbook with the goal to assist a proactive and problem-oriented approach to learning graph theory. This goal is clear from the offset as many key concepts, examples and even proof arguments are clarified or highlighted by the help of carefully chosen problem questions. A reader unfamiliar with graph theory is not kept in the dark as problems are preceded by some introductory text which nudges the reader along. The book consists of over three hundred strategically placed problems interspersed by necessary text. There are also further problems to think about at the end of each chapter. The book is written in a similar style as a book on combinatorics by the same author [2] .",31-32,2011.0,http://www.gregory.com/main/listterms.jsp,Mathematics
2111,8bd4a8eb93ede5c55c09bed842be671c8cb3413f,Robust automatic segmentation of corneal layer boundaries in SDOCT images using graph theory and dynamic programming,"Segmentation of anatomical structures in corneal images is crucial for the diagnosis and study of anterior segment diseases. However, manual segmentation is a time-consuming and subjective process. This paper presents an automatic approach for segmenting corneal layer boundaries in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Our approach is robust to the low-SNR and different artifact types that can appear in clinical corneal images. We show that our method segments three corneal layer boundaries in normal adult eyes more accurately compared to an expert grader than a second grader—even in the presence of significant imaging outliers.",1524 - 1538,2011.0,https://daugherty.org/main/tag/appabout.html,Computer Science
2112,cc2c33aea50ef4e2aec9413f83ccd7d35dace3c6,Applications of Graph Theory in Computer Science,"Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in computer science, physical, biological and social systems. Many problems of practical interest can be represented by graphs. In general graphs theory has a wide range of applications in diverse fields. This paper explores different elements involved in graph theory including graph representations using computer systems and graph-theoretic data structures such as list structure and matrix structure. The emphasis of this paper is on graph applications in computer science. To demonstrate the importance of graph theory in computer science, this article addresses most common applications for graph theory in computer science. These applications are presented especially to project the idea of graph theory and to demonstrate its importance in computer science.",142-145,2011.0,https://www.aguilar-george.com/explore/wp-contentterms.jsp,Computer Science
2113,a44b5f0314a12aca034d52cf047445cdf7c884bf,"Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications","Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications focuses on discrete mathematics and combinatorial algorithms interacting with real world problems in computer science, operations research, applied mathematics and engineering.The book containseleven chapters written by experts in their respective fields, and covers a wide spectrum of high-interest problems across these discipline domains. Among the contributing authors are Richard Karp of UC Berkeley and Robert Tarjan of Princeton; both are at the pinnacle of research scholarship in Graph Theory and Combinatorics. The chapters from the contributing authors focus on ""real world"" applications, all of which will be of considerable interest across the areas of Operations Research, Computer Science, Applied Mathematics, and Engineering. These problems include Internet congestion control, high-speed communication networks, multi-object auctions, resource allocation, software testing, data structures, etc. In sum, this is a book focused on major, contemporary problems, written by the top research scholars in the field, using cutting-edge mathematical and computational techniques.",96-115,2011.0,http://www.peters.org/categorypost.html,Computer Science
2114,1a6b1bac33da9cdff5fc311e9b9e4bd065bd7a9f,Systemic Risk in Energy Derivative Markets: A Graph-Theory Analysis,"Abstract This article uses graph theory to provide novel evidence regarding market integration, a favorable condition for systemic risk to appear in. Relying on daily futures returns covering a 12-year period, we examine cross- and intermarket linkages, both within the commodity complex and between commodities and other financial assets. In such a high dimensional analysis, graph theory enables us to understand the dynamic behavior of our price system. We show that energy markets—as a whole—stand at the heart of this system. We also establish that crude oil is itself at the center of the energy complex. Further, we provide evidence that commodity markets have become more integrated over time. Keywords: Systemic risk, Energy, Derivative markets, High dimensional analysis, Graph theory, Minimum spanning trees",215 - 240,2011.0,https://www.lucero-hardin.info/posts/tags/mainauthor.php,Economics
2115,54be361ad20a895831dd19f70712c885c94a6289,Spatio-Temporal Graph Deep Neural Network for Short-Term Wind Speed Forecasting,"Wind speed forecasting is still a challenge due to the stochastic and highly varying characteristics of wind. In this paper, a graph deep learning model is proposed to learn the powerful spatio-temporal features from the wind speed and wind direction data in neighboring wind farms. The underlying wind farms are modeled by an undirected graph, where each node corresponds to a wind site. For each node, temporal features are extracted using a long short-term memory Network. A scalable graph convolutional deep learning architecture (GCDLA), motivated by the localized first-order approximation of spectral graph convolutions, leverages the extracted temporal features to forecast the wind-speed time series of the whole graph nodes. The proposed GCDLA captures spatial wind features as well as deep temporal features of the wind data at each wind site. To further improve the prediction accuracy and capture robust latent representations, the rough set theory is incorporated with the proposed graph deep network by introducing upper and lower bound parameter approximations in the model. Simulation results show the advantages of capturing deep spatial and temporal interval features in the proposed framework compared to the state-of-the-art deep learning models as well as shallow architectures in the recent literature.",670-681,2019.0,http://walls.com/list/mainindex.htm,Computer Science
2116,30cb5c2906115873a4b9fcb0fc099a10f30465cc,Applications of Graph Theory,"Graph theory is becoming increasingly significant as it is applied to other areas of mathematics, science and technology. It is being actively used in fields as varied as biochemistry (genomics), electrical engineering (communication networks and coding theory), computer science (algorithms and computation) and operations research (scheduling). The powerful combinatorial methods found in graph theory have also been used to prove fundamental results in other areas of pure mathematics. This paper, besides giving a general outlook of these facts, includes new graph theoretical proofs of Fermat’s Little Theorem and the Nielson-Schreier Theorem. New applications to DNA sequencing (the SNP assembly problem) and computer network security (worm propagation) using minimum vertex covers in graphs are discussed. We also show how to apply edge coloring and matching in graphs for scheduling (the timetabling problem) and vertex coloring in graphs for map coloring and the assignment of frequencies in GSM mobile phone networks. Finally, we revisit the classical problem of finding re-entrant knight’s tours on a chessboard using Hamiltonian circuits in graphs.",23-116,2011.0,https://chaney.com/app/categoryfaq.htm,Computer Science
2117,f4d8cfaab1a9bf229ca8b7de997a445446754a3a,Graph Theory with Applications to Engineering and Computer Science,Key,299-300,1975.0,http://lynn.com/explore/categoryterms.html,Computer Science
2118,e39ba4c989874285e9473b4532dc275c12ed78c0,Computing the shortest path: A search meets graph theory,"We propose shortest path algorithms that use A* search in combination with a new graph-theoretic lower-bounding technique based on landmarks and the triangle inequality. Our algorithms compute optimal shortest paths and work on any directed graph. We give experimental results showing that the most efficient of our new algorithms outperforms previous algorithms, in particular A* search with Euclidean bounds, by a wide margin on road networks and on some synthetic problem families.",156-165,2005.0,https://www.gray-chen.com/wp-content/explore/tagspost.html,Mathematics
2119,db9967871249ffd77adc0c0cc16154ccc37d09ad,A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory,,619-633,1975.0,http://evans-wilson.com/list/wp-contentsearch.html,Mathematics
2120,f96f659ada20f35e9fce66306c8f8f6a471e592d,Graph theoretical analysis of magnetoencephalographic functional connectivity in Alzheimer's disease.,"In this study we examined changes in the large-scale structure of resting-state brain networks in patients with Alzheimer's disease compared with non-demented controls, using concepts from graph theory. Magneto-encephalograms (MEG) were recorded in 18 Alzheimer's disease patients and 18 non-demented control subjects in a no-task, eyes-closed condition. For the main frequency bands, synchronization between all pairs of MEG channels was assessed using a phase lag index (PLI, a synchronization measure insensitive to volume conduction). PLI-weighted connectivity networks were calculated, and characterized by a mean clustering coefficient and path length. Alzheimer's disease patients showed a decrease of mean PLI in the lower alpha and beta band. In the lower alpha band, the clustering coefficient and path length were both decreased in Alzheimer's disease patients. Network changes in the lower alpha band were better explained by a 'Targeted Attack' model than by a 'Random Failure' model. Thus, Alzheimer's disease patients display a loss of resting-state functional connectivity in lower alpha and beta bands even when a measure insensitive to volume conduction effects is used. Moreover, the large-scale structure of lower alpha band functional networks in Alzheimer's disease is more random. The modelling results suggest that highly connected neural network 'hubs' may be especially at risk in Alzheimer's disease.","
          213-24
        ",2009.0,https://doyle.com/wp-contentcategory.jsp,Medicine
2121,3375dc753685773f050392d1b5dfd9a4e3d20b96,Chemical Graph Theory,"INTRODUCTION. ELEMENTS OF GRAPH THEORY. The Definition of a Graph. Isomorphic Graphs and Graph Automorphism. Walks, Trails, Paths, Distances and Valencies in Graphs. Subgraphs. Regular Graphs. Trees. Planar Graphs. The Story of the Koenigsberg Bridge Problem and Eulerian Graphs. Hamiltonian Graphs. Line Graphs. Vertex Coloring of a Graph. CHEMICAL GRAPHS. The Concept of a Chemical Graph. Molecular Topology. Huckel Graphs. Polyhexes and Benzenoid Graphs. Weighted Graphs. GRAPH-THEORETICAL MATRICES. The Adjacency Matrix. The Distance Matrix. THE CHARACTERISTIC POLYNOMIAL OF A GRAPH. The Definition of the Characteristic Polynomial. The Method of Sachs for Computing the Characteristic Polynomial. The Characteristic Polynomials of Some Classes of Simple Graphs. The Le Verrier-Faddeev-Frame Method for Computing the Characteristic Polynomial. TOPOLOGICAL ASPECTS OF HUECKEL THEORY. Elements of Huckel Theory. Isomorphism of Huckel Theory and Graph Spectral Theory. The Huckel Spectrum. Charge Densities and Bond Orders in Conjugated Systems. The Two-Color Problem in Huckel Theory. Eigenvalues of Linear Polyenes. Eigenvalues of Annulenes. Eigenvalues of Moebius Annulenes. A Classification Scheme for Monocyclic Systems. Total p-Electron Energy. TOPOLOGICAL RESONANCE ENERGY. Huckel Resonance Energy. Dewar Resonance Energy. The Concept of Topological Resonance Energy. Computation of the Acyclic Polynomial. Applications of the TRE Model. ENUMERATION OF KEKULE VALENCE STRUCTURES. The Role of Kekule Valence Structures in Chemistry. The Identification of Kekule Systems. Methods for the Enumeration of Kekule Structures. The Concept of Parity of Kekule Structures. THE CONJUGATED-CIRCUIT MODEL. The Concept of Conjugated Circuits. The p-Resonance Energy Expression. Selection of the Parameters. Computational Procedure. Applications of the Conjugated-Circuit Model. Parity of Conjugated Circuits. TOPOLOGICAL INDICES. Definitions of Topological Indices. The Three-Dimensional Wiener Number. ISOMER ENUMERATION. The Cayley Generation Functions. The Henze-Blair Approach. The Polya Enumeration Method. The Enumeration Method Based on the N-Tuple Code.",26-131,1992.0,http://moore-torres.com/blogprivacy.php,Technology
2122,c50328abeea3d9dea2146af86d1ca90996de067d,Assessing the vulnerability of supply chains using graph theory,,121-129,2010.0,http://www.adams-davidson.com/wp-contentfaq.html,Business
2123,3f2e5b88c4b631c931f6db805f4ae3c0b531a8e8,APPLICATIONS OF GRAPH THEORY IN COMPUTER SCIENCE AN OVERVIEW,"The field of mathematics plays vital role in various fields. One of the important areas in mathematics is graph theory which is used in structural models. This structural arrangements of various objects or technologies lead to new inventions and modifications in the existing environment for enhancement in those fields. The field graph theory started its journey from the problem of Koinsberg bridge in 1735. This paper gives an overview of the applications of graph theory in heterogeneous fields to some extent but mainly focuses on the computer science applications that uses graph theoretical concepts. Various papers based on graph theory have been studied related to scheduling concepts, computer science applications and an overview has been presented here.",64-150,2010.0,http://marquez.com/searchpost.html,Computer Science
2124,6362675dc7f0a41061ceb6c10852e84e6141e509,Graph Theory in the Information Age,"I n the past decade, graph theory has gone through a remarkable shift and a profound transformation. The change is in large part due to the humongous amount of information that we are confronted with. A main way to sort through massive data sets is to build and examine the network formed by interrelations. For example, Google’s successful Web search algorithms are based on the WWW graph, which contains all Web pages as vertices and hyperlinks as edges. There are all sorts of information networks, such as biological networks built from biological databases and social networks formed by email, phone calls, instant messaging, etc., as well as various types of physical networks. Of particular interest to mathematicians is the collaboration graph, which is based on the data from Mathematical Reviews. In the collaboration graph, every mathematician is a vertex, and two mathematicians who wrote a joint paper are connected by an edge. Figure 1 illustrates a portion of the collaboration graph consisting of about 5,000 vertices, representing mathematicians with Erdős number 2 (i.e., mathematicians who wrote a paper with a coauthor of Paul Erdős). Graph theory has two hundred years of history studying the basic mathematical structures called graphs. A graph G consists of a collection V of vertices and a collection E of edges that connect pairs of vertices. In the past, graph theory has",60-130,2010.0,http://www.smith.org/wp-contentterms.htm,Computer Science
2125,e24f2851d5cf4282239b4fd6a74a5d3bff7f8897,Applied Graph Theory in Computer Vision and Pattern Recognition,,20-146,2010.0,http://jackson.net/postsauthor.html,Computer Science
2126,3bc2b3028064f5b580ee8d5dcc17b6bb346015fb,Graph Theory and Complex Networks: An Introduction,"Chapter Description 01: Introduction History, background 02: Foundations Basic terminology and properties of graphs 03: Extensions Directed & weighted graphs, colorings 04: Network traversal Walking through graphs (cf. traveling) 05: Trees Graphs without cycles; routing algorithms 06: Network analysis Basic metrics for analyzing large graphs 07: Random networks Introduction modeling real-world networks 08: Computer networks The Internet & WWW seen as a huge graph 09: Social networks Communities seen as graphs",78-121,2010.0,https://www.harrison.com/wp-content/posts/appcategory.html,Computer Science
2127,250748b4494cec56abd55ae049bdd38f4d42e5c8,An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation,"A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size ( approximately 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of G, which can be efficiently constructed using the Gomory-Hu algorithm (1961). However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed contours of edge elements (equivalent to minimum cuts in G), which consist mostly of strong edges, while rejecting contours containing isolated strong edges. This method is able to accurately locate region boundaries and at the same time guarantees the formation of closed edge contours. >",1101-1113,1993.0,http://johnson.com/searchfaq.htm,Mathematics
2128,66eaaeb2ddcc73a27247cfce92f7e6691fe988c8,Introduction to Graph Theory,,13-49,2010.0,http://serrano-lewis.com/main/tag/tagsterms.html,Mathematics
2129,ef2704b8e44692c7f6430451434af64df913d575,Introduction to Graph and Hypergraph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Traversals and Flows Basic Hypergraph Concepts Hypertrees and Chordal Hypergraphs Some Other Remarkable Hypergraph Classes Hypergraph Coloring Modeling with Hypergraphs Appendix Index.,44-134,2013.0,http://woods.com/wp-contentlogin.html,Mathematics
2130,dbbef7c54b88ed4814954062d0922fbf0cc34b28,Distance Metric Learning Using Graph Convolutional Networks: Application to Functional Brain Networks,,98-102,2017.0,http://www.ross.com/appterms.htm,Mathematics
2131,75264a58faee7b29b72ff329951d7fd353649da8,On an extremal problem in graph theory,"G ( n;l ) will denote a graph of n vertices and l edges. Let f 0 ( n, k ) be the smallest integer such that there is a G ( n;f 0 (n, k )) in which for every set of k vertices there is a vertex joined to each of these. Thus for example f o = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f o = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k , and then f 0 ( n, k ) = f(n, k) .",251-254,1970.0,https://www.buck-brady.net/posts/explorehome.htm,Mathematics
2132,b696862bbb0f73c6f39fce6a46f9f02842295040,Signed Graph Attention Networks,,566-577,2019.0,https://www.thomas.com/categories/tags/postsabout.asp,Computer Science
2133,a2b76b4529bc7da1696b3b5883ba5b9ab9564d20,Algebraic Graph Theory: COLOURING PROBLEMS,1. Introduction to algebraic graph theory Part I. Linear Algebra in Graphic Thoery: 2. The spectrum of a graph 3. Regular graphs and line graphs 4. Cycles and cuts 5. Spanning trees and associated structures 6. The tree-number 7. Determinant expansions 8. Vertex-partitions and the spectrum Part II. Colouring Problems: 9. The chromatic polynomial 10. Subgraph expansions 11. The multiplicative expansion 12. The induced subgraph expansion 13. The Tutte polynomial 14. Chromatic polynomials and spanning trees Part III. Symmetry and Regularity: 15. Automorphisms of graphs 16. Vertex-transitive graphs 17. Symmetric graphs 18. Symmetric graphs of degree three 19. The covering graph construction 20. Distance-transitive graphs 21. Feasibility of intersection arrays 22. Imprimitivity 23. Minimal regular graphs with given girth References Index.,40-134,1974.0,http://www.king.net/tagterms.asp,Mathematics
2134,2d832d0d1baaaba224a21ca926a10c0529da3628,Functional neural network analysis in frontotemporal dementia and Alzheimer's disease using EEG and graph theory,,101 - 101,2009.0,https://garrett.com/searchauthor.asp,Medicine
2135,54155120a8801ed3afcc3bd3d4c6eb0fb4d028c7,Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory,"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings.",1156-1167,2009.0,http://kim-hunt.com/categoriesfaq.asp,Computer Science
2136,70b0f299d5838aecf6386fb53429407487cae1a8,Efficient Sampling Set Selection for Bandlimited Graph Signals Using Graph Spectral Proxies,"We study the problem of selecting the best sampling set for bandlimited reconstruction of signals on graphs. A frequency domain representation for graph signals can be defined using the eigenvectors and eigenvalues of variation operators that take into account the underlying graph connectivity. Smoothly varying signals defined on the nodes are of particular interest in various applications, and tend to be approximately bandlimited in the frequency basis. Sampling theory for graph signals deals with the problem of choosing the best subset of nodes for reconstructing a bandlimited signal from its samples. Most approaches to this problem require a computation of the frequency basis (i.e., the eigenvectors of the variation operator), followed by a search procedure using the basis elements. This can be impractical, in terms of storage and time complexity, for real datasets involving very large graphs. We circumvent this issue in our formulation by introducing quantities called graph spectral proxies, defined using the powers of the variation operator, in order to approximate the spectral content of graph signals. This allows us to formulate a direct sampling set selection approach that does not require the computation and storage of the basis elements. We show that our approach also provides stable reconstruction when the samples are noisy or when the original signal is only approximately bandlimited. Furthermore, the proposed approach is valid for any choice of the variation operator, thereby covering a wide range of graphs and applications. We demonstrate its effectiveness through various numerical experiments.",3775-3789,2015.0,https://murphy.com/categorieslogin.htm,Computer Science
2137,f81a943a22abc31009f5bafc8c6c966189623593,Graph theory with applications to engineering and computer science,,1533-1534,1975.0,http://morton.net/tagfaq.jsp,Computer Science
2138,2b10bdea7bfe8061a47185d7525c998b8e446832,Graph Theory. An Algorithmic Approach,,1027-1028,1975.0,http://white.info/tags/listfaq.php,Computer Science
2139,ca50b8cb4e34aeca0df18b060911ba28dd79ef90,Graph theory and molecular orbitals,,129-136,1974.0,http://www.cook.org/wp-content/category/categoryauthor.asp,Physics
2140,54cf44503876c4a0c4875aa6f0dff837f0e17f39,Network Analysis of World Subway Systems Using Updated Graph Theory,"This paper demonstrates that network topologies play a key role in attracting people to use public transit; ridership is not solely determined by cultural characteristics (North American versus European versus Asian) or city design (transit oriented versus automobile oriented). The analysis considers 19 subway systems worldwide: those in Toronto, Ontario, Canada; Montreal, Quebec, Canada; Chicago, Illinois; New York City; Washington, D.C.; San Francisco, California; Mexico City, Mexico; London; Paris; Lyon, France; Madrid, Spain; Berlin; Athens, Greece; Stockholm, Sweden; Moscow; Tokyo; Osaka, Japan; Seoul, South Korea; and Singapore. The relationship between ridership and network design was studied by using updated graph theory concepts. Ridership was computed as the annual number of boardings per capita. Network design was measured according to three major indicators. The first is a measure of transit coverage and is based on the total number of stations and land area. The second relates to the maximum number of transfers necessary to go from one station to another and is called directness. The third attempts to get an overall view of transfer possibilities to travel in the network to appreciate a sense of mobility; it is termed connectivity. Multiple-regression analysis showed a strong relationship between these three indicators and ridership, achieving a goodness of fit (adjusted R2 value) of .725. The importance of network design is significant and should be considered in future public transportation projects.",17 - 25,2009.0,https://sheppard.com/posts/category/mainabout.html,Geography
2141,e0ba6ba6c753958b1111b0fda344d9b2fe43ab5d,Insights into the Organization of Biochemical Regulatory Networks Using Graph Theory Analyses*,"Graph theory has been a valuable mathematical modeling tool to gain insights into the topological organization of biochemical networks. There are two types of insights that may be obtained by graph theory analyses. The first provides an overview of the global organization of biochemical networks; the second uses prior knowledge to place results from multivariate experiments, such as microarray data sets, in the context of known pathways and networks to infer regulation. Using graph analyses, biochemical networks are found to be scale-free and small-world, indicating that these networks contain hubs, which are proteins that interact with many other molecules. These hubs may interact with many different types of proteins at the same time and location or at different times and locations, resulting in diverse biological responses. Groups of components in networks are organized in recurring patterns termed network motifs such as feedback and feed-forward loops. Graph analysis revealed that negative feedback loops are less common and are present mostly in proximity to the membrane, whereas positive feedback loops are highly nested in an architecture that promotes dynamical stability. Cell signaling networks have multiple pathways from some input receptors and few from others. Such topology is reminiscent of a classification system. Signaling networks display a bow-tie structure indicative of funneling information from extracellular signals and then dispatching information from a few specific central intracellular signaling nexuses. These insights show that graph theory is a valuable tool for gaining an understanding of global regulatory features of biochemical networks.",5451 - 5455,2009.0,https://www.robertson-barnett.net/explorehomepage.html,Biology
2142,cb03d03377bba14607b0a5603bc90aa4c0650b78,Discovering genetic ancestry using spectral graph theory,"As one approach to uncovering the genetic underpinnings of complex disease, individuals are measured at a large number of genetic variants (usually SNPs) across the genome and these SNP genotypes are assessed for association with disease status. We propose a new statistical method called Spectral‐GEM for the analysis of genome‐wide association studies; the goal of Spectral‐GEM is to quantify the ancestry of the sample from such genotypic data. Ignoring structure due to differential ancestry can lead to an excess of spurious findings and reduce power. Ancestry is commonly estimated using the eigenvectors derived from principal component analysis (PCA). To develop an alternative to PCA we draw on connections between multidimensional scaling and spectral graph theory. Our approach, based on a spectral embedding derived from the normalized Laplacian of a graph, can produce more meaningful delineation of ancestry than by using PCA. Often the results from Spectral‐GEM are straightforward to interpret and therefore useful in association analysis. We illustrate the new algorithm with an analysis of the POPRES data [Nelson et al., 2008]. Genet. Epidemiol. 34:51–59, 2010. © 2009 Wiley‐Liss, Inc.",65-122,2009.0,http://frey-bean.com/explore/main/searchindex.asp,Biology
2143,abdf8623c1e5d8d2fd234a255a1ca6c32c8d6be7,Algorithmic Graph Theory,"Preface 1. Introducing graphs and algorithmic complexity 2. Spanning-trees, branchings and connectivity 3. Planar graphs 4. Networks and flows 5. Matchings 6. Eulerian and Hamiltonian tours 7. Colouring graphs 8. Graph problems and intractability Appendix Author Index Subject Index.",77-110,1985.0,http://www.leonard.com/category/categoriesauthor.htm,Mathematics
2144,6211974939c9e83dd332278af5645228a32ccf57,Discrete Mathematics and Graph Theory,"This comprehensive and self-contained text provides a thorough understanding of the concepts and applications of discrete mathematics and graph theory. It is written in such a manner that beginners can develop an interest in the subject. Besides providing the essentials, it also provides problem-solving techniques and develops the skill of how to think logically. Organized into two parts. The first part on discrete mathematics covers a wide range of topics such as predicate logic, recurrences, generating function, combinatorics, partially-ordered sets, lattices, Boolean algebra, finite state machines, finite fields, elementary number theory and discrete probability. The second part on graph theory covers planarity, colouring and partitioning, directed and algebraic graphs.",30-102,2009.0,https://www.ross.com/tag/categorypost.php,Mathematics
2145,3fdef60d2c7738739bd7b74f5a3f7c076826ed30,Spectral Graph Theory and Network Dependability,"The paper introduces methods of graph theory for ranking substations of an electric power grid. In particular, spectral graph theory is used and several ranking algorithms are described. The procedure is illustrated on a practical numerical example",356-363,2009.0,http://hall.com/search/mainmain.html,Computer Science
2146,bdf8787c9aa949043f866116354e8c8fbc4d5f41,Introduction to Graph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Graph Traversals and Flows Appendix Index.,38-137,2009.0,http://david-thomas.com/searchauthor.htm,Mathematics
2147,4a95403d8fcadaccbf6d46c2a03c0ecf70cb3f9c,Computational Discrete Mathematics: Combinatorics and Graph Theory with Mathematica ®,"With examples of all 450 functions in action plus tutorial text on the mathematics, this book is the definitive guide to Experimenting with Combinatorica, a widely used software package for teaching and research in discrete mathematics. Three interesting classes of exercises are provided--theorem/proof, programming exercises, and experimental explorations--ensuring great flexibility in teaching and learning the material. The Combinatorica user community ranges from students to engineers, researchers in mathematics, computer science, physics, economics, and the humanities. Recipient of the EDUCOM Higher Education Software Award, Combinatorica is included with every copy of the popular computer algebra system Mathematica.",84-122,2009.0,http://www.sanchez.org/explore/bloghome.htm,Computer Science
2148,08b24601e1a1db623f6cebf740c557b23809b509,Application of the graph theory and matrix methods to contractor ranking,,610-619,2009.0,https://www.walters.com/list/tagsmain.html,Engineering
2149,a02c8182213c3501962bc3214cf236beb85b1684,A Graph‐Theory Framework for Evaluating Landscape Connectivity and Conservation Planning,"Abstract:  Connectivity of habitat patches is thought to be important for movement of genes, individuals, populations, and species over multiple temporal and spatial scales. We used graph theory to characterize multiple aspects of landscape connectivity in a habitat network in the North Carolina Piedmont (U.S.A).. We compared this landscape with simulated networks with known topology, resistance to disturbance, and rate of movement. We introduced graph measures such as compartmentalization and clustering, which can be used to identify locations on the landscape that may be especially resilient to human development or areas that may be most suitable for conservation. Our analyses indicated that for songbirds the Piedmont habitat network was well connected. Furthermore, the habitat network had commonalities with planar networks, which exhibit slow movement, and scale‐free networks, which are resistant to random disturbances. These results suggest that connectivity in the habitat network was high enough to prevent the negative consequences of isolation but not so high as to allow rapid spread of disease. Our graph‐theory framework provided insight into regional and emergent global network properties in an intuitive and visual way and allowed us to make inferences about rates and paths of species movements and vulnerability to disturbance. This approach can be applied easily to assessing habitat connectivity in any fragmented or patchy landscape.",96-130,2008.0,https://bishop.com/tags/posts/categorieslogin.php,Computer Science
2150,c29287c9f51cb5adc2db7d62439d1a54a295debb,Chromatic Graph Theory,"Beginning with the origin of the four color problem in 1852, the field of graph colorings has developed into one of the most popular areas of graph theory. Introducing graph theory with a coloring theme, Chromatic Graph Theory explores connections between major topics in graph theory and graph colorings as well as emerging topics. This self-contained book first presents various fundamentals of graph theory that lie outside of graph colorings, including basic terminology and results, trees and connectivity, Eulerian and Hamiltonian graphs, matchings and factorizations, and graph embeddings. The remainder of the text deals exclusively with graph colorings. It covers vertex colorings and bounds for the chromatic number, vertex colorings of graphs embedded on surfaces, and a variety of restricted vertex colorings. The authors also describe edge colorings, monochromatic and rainbow edge colorings, complete vertex colorings, several distinguishing vertex and edge colorings, and many distance-related vertex colorings. With historical, applied, and algorithmic discussions, this text offers a solid introduction to one of the most popular areas of graph theory.",80-132,2008.0,http://brown-walter.com/wp-contentauthor.html,Mathematics
2151,d869bcd9c4c2eb29fb4d01906841d1d79d5c9cb6,AN EXTREMAL PROBLEM IN GRAPH THEORY,"G(?z; I) will denote a graph of n vertices and 1 edges. Let fO(lz, K) be the smallest integer such that there is a G (n; f,, (n, k)) in which for every set of K vertices there is a vertex joined to each of these. Thus for example fO(3, 2) = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f,(4, 2) = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, and",47-126,2001.0,http://pearson.info/app/tagscategory.htm,Technology
2152,7fc5859635b8779519698c33168256fd99ac3741,"Algorithmic Graph Theory and Perfect Graphs (Annals of Discrete Mathematics, Vol 57)",,35-141,2004.0,https://www.white.com/wp-content/categorieshomepage.php,Mathematics
2153,f96289e50a9486e8d423e9d207e89777d8eed5ef,What can graph theory tell us about word learning and lexical retrieval?,"PURPOSE
Graph theory and the new science of networks provide a mathematically rigorous approach to examine the development and organization of complex systems. These tools were applied to the mental lexicon to examine the organization of words in the lexicon and to explore how that structure might influence the acquisition and retrieval of phonological word-forms.


METHOD
Pajek, a program for large network analysis and visualization (V. Batagelj & A. Mvrar, 1998), was used to examine several characteristics of a network derived from a computerized database of the adult lexicon. Nodes in the network represented words, and a link connected two nodes if the words were phonological neighbors.


RESULTS
The average path length and clustering coefficient suggest that the phonological network exhibits small-world characteristics. The degree distribution was fit better by an exponential rather than a power-law function. Finally, the network exhibited assortative mixing by degree. Some of these structural characteristics were also found in graphs that were formed by 2 simple stochastic processes suggesting that similar processes might influence the development of the lexicon.


CONCLUSIONS
The graph theoretic perspective may provide novel insights about the mental lexicon and lead to future studies that help us better understand language development and processing.","
          408-22
        ",2008.0,https://bell.com/tag/blogabout.jsp,Mathematics
2154,36d3fc0411bec5aadfba8087a33022f99658ccf4,Graph Theory and Interconnection Networks,"The advancement of large scale integrated circuit technology has enabled the construction of complex interconnection networks. Graph theory provides a fundamental tool for designing and analyzing such networks. Graph Theory and Interconnection Networks provides a thorough understanding of these interrelated topics. After a brief introduction to graph terminology, this book presents well-known interconnection networks as examples of graphs, followed by in-depth coverage of Hamiltonian graphs. Different types of problems illustrate the wide range of available methods for solving such problems. The text also explores recent progress on the diagnosability of graphs under various models.",57-126,2008.0,https://contreras.com/blogmain.htm,Computer Science
2155,a83bfe8999ba5f6c4baceb03e7a212a9d3b18e00,Topics in Graph Theory: Graphs and Their Cartesian Product,"From specialists in the field, learn about interesting connections and recent developments in the field of graph theory by looking in particular at Cartesian products arguably the most important of the four standard graph products. Many new results in this area appear for the first time in print in this book. Written in an accessible way, this book can be used for personal study in advanced applications of graph theory or for an advanced graph theory course.",72-104,2008.0,http://www.jones-brown.org/categoriessearch.html,Computer Science
2156,e71dc6f83f18f654824c00f66361d346aa896049,Applications of graph theory to landscape genetics,"We investigated the relationships among landscape quality, gene flow, and population genetic structure of fishers (Martes pennanti) in ON, Canada. We used graph theory as an analytical framework considering each landscape as a network node. The 34 nodes were connected by 93 edges. Network structure was characterized by a higher level of clustering than expected by chance, a short mean path length connecting all pairs of nodes, and a resiliency to the loss of highly connected nodes. This suggests that alleles can be efficiently spread through the system and that extirpations and conservative harvest are not likely to affect their spread. Two measures of node centrality were negatively related to both the proportion of immigrants in a node and node snow depth. This suggests that central nodes are producers of emigrants, contain high‐quality habitat (i.e., deep snow can make locomotion energetically costly) and that fishers were migrating from high to low quality habitat. A method of community detection on networks delineated five genetic clusters of nodes suggesting cryptic population structure. Our analyses showed that network models can provide system‐level insight into the process of gene flow with implications for understanding how landscape alterations might affect population fitness and evolutionary potential.",620 - 630,2008.0,http://martin.biz/posts/tagmain.html,Biology
2157,fa1f95d61ab2ac44c9bfdf9b53a80d6617545b95,Proteins as networks: usefulness of graph theory in protein science.,"The network paradigm is based on the derivation of emerging properties of studied systems by their representation as oriented graphs: any system is traced back to a set of nodes (its constituent elements) linked by edges (arcs) correspondent to the relations existing between the nodes. This allows for a straightforward quantitative formalization of systems by means of the computation of mathematical descriptors of such graphs (graph theory). The network paradigm is particularly useful when it is clear which elements of the modelled system must play the role of nodes and arcs respectively, and when topological constraints have a major role with respect to kinetic ones. In this review we demonstrate how nodes and arcs of protein topology are characterized at different levels of definition: 1. Recurrence matrix of hydrophobicity patterns along the sequence 2. Contact matrix of alpha carbons of 3D structures 3. Correlation matrix of motions of different portion of the molecule in molecular dynamics. These three conditions represent different but potentially correlated reticular systems that can be profitably analysed by means of network analysis tools.","
          28-38
        ",2008.0,http://white-moore.com/tagshomepage.php,Computer Science
2158,17c4b0fe4eaf5d96053952a0ffb5ac0b1fbfbe3d,A BIRD'S EYE VIEW OF THE CUT METHOD AND A SURVEY OF ITS APPLICATIONS IN CHEMICAL GRAPH THEORY,"A general description of the cut method is presented and an overview of its applications in chemical graph theory is given. Applications include the Wiener index, the Szeged index, the hyper-Wiener index, the PI index, the weighted Wiener index, Wiener-type indices, and classes of chemical graphs such as trees, benzenoid graphs and phenylenes. A computation of the Wiener index of an arbitrary connected graph using its canonical metric representation is described. Algorithmic issues are also briefly mentioned as well as are the recently introduced CI index and related polynomials.",34-136,2008.0,http://www.sanchez.biz/categoriesterms.html,Mathematics
2159,7233d8f8d9a0aaaa98b395b998b6e82f2f44ca28,GRAPH THEORY AND COMPLEX NETWORKS,"In the past ten years,the fast development of complex network theory has provided a good support for the study of complexity and complex systems,since they describe clearly the important characteristics of complex systems,and show bright prospects in theory and applications.This paper presents mainly the application of graph theory to complex networks,especially to the synchronization problem of complex networks First,its application to the estimations of smallest nonzero,largest eigenvalues and synchronizability index of certain graphs are commented,followed by the effects of subgraph and graph eigenvector in the estimation of synchronizability index.Furthermore,the complexity between the relationships of synchronizability and network structural parameters are discussed via two simple graphs,and the effects of complementary graph, edge-addition and graph operation on the synchronization of complex networks are elaborated.Finally,some possible development directions in complex networks are predicted from the viewpoint of graph and control theory.",98-135,2008.0,http://carpenter.com/tags/list/explorecategory.php,Mathematics
2160,20980d595d9dbdd3f3ef47e5b072b65799f0525a,From Graph Theory to Models of Economic Networks. A Tutorial.,,23-63,2008.0,https://www.friedman.com/categories/blogindex.html,Computer Science
2161,4f1e6ec98650287ead42b1f231831234385c5e6b,Applying Graph Theory to Interaction Design,,501-519,2008.0,http://hopkins-maldonado.org/app/list/blogregister.htm,Computer Science
2162,616e3dce90c8c261dc51c2f8db6306e0c634f518,ON A PROBLEM OF GRAPH THEORY,"Let G "" be a non-directed graph having n vertices, without parallel edges and slings. Let the vertices of Gn be denoted by F 1 ,. . ., Pn. Let v(P j) denote the valency of the point P i and put (0. 1) V(G,) = max v(Pj). 1ninn Let E(G.) denote the number of edges of Gn. Let H d (n, k) denote the set of all graphs Gn for which V (G n) = k and the diameter D (Gn) of which is-d, In the present paper we shall investigate the quantity (0 .2) Thus we want to determine the minimal number N such that there exists a graph having n vertices, N edges and diameter-d and the maximum of the valencies of the vertices of the graph is equal to k. To help the understanding of the problem let us consider the following interpretation. Let be given in a country n airports ; suppose we want to plan a network of direct flights between these airports so that the maximal number of airports to which a given airport can be connected by a direct flight should be equal to k (i .e. the maximum of the capacities of the airports is prescribed), further it should be possible to fly from every airport to any other by changing the plane at most d-1 times ; what is the minimal number of flights by which such a plan can be realized? For instance, if n = 7, k = 3, d= 2 we have F2 (7, 3) = 9 and the extremal graph is shown by Fig. 1. The problem of determining Fd (n, k) has been proposed and discussed recently by two of the authors (see [1]). In § 1 we give a short summary of the results of the paper [1], while in § 2 and 3 we give some new results which go beyond those of [1]. Incidentally we solve a long-standing problem about the maximal number of edges of a graph not containing a cycle of length 4. In § 4 we mention some unsolved problems. Let us mention that our problem can be formulated also in terms of 0-1 matrices as follows : Let M=(a il) be a symmetrical n by n zero-one matrix such 2",81-130,1966.0,http://harris.com/search/taghome.html,Computer Science
2163,d52a75da5a3785753437bd0e74935692764cb157,Recent Results in the Theory of Graph Spectra,,95-148,2012.0,http://daniels.com/taghome.htm,Mathematics
2164,48ebab32e739e314cfcc6a8de9dd145b934aa5e6,Protein flexibility predictions using graph theory,"Techniques from graph theory are applied to analyze the bond networks in proteins and identify the flexible and rigid regions. The bond network consists of distance constraints defined by the covalent and hydrogen bonds and salt bridges in the protein, identified by geometric and energetic criteria. We use an algorithm that counts the degrees of freedom within this constraint network and that identifies all the rigid and flexible substructures in the protein, including overconstrained regions (with more crosslinking bonds than are needed to rigidify the region) and underconstrained or flexible regions, in which dihedral bond rotations can occur. The number of extra constraints or remaining degrees of bond‐rotational freedom within a substructure quantifies its relative rigidity/flexibility and provides a flexibility index for each bond in the structure. This novel computational procedure, first used in the analysis of glassy materials, is approximately a million times faster than molecular dynamics simulations and captures the essential conformational flexibility of the protein main and side‐chains from analysis of a single, static three‐dimensional structure. This approach is demonstrated by comparison with experimental measures of flexibility for three proteins in which hinge and loop motion are essential for biological function: HIV protease, adenylate kinase, and dihydrofolate reductase. Proteins 2001;44:150–165. © 2001 Wiley‐Liss, Inc.",97-108,2001.0,https://phillips.com/posts/list/tagsterms.htm,Medicine
2165,603c893660f04899f6b220795348f236d6896cf3,Graph theory as a proxy for spatially explicit population models in conservation planning.,"Spatially explicit population models (SEPMs) are often considered the best way to predict and manage species distributions in spatially heterogeneous landscapes. However, they are computationally intensive and require extensive knowledge of species' biology and behavior, limiting their application in many cases. An alternative to SEPMs is graph theory, which has minimal data requirements and efficient algorithms. Although only recently introduced to landscape ecology, graph theory is well suited to ecological applications concerned with connectivity or movement. This paper compares the performance of graph theory to a SEPM in selecting important habitat patches for Wood Thrush (Hylocichla mustelina) conservation. We use both models to identify habitat patches that act as population sources and persistent patches and also use graph theory to identify patches that act as stepping stones for dispersal. Correlations of patch rankings were very high between the two models. In addition, graph theory offers the ability to identify patches that are very important to habitat connectivity and thus long-term population persistence across the landscape. We show that graph theory makes very similar predictions in most cases and in other cases offers insight not available from the SEPM, and we conclude that graph theory is a suitable and possibly preferable alternative to SEPMs for species conservation in heterogeneous landscapes.","
          1771-82
        ",2007.0,https://lopez.net/tag/app/listmain.htm,Medicine
2166,fd6432fa2b032dd5f246d26460bf3353c43c257a,Discrete Signal Processing on Graphs: Sampling Theory,"We propose a sampling theory for signals that are supported on either directed or undirected graphs. The theory follows the same paradigm as classical sampling theory. We show that perfect recovery is possible for graph signals bandlimited under the graph Fourier transform. The sampled signal coefficients form a new graph signal, whose corresponding graph structure preserves the first-order difference of the original graph signal. For general graphs, an optimal sampling operator based on experimentally designed sampling is proposed to guarantee perfect recovery and robustness to noise; for graphs whose graph Fourier transforms are frames with maximal robustness to erasures as well as for Erdös-Rényi graphs, random sampling leads to perfect recovery with high probability. We further establish the connection to the sampling theory of finite discrete-time signal processing and previous work on signal recovery on graphs. To handle full-band graph signals, we propose a graph filter bank based on sampling theory on graphs. Finally, we apply the proposed sampling theory to semi-supervised classification of online blogs and digit images, where we achieve similar or better performance with fewer labeled samples compared to previous work.",6510-6523,2015.0,http://jones.com/categoriesindex.asp,Mathematics
2167,bf187d5d6bf7b18d8ab19b01da796665d622c60c,Some new trends in chemical graph theory.,"Unidad de Investigación de Diseño de Farmacos y Conectividad Molecular, Departamento de Quı́mica Fisica, Facultad de Farmacı́a, Universitat de València, 46100 Burjassot, València, Spain, Instituto de Tecnologia Quimica, CSIC-Universidad Politecnica de Valencia, Av. de los Naranjos s/n, 46022 València, Spain, and Dipartimento di Chimica, Università della Calabria, via P. Bucci 14/C, 87036 Rende (CS), Italy","
          1127-69
        ",2008.0,http://lewis.org/explore/exploreauthor.jsp,Chemistry
2168,88552bf7baa57b884e3e4aa0838f20da921b8cbe,Graph theory and its applications,"In this paper we will discuss how problems like Page ranking and finding the shortest paths can be solved by using Graph Theory. At its core, graph theory is the study of graphs as mathematical structures. In our paper, we will first cover Graph Theory as a broad topic. Then we will move on to Linear Algebra. Linear Algebra is the study of matrices. We will apply the skills discussed in these two sections to Dijkstra Algorithms which cover how to find the shortest paths in graphs. Finally, we will present PageRank where we will demonstrate how to rank pages based on their importance.",54-118,1970.0,https://miller.com/explorepost.html,Computer Science
2169,04ff6d8d8708aef4ebc45ebee132fcc6f055bbd4,Modern temporal network theory: a colloquium,,45-139,2015.0,http://nielsen-morales.com/categories/searchhomepage.htm,Physics
2170,5076d4a29581a986ec08e5c48cac0d6f3b2a1f3c,Landscape connectivity: A conservation application of graph theory,"We use focal-species analysis to apply a graph-theoretic approach to landscape connectivity in the Coastal Plain of North Carolina. In doing so we demonstrate the utility of a mathematical graph as an ecological construct with respect to habitat connectivity. Graph theory is a well established mainstay of information technology and is concerned with highly efficient network flow. It employs fast algorithms and compact data structures that are easily adapted to landscape-level focal species analysis. American mink (Mustela vison) and prothonotary warblers (Protonotaria citrea) share the same habitat but have different dispersal capabilities, and therefore provide interesting comparisons on connections in the landscape. We built graphs using GIS coverages to define habitat patches and determined the functional distance between the patches with least-cost path modeling. Using graph operations concerned with edge and node removal we found that the landscape is fundamentally connected for mink and fundamentally unconnected for prothonotary warblers. The advantage of a graph-theoretic approach over other modeling techniques is that it is a heuristic framework which can be applied with very little data and improved from the initial results. We demonstrate the use of graph theory in a metapopulation context, and suggest that graph theory as applied to conservation biology can provide leverage on applications concerned with landscape connectivity.",265-278,2000.0,http://www.valencia-mendoza.net/categoriesabout.htm,Physics
2171,125bb9e2f465f3fec6646ef286edc6d41074ca50,On Graph Theory and Its Application,"Graph theory has around 300 years of history,but many problems haven't been solved.With the development of computer science,graph theory becomes hot point again.In this paper,the application of graph theory is discussed.",64-105,2007.0,https://sanchez.info/tags/postscategory.html,Mathematics
2172,f253627bfa7485c0e6e5e8f53a682d7149f0f790,Characterizing brain anatomical connections using diffusion weighted MRI and graph theory,,645-660,2007.0,http://luna-hall.info/mainterms.html,Medicine
2173,47199ebc578a0c80ecdafe496bd05b44140d6cdc,Extremal Graph Theory for Metric Dimension and Diameter,"A set of vertices $S$ resolves a connected graph $G$ if every vertex is uniquely determined by its vector of distances to the vertices in $S$. The metric dimension of $G$ is the minimum cardinality of a resolving set of $G$. Let ${\cal G}_{\beta,D}$ be the set of graphs with metric dimension $\beta$ and diameter $D$. It is well-known that the minimum order of a graph in ${\cal G}_{\beta,D}$ is exactly $\beta+D$. The first contribution of this paper is to characterise the graphs in ${\cal G}_{\beta,D}$ with order $\beta+D$ for all values of $\beta$ and $D$. Such a characterisation was previously only known for $D\leq2$ or $\beta\leq1$. The second contribution is to determine the maximum order of a graph in ${\cal G}_{\beta,D}$ for all values of $D$ and $\beta$. Only a weak upper bound was previously known.",77-103,2007.0,http://www.rodriguez.org/list/appprivacy.php,Computer Science
2174,04eb71ccc65edf1c2b2f3b43bdb89a8e90fe8a8b,PEARLS in GRAPH THEORY,,21-135,2007.0,http://www.shah-taylor.com/bloglogin.asp,Mathematics
2175,de6f9eb6762046bde56edf788be65f1858e33396,Fast Approximate Quadratic Programming for Graph Matching,"Quadratic assignment problems arise in a wide variety of domains, spanning operations research, graph theory, computer vision, and neuroscience, to name a few. The graph matching problem is a special case of the quadratic assignment problem, and graph matching is increasingly important as graph-valued data is becoming more prominent. With the aim of efficiently and accurately matching the large graphs common in big data, we present our graph matching algorithm, the Fast Approximate Quadratic assignment algorithm. We empirically demonstrate that our algorithm is faster and achieves a lower objective value on over 80% of the QAPLIB benchmark library, compared with the previous state-of-the-art. Applying our algorithm to our motivating example, matching C. elegans connectomes (brain-graphs), we find that it efficiently achieves performance.",20-132,2015.0,http://www.garrison.net/wp-content/mainindex.jsp,Computer Science
2176,52e2d4299190d1391ca92794e9e708b5c4f6463c,Graph Theory,,69-146,1997.0,http://johnson-johnson.net/category/main/postshomepage.php,Computer Science
2177,ced040290fa21c51c43a5be0eb2bbd5f2e59d54b,Implementing discrete mathematics - combinatorics and graph theory with Mathematica,"Permutations and Combinations Permutations Permutation Groups Inversions and Inversion Vectors Special Classes of Permutations Combinations Exercises and Research Problems * Partitions, Compositions, and Young Tableaux Partitions Compositions Young Tableaux Exercises and Research Problems * Representing Graphs Data Structures for Graphs Elementary Graph Operations Graph Embeddings Storage Formats Exercises and Research Problems * Generating Graphs Regular Structures Trees Random Graphs Relations and Functional Graphs Exercises and Research Problems * Properties of Graphs Connectivity Graph Isomorphism Cycles in Graphs Partial Orders Graph Coloring Cliques, Vertex Covers, and Independent Sets Exercises and Research Problems * Algorithmic Graph Theory Shortest Paths Minimum Spanning Trees Network Flow Matching Planar Graphs Exercises and Research Problems",I-VIII,1990.0,http://www.wilkerson.com/app/postspost.html,Mathematics
2178,eee1a2253d1d90cdb69b5666c59cfeb25498c957,Szemeredi''s Regularity Lemma and its applications in graph theory,"Szemer\''edi''s Regularity Lemma is an important tool in discrete mathematics. It says that, in some sense, all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. Recently quite a few new results were obtained by using the Regularity Lemma, and also some new variants and generalizations appeared. In this survey we describe some typical applications and some generalizations.",37-142,1995.0,http://parks-henry.com/category/search/appprivacy.html,Mathematics
2179,e764da56e8b4ba6bc21a80ec70b34a8a7b69de59,Graph theory and networks in Biology.,"A survey of the use of graph theoretical techniques in Biology is presented. In particular, recent work on identifying and modelling the structure of bio-molecular networks is discussed, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. Work on the link between structural network properties and dynamics is also described, with emphasis on synchronisation and disease propagation.","
          89-119
        ",2006.0,http://www.lin-simpson.org/explorelogin.jsp,Biology
2180,37c1011ac59b0b4685b3dc8a9dda4d8f6e68f6c2,A textbook of graph theory,Basic Results.- Directed Graphs.- Connectivity.- Trees.- Independent Sets and Matchings.- Eulerian and Hamiltonian Graphs.- Graph Colourings.- Planarity.- Triangulated Graphs.- Applications.,65-147,1999.0,https://www.robinson.com/wp-contentpost.jsp,Mathematics
2181,4eb5e8f4ff335fcc7115ab382db847fd3dfb0a14,Introduction to Graph Theory,"Introduction * Definitions and examples* Paths and cycles* Trees* Planarity* Colouring graphs* Matching, marriage and Menger's theorem* Matroids Appendix 1: Algorithms Appendix 2: Table of numbers List of symbols Bibliography Solutions to selected exercises Index",90-118,1972.0,https://www.romero-acosta.com/search/tag/listindex.htm,Mathematics
2182,f6433be2ca69c760d51e9e68f7b2859e50422985,A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",84-125,2006.0,http://warren.com/tag/app/tagcategory.html,Mathematics
2183,d141ad437d2b932d647cf16b81b8531645a4cd6c,Applied and algorithmic graph theory,"Designed as the bridge to cross the widening gap between mathematics and computer science, and planned as the mathematical base for computer science students, this maths text is written for upper-level college students who have had previous coursework involving proofs and proof techniques. The close tie between the theoretical and algorithmic aspects of graph theory, and graphs that lend themselves naturally as models in computer science, results in a need for efficient algorithims to solve any large scale problems. Each algorithm in the text includes explanatory statements that clarify individual steps, a worst-case complexity analysis, and algorithmic correctness proofs. As a result, the student will develop an understanding of the concept of an efficient algorithm.",85-143,1992.0,http://www.cruz-dennis.org/tagcategory.html,Computer Science
2184,b7bf5e535ea38a781268b4580265a96cfc7da7c2,A LIMIT THEOREM IN GRAPH THEORY,"In this paper G(n ; I) will denote a graph of n vertices and l edges, K„ will denote the complete graph of p vertices G (p ; (PA and K,(p i , . . ., p,) will denote the rchromatic graph with p i vertices of the i-th colour, in which every two vertices of different colour are adjacent . 7r(G) will denote the number of vertices of G and v(G) denotes the number of edges of G . G(n :1) denotes the complementary graph of G(n : l) i . e. G(n ; 1) is the G (ii : (211) -/) which has the samevertices as G(n ; 1)",44-108,1966.0,http://church-sanford.biz/exploreprivacy.html,Technology
2185,afb562a7910b7c7be22fc0638d2d4951a5ff7b36,Topics in algebraic graph theory,Foreword Peter J. Cameron Introduction 1. Eigenvalues of graphs Michael Doob 2. Graphs and matrices Richard A. Brualdi and Bryan L. Shader 3. Spectral graph theory Dragos Cvetkovic and Peter Rowlinson 4. Graph Laplacians Bojan Mohar 5. Automorphism groups Peter J. Cameron 6. Cayley graphs Brian Alspach 7. Finite symmetric graphs Cheryle E. Praeger 8. Strongly regular graphs Peter J. Cameron 9. Distance-transitive graphs Arjeh M. Cohen 10. Computing with graphs and groups Leonard H. Soicher.,35-138,2004.0,https://zhang.com/tagsterms.html,Mathematics
2186,0b8a18c78e7493fe145382d98fc46f6b427a093f,"Graph Theory: Modeling, Applications, and Algorithms","Preface 1 Introduction to Graph Theory 2 Basic Concepts in Graph Theory 3 TreesandForests 4 Spanning Trees 5 Fundamental Properties of Graphs and Digraphs 6 Connectivity and Flow 7 Planar Graphs 8 Graph Coloring 9 Coloring Enumerations and Chordal Graphs 10 Independence,Dominance, and Matchings 11 Cover Parameters and MatchingPolynomials 12 GraphCounting 13 Graph Algorithms APPENDICES A Greek Alphabet B Notation C Top Ten Online References Index ix",83-131,2006.0,http://schmidt-herrera.com/tags/explore/tagauthor.php,Mathematics
2187,f6ab7318774c16680681d0b764cefe51a31bd49c,A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",31-146,2006.0,http://www.cox-williams.com/tag/wp-contentindex.asp,Mathematics
2188,5dde7192c840ae6f348008bd0fa0a652306da145,GRAPH THEORY AND PROBABILITY,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(lz) are (3)",18-129,1957.0,https://www.ferguson.org/category/tags/apppost.html,Technology
2189,9a5a7a1b13f51af9369ef4ddeb75bb8b5ff87e70,Graph Theory: An Introductory Course,"From the reviews: ""Bla Bollob's introductory course on graph theory deserves to be considered as a watershed in the development of this theory as a serious academic subject...The book has chapters on electrical networks, flows, connectivity and matchings, extremal problems, colouring, Ramsey theory, random graphs, and graphs and groups. Each chapter starts at a measured and gentle pace. Classical results are proved and new insight is provided, with the examples at the end of each chapter fully supplementing the text...Even so this allows an introduction not only to some of the deeper results but, more vitally, provides outlines of, and firm insights into, their proofs. Thus in an elementary text book, we gain an overall understanding of well-known standard results, and yet at the same time constant hints of, and guidelines into, the higher levels of the subject. It is this aspect of the book which should guarantee it a permanent place in the literature.""",217 - 217,1980.0,https://perez.com/categorieshome.htm,Computer Science
2190,c8a9622e6cde873c680d1888e957837553672e89,Applications of graph theory in chemistry,"Graph theoretical (GT) applications in chemistry underwent a dramatic revival lately. Constitutional (molecular) graphs have points (vertices) representing atoms and lines (edges) symbolizing malent bonds. This review deals with definition. enumeration. and systematic coding or nomenclature of constitutional or steric isomers, valence isomers (especially of annulenes). and condensed polycyclic aromatic hydrocarbons. A few key applications of graph theory in theoretical chemistry are pointed out. The complete set of all poasible monocyclic aromatic and heteroaromatic compounds may be explored by a mmbination of Pauli's principle, P6lya's theorem. and electronegativities. Topological indica and some of their applications are reviewed. Reaction graphs and synthon graphs differ from constitutional graphs i n their meaning of vertices and edges and find other kinds of chemical applications. This paper ends with a review of the use of GT applications for chemical nomenclature (nodal nomenclature and related areas), coding. and information processing/storage/retrieval",334-343,1985.0,http://www.barber.biz/apppost.htm,Mathematics
2191,67fbb295f46f4d630fd76f6bee13f0b985216bce,PROTEIN STRUCTURE: INSIGHTS FROM GRAPH THEORY,"The sequence and structure of a large body of proteins are becoming increasingly available. It is desirable to explore mathematical tools for ecient extraction of information from such sources. The principles of graph theory, which was earlier applied in elds such as electrical engineering and computer networks are now being adopted to investigate protein structure, folding, stability, function and dynamics. This review deals with a brief account of relevant graphs and graph theoretic concepts. The concepts of protein graph construction are discussed. The manner in which graphs are analyzed and parameters relevant to protein structure are extracted, are explained. The structural and biological information derived from protein structures using these methods is presented.",187-211,2002.0,http://webb.biz/listpost.html,Computer Science
2192,bfb523f12a2d3b7b3cc4fa11effd4de0a7ff998f,Graph Theory Methods for the Analysis of Neural Connectivity Patterns,,171-185,2003.0,https://www.gardner-davis.com/main/appfaq.php,Computer Science
2193,84ba27f5ed15a9751aaac63d0d5068034351da0a,SPECTRAL GRAPH THEORY AND THE INVERSE EIGENVALUE PROBLEM OF A GRAPH,"Spectral Graph Theory is the study of the spectra of certain matrices defined from a given graph, including the adjacency matrix, the Laplacian matrix and other related matrices. Graph spectra have been studied extensively for more than fifty years. In the last fifteen years, interest has developed in the study of generalized Laplacian matrices of a graph, that is, real symmetric matrices with negative off-diagonal entries in the positions described by the edges of the graph (and zero in every other off-diagonal position). The set of all real symmetric matrices having nonzero off-diagonal entries exactly where the graph G has edges is denoted by S(G). Given a graph G, the problem of characterizing the possible spectra of B, such that B ∈S (G), has been referred to as the Inverse Eigenvalue Problem of a Graph .I n the last fifteen years a number of papers on this problem have appeared, primarily concerning trees. The adjacency matrix and Laplacian matrix of G and their normalized forms are all in S(G). Recent work on generalized Laplacians and Colin de Verdiere matrices is bringing the two areas closer together. This paper surveys results in Spectral Graph Theory and the Inverse Eigenvalue Problem of a Graph, examines the connections between these problems, and presents some new results on construction of a matrix of minimum rank for a given graph having a special form such as a 0,1-matrix or a generalized Laplacian.",12-31,2005.0,http://www.villarreal.com/main/explore/mainsearch.jsp,Mathematics
2194,7d0d69eec235daf374e5a72a0831346e27fd1cae,Application of graph theory to OO software engineering,"Graph Theory, which studies the properties of graphs, has been widely accepted as a core subject in the knowledge of computer scientists. So is Object-Oriented (OO) software engineering, which deals with the analysis, design and implementation of systems employing classes as modules. The latter field can greatly benefit from the application of Graph Theory, since the main mode of representation, namely the class diagram, is essentially a directed graph. The study of graph properties can be valuable in many ways for understanding the characteristics of the underlying software systems. Representative examples for the usefulness of graph theory on OO systems based on recent research results are presented in this paper.",29-36,2006.0,http://www.humphrey-taylor.net/categories/postssearch.htm,Computer Science
2195,0a99f890d1190443b0c8c02062363780d64dd907,Complex Networks: from Graph Theory to Biology,,235-262,2006.0,https://martinez.org/search/tags/categoryabout.htm,Mathematics
2196,6a4e2850ac32e258fcf23c2d7d8cf1b42fccae15,SOME UNSOLVED PROBLEMS IN GRAPH THEORY,CONTENTSIntroduction § 1. Fundamental concepts § 2. Isomorphism problems § 3. Metric questions § 4. Thickness and genus of graph § 5. Colouring problems § 6. Parts with given propertiesReferences,125-141,1968.0,https://www.hawkins.com/tags/postspost.jsp,Mathematics
2197,a0894388f6dfc841c68d2458a390d924a9c0db0f,A graph theory interpretation of nodal regions,,29-42,1961.0,https://www.bond-sawyer.com/categorieshomepage.html,Mathematics
2198,c091d3b62283cecb205f5e4af63e677166bf281f,Dynamic Modelling of Mechatronic Multibody Systems With Symbolic Computing and Linear Graph Theory,"The application of linear graph theory to the modelling of flexible multibody systems is described. When combined with symbolic computing methods, linear graph theory leads to efficient dynamic models that facilitate real-time simulation of systems of rigid bodies and flexible beams. The natural extension of linear graphs to the modelling of mechatronic multibody systems is presented, along with a recently-developed theory for building complex system models from models of individual subsystems.",1 - 23,2004.0,http://patterson-turner.com/explore/postsmain.html,Computer Science
2199,92a46efd598cd478bccddacd8703a254bfc885b0,Model structure analysis through graph theory: partition heuristics and feedback structure decomposition,"The argument of this article is that it is possible to focus on the structural complexity of system dynamics models to design a partition strategy that maximizes the test points between the model and the real world, and a calibration sequence that permits an incremental development of model confidence. It further argues that graph theory could be used as a basis for making sense of the structural complexity of system dynamics models, and that this structure could be used as a basis for more formal analysis of dynamic complexity. After reviewing the graph representation of system structure, the article presents the rationale and algorithms for model partitions based on data availability and structural characteristics. Special attention is given to the decomposition of cycle partitions that contain all the model’s feedback loops, and a unique and granular representation of feedback complexity is derived. The article concludes by identifying future research avenues in this arena. Copyright © 2004 John Wiley & Sons, Ltd.",313-336,2004.0,https://wyatt-moore.com/blog/tagabout.php,Computer Science
2200,f2e614adc8a6139cc1f5c725365bd132df4ed0a7,SOME RECENT RESULTS ON EXTREMAL PROBLEMS IN GRAPH THEORY (Results),"Three years ago I gave a talk on extremal problems in graph theory at Smolenice [2]. I will refer to this paper as I. I will only discuss results which have been found since the publication of I. ~(9) will denote the number of vertices, V(g) the number of edges of 9. s(a ; I) will denote a graph of n vertices and I edges. The vertices of 3 will be denoted by letters x, y, . . . 9(x,, . . . . xk) will denote the subgraph of 9 spanned by the vertices x1, . . . . xA. u(x), the valence of X, denotes the number of edges incident to x. x(S) will denote the",92-113,2002.0,https://olson.com/explore/tagsfaq.html,Technology
2201,8a18f6b55267c844060652ed38ffe49bfef9be88,The Foundations of Topological Graph Theory,,61-119,1995.0,http://moreno.com/posts/explore/categorieshome.html,Mathematics
2202,da4ab623477d041c1944a66808cf38740e913a3f,Combinatorics and graph theory,,57-129,2000.0,https://nelson.com/tag/list/categorieshomepage.html,Mathematics
2203,ffeab4ba1bc96bf7a5c10d9c7b4b33b8855fde2b,Graph theory and sparse matrix computation,,98-113,1993.0,https://wilson-stanton.info/posts/tag/tagspost.html,Mathematics
2204,c637b74814bd5049323bb6bbf077875a3282e02b,Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity,"Concepts taken from graph theory and other branches of topology have been used by many sociologists and social psychologists, in particular Kurt Lewin and J. L. Moreno. Similar ideas have been used to construct statistical models of nervous systems, and these have been applied by J. S. Coleman and others to the spread of information and other social phenomena. The study of social networks by anthropologists has been based, knowingly or unknowingly, on the basic notions of graph theory, as has the identification and analysis of social cliques. There is little consensus among mathematicians about terminology, and social scientists have drawn fortuitously on various mathematical vocabularies as well as inventing their own technical terms. Applied to social networks, the words `connectedness' and `connectivity' may refer to properties of the distance between persons, the number of paths between them, whether there is a path at all, or the proportion of possible paths actually in existence. These different usages are contrasted by restating them all in the terminology set out in Structural models (1965) by Harary, Norman and Cartwright.",215 - 232,1969.0,https://olson.com/tags/categoriesabout.html,Sociology
2205,142ea3e53037cb8d72849380dab8dcd1a6bb910c,Some applications of graph theory to clustering,,283-309,1974.0,https://www.nichols.com/tags/tags/appregister.php,Mathematics
2206,73799d9f998feb583c9e2a0ea42f419952a41f04,On the ground states of the frustration model of a spin glass by a matching method of graph theory,"The ground states of a quenched random Ising spin system with variable concentration of mixed nearest-neighbour exchange couplings +or-J on a square lattice (frustration model) are studied by a new method of graph theory. The search for ground states is mapped into the problem of perfect matching of minimum weight in the graph of frustrated plaquettes, a problem which can be solved by the algorithm of Edmonds. A pedestrian presentation of this elaborated algorithm is given with a discussion of the condition of validity.",2553-2576,1980.0,https://austin.biz/appcategory.htm,Mathematics
2207,3f558ea2e9a0aab2e247b3a67e3e6e343398056a,Reflections on graph theory,"At the occasion of the 250th anniversary of graph theory, we recall some of the basic results and unsolved problems, some of the attractive and surprising methods and results, and some possible future directions in graph theory.",309-324,1986.0,https://barnes-nelson.info/postscategory.asp,Mathematics
2208,8ea2e1fc1fe8367627d868980a9f6bc4ca88d57a,On the use of linear graph theory in multibody system dynamics,,73-90,1996.0,https://www.parker.com/tags/category/categoryfaq.htm,Mathematics
2209,d0eb57dc62688054d62f60117394288b5b9ef11f,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",494-497,1995.0,https://pope.org/wp-content/tag/exploreindex.php,Mathematics
2210,689d1028008a1e14f437c4f80693afbcebe81084,Neural Network-Graph Theory Approach to the Prediction of the Physical Properties of Organic Compounds,A new computational scheme is developed to predict physical properties of organic compounds on the basis of their molecular structure. The method uses graph theory to encode the structural information which is the numerical input for a neutral network. Calculated results for a series of saturated hydrocarbons demonstrate average accuracies of 1--2% with maximum deviations of 12--14%.,832-839,1994.0,http://hunter.com/main/tagcategory.jsp,Mathematics
2211,7d6094c924f5e0d8a10bfa78ff43b97d286e2b2b,Landscape graphs: Ecological modeling with graph theory to detect configurations common to diverse landscapes,,239-255,1993.0,https://www.ritter.biz/app/explorelogin.jsp,Computer Science
2212,1b62915056c5b768db22b47684eea15bce0aa450,"Topology optimization of trusses using genetic algorithm, force method and graph theory","In this article size/topology optimization of trusses is performed using a genetic algorithm (GA), the force method and some concepts of graph theory. One of the main difficulties with optimization with a GA is that the parameters involved are not completely known and the number of operations needed is often quite high. Application of some concepts of the force method, together with theory of graphs, make the generation of a suitable initial population well‐matched with critical paths for the transformation of internal forces feasible. In the process of optimization generated topologically unstable trusses are identified without any matrix manipulation and highly penalized. Identifying a suitable range for the cross‐section of each member for the ground structure in the list of profiles, the length of the substrings representing the cross‐sectional design variables are reduced. Using a contraction algorithm, the length of the strings is further reduced and a GA is performed in a smaller domain of design space. The above process is accompanied by efficient methods for selection, and by using a suitable penalty function in order to reduce the number of numerical operations and to increase the speed of the optimization toward a global optimum. The efficiency of the present method is illustrated using some examples, and compared to those of previous studies. Copyright © 2003 John Wiley & Sons, Ltd.",59-112,2003.0,http://www.keller.com/tag/posts/listsearch.html,Mathematics
2213,ef83866e2aa9e807bb4d3f07456fd9ec33b79f8b,Graph Theory 1736-1936,1. Oaths 2. Circuits 3. Trees 4. Chemical graphs 5. Euler's polyhedral formula 6. The four-colour problem - early history 7. Colouring maps on surfaces 8. Ideas from algebra and topology 9. The four-colour problem - to 1936 10. The factorization of graphs Appendix 1: Graph theory since 1936 Appendix 2: Bibliographical notes Appendix 3: Bibliography: 1736-1936,70-121,1976.0,http://baxter-cooley.com/categories/categories/searchauthor.html,Mathematics
2214,c6fe0d808c37848ce661400a6cba5adee07304ac,Dynamics of Flexible Multibody Systems Using Virtual Work and Linear Graph Theory,,355-381,2000.0,https://allen.info/search/blogterms.htm,Mathematics
2215,686b3b9e3cb8d757c73a5a1813890c214a8a809e,Some Basic Definitions in Graph Theory,"A systematic list of definitions of some basic concepts in graph theory of application to physics is presented. An index, some illustrative theorems, and a brief bibliography are included.",271-288,1970.0,http://www.dixon-taylor.com/appfaq.htm,Physics
2216,d326d75238945047fcaa7401eeb87cb1b400ecd0,Power transfer allocation for open access using graph theory-fundamentals and applications in systems without loopflow,"In this paper, graph theory is used to calculate the contributions of individual generators and loads to line flows and the real power transfer between individual generators and loads that are significant to transmission open access. Related lemmas are proved which present necessary conditions required by the method. Based on AC load flow solution a novel method is suggested which can decide downstream and upstream power flow tracing paths very fast and can calculate the contribution factors of generations and loads to the line flows efficiently. The power transfer between generators and loads can also be determined. The suggested method is suitable for both active and reactive power tracings of real power systems.",923-929,2000.0,https://www.roberts.com/searchindex.html,Computer Science
2217,278fde8c3c7d0ee96f0e562eba1c9cbb15428532,Graph theory and molecular orbitals. XII. Acyclic polyenes,"A graph‐theoretical study of acyclic polyenes is carried out with an emphasis on the influence of branching on several molecular properties. A definition of branching is given and several branching indices are analyzed. The case of polyenes without a Kekule structure is discussed briefly. The main conclusions are: (a) thermodynamic stability of conjugated polyenes decreases with branching, but (b) reactivity, in general, increases with branching.",3399-3405,1975.0,http://elliott-kidd.biz/posts/searchlogin.php,Chemistry
2218,7f7227a16c26b6f90693a2ec8ae2270e272eb548,Metric graph theory and geometry: a survey,"The article surveys structural characterizations of several graph classes defined by distance properties, which have in part a general algebraic flavor and can be interpreted as subdirect decomposition. The graphs we feature in the first place are the median graphs and their various kinds of generalizations, e.g., weakly modular graphs, or fiber-complemented graphs, or l1-graphs. Several kinds of l1-graphs admit natural geometric realizations as polyhedral complexes. Particular instances of these graphs also occur in other geometric contexts, for example, as dual polar graphs, basis graphs of (even ∆-)matroids, tope graphs, lopsided sets, or plane graphs with vertex degrees and face sizes bounded from below. Several other classes of graphs, e.g., Helly graphs (as injective objects), or bridged graphs (generalizing chordal graphs), or tree-like graphs such as distance-hereditary graphs occur in the investigation of graphs satisfying some basic properties of the distance function, such as the Helly property for balls, or the convexity of balls or of the neighborhoods of convex sets, etc. Operators between graphs or complexes relate some of the graph classes reported in this survey.",79-147,2006.0,https://harmon.com/categories/category/appprivacy.jsp,Technology
2219,0ddc015d3ff0a4247eca90a97cae64970275a19b,A method in graph theory,,111-135,1976.0,http://www.allen-graham.biz/blogmain.jsp,Computer Science
2220,13ec4520f33b8724d6dc2b66da1302ffeb4231b5,Graph Theory Techniques in Model-Based Testing,"Models are a method of representing software behavior. Graph theory is an area of mathematics that can help us use this model information to test applications in many different ways. This paper describes several graph theory techniques, where they came from, and how they can be used to improve software testing.",98-118,1999.0,http://warner.com/wp-content/listauthor.htm,Computer Science
2221,6f18387ee1b3533126d5d1e0dccc1fa1c81c8e17,Lattice Constant Systems and Graph Theory,"The two principal systems of lattice constants that have arisen in the study of cooperative phenomena and related problems on crystal lattices are the strong (low‐temperature) and the weak (high‐temperature) systems. The two systems are defined in terms of the concepts of graph theory, and a general theorem relevant to cluster expansions is stated. The interrelation of the two systems is studied and exploited to derive configurational data for the face‐centered cubic lattice. All star graphs with up to seven points (vertices) or nine lines (edges) that are embeddable on the face‐centered cubic are described. A general classification of stars with cyclomatic index 3 is given.",1557-1572,1966.0,http://www.bennett.biz/tag/category/categoryhomepage.asp,Mathematics
2222,b879e3124a79513ac0cf86979fc1a33fbbeb4875,Hybrid Graph Theory and Network Analysis,"This book combines traditional graph theory with the matroid view of graphs in order to throw light on the mathematical approach to network analysis. The authors examine in detail two dual structures associated with a graph, namely circuits and cutsets. These are strongly dependent on one another and together constitute a third, hybrid, vertex-independent structure called a graphoid, whose study is here termed hybrid graph theory. This approach has particular relevance for network analysis. The first account of the subject in book form, the text includes many new results as well as the synthesizing and reworking of much research done over the past thirty years (historically, the study of hybrid aspects of graphs owes much to the foundational work of Japanese researchers). This work will be regarded as the definitive account of the subject, suitable for all working in theoretical network analysis: mathematicians, computer scientists or electrical engineers.",76-149,1999.0,https://www.collier-kennedy.com/category/categories/categoryauthor.php,Computer Science
2223,342a3f86045d99b1d83aec36b3c68611d945c227,Graph Connections: Relationships between Graph Theory and Other Areas of Mathematics,"The purpose of this book is to inform mathematicians about the applicability of graph theory to other areas of mathematics, from number theory, to linear algebra, knots, neural networks, and finance. This is achieved through a series of expository chapters, each devoted to a different field and written by an expert in that field. The book, however, is more than a collection of essays. Each chapter has been carefully edited to ensure a common level of exposition, with terminology and notation standarised as far as possible.",30-128,1997.0,http://www.calderon.com/posts/explore/mainauthor.html,Mathematics
2224,1098c6bad469b388be83566bfe0445b47ba4256f,Open problems of Paul Erdös in graph theory,"The main treasure that Paul Erdős has left us is his collection of problems, most of which are still open today. These problems are seeds that Paul sowed and watered by giving numerous talks at meetings big and small, near and far. In the past, his problems have spawned many areas in graph theory and beyond (e.g., in number theory, probability, geometry, algorithms and complexity theory). Solutions or partial solutions to Erdős problems usually lead to further questions, often in new directions. These problems provide inspiration and serve as a common focus for all graph theorists. Through the problems, the legacy of Paul Erdős continues (particularly if solving one of these problems results in creating three new problems, for example.) There is a huge literature of almost 1500 papers written by Erdős and his (more than 460) collaborators. Paul wrote many problem papers, some of which appeared in various (really hard-to-find) proceedings. Here is an attempt to collect and organize these problems in the area of graph theory. The list here is by no means complete or exhaustive. Our goal is to state the problems, locate the sources, and provide the references related to these problems. We will include the earliest and latest known references without covering the entire history of the problems because of space limitations (The most up-to-date list of Erdős' papers can be found in [65]; an electronic file is maintained by Jerry Grossman at grossman@oakland.edu.) There are many survey papers on the impact of Paul's work, e.g., see those in the books: A Tribute to Paul Erdős [84], Combinatorics, Paul Erdős is Eighty, Volumes 1 and 2 [83], and The Mathematics of Paul Erdős, Volumes I and II [81]. To honestly follow the unique style of Paul Erdős, we will mention the fact that Erdős often offered monetary awards for solutions to a number of his favorite problems. In November 1996, a committee of Erdős' friends decided no more such awards will be given in Erdős' name. However, the author, with the help of Ron Graham, will honor future claims on the problems in this paper, provided the requirements previously set by Paul are satisfied (e.g., proofs have been verified and published in recognized journals). Throughout this paper, the constants c, c′, c1, c2, · · · and extremal functions f(n), f(n, k), f(n, k, r, t), g(n), · · · are used extensively, although within the context of each problem, the",3-36,1997.0,https://davis-chen.org/wp-content/explore/blogcategory.html,Computer Science
2225,682c67d11c3bc884ca6d551b546c8b1c6414c9b3,On domination related concepts in Graph Theory,,308-320,1981.0,https://www.williams.com/posts/explore/postscategory.php,Mathematics
2226,a3c7bcf8e2090de45281cf160e5893766af27100,Examples and Counterexamples in Graph Theory,"It sounds good when knowing the examples and counterexamples in graph theory in this website. This is one of the books that many people looking for. In the past, many people ask about this book as their favourite book to read and collect. And now, we present hat you need quickly. It seems to be so happy to offer you this famous book. It will not become a unity of the way for you to get amazing benefits at all. But, it will serve something that will let you get the best time and moment to spend for reading the book.",274,1978.0,https://www.peterson.com/maincategory.php,Computer Science
2227,2880478c793e500c42f4a0cf06d5036f0061c0bf,Conference on Graph Theory and Topology in Chemistry.,"Abstract : Prof. R. B. King and Dr. D. H. Rouvray organized an International Conference on Graph Theory and Topology in Chemistry which was held at the University of Georgia, Athens, Georgia, during the period March 15-20, 1987. A volume containing the papers presented at this conference is being published by Elsevier Scientific Publishing Company, Amsterdam, and will appear around the end of 1987. The following items are attached: (1) The program of the conference. (2) The short abstracts of the paper presented at the conference. (3) The contents of the conference volume. (4) The preface of the conference volume. Knots, Marcromolecules and Chemical Dynamics Topological Stereochemistry: Knot Theory of Molecular Graphs Extrinsic Topological Chirality Indices of Molecular Graphs A Topological Approach to the Stereochemistry of Nonrigid Molecules Chirality of Non-Standardly Embedded Mobius Ladders .",30-124,1987.0,https://owens.com/categoryfaq.asp,Computer Science
2228,b13a8bd57def8a98f4e59fced4cce361875aa719,On constructing a block layout by graph theory,"This paper examines the problem of developing block layouts using graph theory. It is shown that there are several limitations associated with such layouts, particularly when facility relationships are represented quantitatively by a from-to chart. A modification to conventional construction-type layout procedures is presented which allows a graph theoretic block layout to be developed, regardless of the type of facility relationships used, and without performing all the steps required in the graph theoretic approach. This new method helps to avoid the limitations of the approach and alleviate its computational burden.",1263-1278,1991.0,http://christian-lowery.com/wp-content/explorelogin.html,Mathematics
2229,42eaf1aea33b9da962f32bf5e1a81fc4c60f0e40,A SEMINAR ON GRAPH THEORY,"Abstract : The opening six chapters present a coherent body of graph theoretic concepts. The remaining eight chapters report lectures presented by various seminar participants. Topics presented include: Complete Bipartite Graphs, Extremal Problems in Graph Theory, Applications of Probabilistic Methods to Graph Theory, The Minimal Regular Graph Containing a Given Graph, Various Proofs of Cayley's Formula for Counting Trees, On Well-Quasi-Ordering Trees, Universal Graphs and Graphs and Composite Games. (Author)",200,1969.0,http://baird.net/mainmain.html,Mathematics
2230,207327c3cb2ce8ef6498d433d58488aece6fbf18,A GRAPH THEORY APPROACH TO DEMOGRAPHIC LOOP ANALYSIS,"A demographic analysis of the life-cycle graph can be used to quantify the separate contributions of different life-history types to the population growth rate. Loop analysis has been proposed (van Groenendael et al. 1994) as the appropriate method for partitioning the elasticity matrix to determine these contributions. However, in the analysis of complex demographic models it is difficult to derive the loops by simple inspection of the life-cycle graph. I show how graph theory can be used to describe a general and systematic procedure for deriving the loops from the structure of the life-cycle graph. I demonstrate that the concept of nullity (from graph theory) can be applied in this context to correctly determine the number of loops for any graph. Using examples from Campanula americana, Dipsacus sylvestris, and Caretta caretta, I illustrate the relationship of the loops to biologically relevant life-history contrasts. This relationship is crucial for the application of loop analysis to life-history evolution for the purpose of partitioning the separate effects on the population growth rate among different life-history components.",2539-2549,1998.0,https://www.brown.com/tagsregister.html,Computer Science
2231,82c93321706b13def2d091a6a30f9d5efb627b38,Pattern vectors from algebraic graph theory,"Graph structures have proven computationally cumbersome for pattern analysis. The reason for this is that, before graphs can be converted to pattern vectors, correspondences must be established between the nodes of structures which are potentially of different size. To overcome this problem, in this paper, we turn to the spectral decomposition of the Laplacian matrix. We show how the elements of the spectral matrix for the Laplacian can be used to construct symmetric polynomials that are permutation invariants. The coefficients of these polynomials can be used as graph features which can be encoded in a vectorial manner. We extend this representation to graphs in which there are unary attributes on the nodes and binary attributes on the edges by using the spectral decomposition of a Hermitian property matrix that can be viewed as a complex analogue of the Laplacian. To embed the graphs in a pattern space, we explore whether the vectors of invariants can be embedded in a low-dimensional space using a number of alternative strategies, including principal components analysis (PCA), multidimensional scaling (MDS), and locality preserving projection (LPP). Experimentally, we demonstrate that the embeddings result in well-defined graph clusters. Our experiments with the spectral representation involve both synthetic and real-world data. The experiments with synthetic data demonstrate that the distances between spectral feature vectors can be used to discriminate between graphs on the basis of their structure. The real-world experiments show that the method can be used to locate clusters of graphs.",1112-1124,2005.0,https://hartman.com/app/searchabout.php,Medicine
2232,188c8889b26451ef6e0262e939373a946965ef62,Application of Graph Theory: Relationship of Eccentric Connectivity Index and Wiener's Index with Anti-inflammatory Activity,"Abstract Graph theory was successfully applied in developing a relationship between chemical structure and biological activity. The relationship of two graph invariants—the eccentric connectivity index and the Wiener's index was investigated with regard to anti-inflammatory activity, for a data set consisting of 76 pyrazole carboxylic acid hydrazide analogues. The values of the eccentric connectivity index and the Wiener's index of each analogue in the data set were computed and active ranges were identified. Subsequently, each analogue was assigned a biological activity that was compared with the anti-inflammatory activity reported as percent reduction in paw swelling. Prediction with an accuracy of ∼90% was obtained using the eccentric connectivity index as compared to 84% in the case of Wiener's index.",259-268,2002.0,http://www.norman-johnson.info/wp-contentlogin.jsp,Mathematics
2233,19ccc67b2442c08baefdb30a772463134ec21d37,Computational Graph Theory,,95-135,2002.0,https://martinez.com/bloghomepage.html,Mathematics
2234,2777556053aed8a2e97055dd82f2ac026319b32c,Applications of combinatorics and graph theory to the biological and social sciences,,65-141,1989.0,https://montoya.biz/categories/postshome.jsp,Mathematics
2235,056c50de715e9a3c2aff98d6f90caf877e9acf71,Some recent results in topological graph theory,,76-108,1974.0,http://ellis.com/categories/blog/blogpost.html,Mathematics
2236,98db9d44877f40ea346844a7708c8d7cb64de3dd,Chemical signed graph theory,"Chemical signed graph theory is presented. Each topological orbital of an N-vertex molecular graph is represented by a vertex-signed graph (VSG) that is generated by assigning a sign, either plus or minus, to the vertices without solving the secular matrix equation. The bonding capacity of each VSG is represented by its corresponding edge-signed graph (ESG) and is quantified by the net sign of the ESG. The resulting 2N–1 configurations of VSGs can be divided into several groups according to the net signs of the corresponding ESGs. Summation of an appropriate set of degenerate VSGs is found to yield the conventional, canonical molecular orbitals. The distribution of the number of VSGs with respect to the net sign is found to be binomial, which can be related to bond percolation in statistical physics. © 1994 John Wiley & Sons, Inc.",639-648,1994.0,https://anderson.com/tag/exploreprivacy.html,Mathematics
2237,2c494605fe60583c0b8f20facc463ec49cb06a0c,Potts model and graph theory,,99-112,1988.0,https://www.johnson.net/posts/exploresearch.htm,Mathematics
2238,9aa530783226dc392f570207e90f32cee8b20b96,AN APPLICATION OF GRAPH THEORY TO ALGEBRA,"[Al, * * * , Ak ] 54-0. The original proof of the theorem [1] was elementary but very complicated. In attempting to simplify this proof, I found a more transparent proof based on the use of graph theory.3 One advantage of this approach is that complicated algebraic definitions can be replaced by much simpler geometric definitions merely by drawing a picture of the appropriate graph. Before stating the graph theoretic theorem which implies Theorem 1, I will give some elementary definitions and lemmas from graph theory.",367-373,1963.0,http://www.meyers.info/category/tags/mainsearch.asp,Computer Science
2239,e47f58edd1af4ee6b236d771b1e863920382c45d,Fractional Graph Theory: A Rational Approach to the Theory of Graphs,General Theory: Hypergraphs. Fractional Matching. Fractional Coloring. Fractional Edge Coloring. Fractional Arboricity and Matroid Methods. Fractional Isomorphism. Fractional Odds and Ends. Appendix. Bibliography. Indexes.,42-142,1997.0,http://www.henderson.com/tagssearch.php,Mathematics
2240,24f1ff4e693c9b362a565047b12659189a3eadbf,Neural networks and graph theory,"The relationships between artificial neural networks and graph theory are considered in detail. The applications of artificial neural networks to many difficult problems of graph theory, especially NP-complete problems, and the applications of graph theory to artificial neural networks are discussed. For example graph theory is used to study the pattern classification problem on the discrete type feedforward neural networks, and the stability analysis of feedback artificial neural networks etc.",1-24,2002.0,https://www.odom.com/wp-content/categories/blogpost.asp,Computer Science
2241,7d0e37b3f932ac8c0dcec37c43e71bbbb0ce2811,Graph theory and molecular orbitals. VII. The role of resonance structures,"The relations between the simplest variants of MO and VB theory are discussed. It is shown that there is a unique principle causing all the cases of congruity between these two theories‐Kekule structures being related to the permutations contained in the molecular graph [Eqs. (6) and (7)]. The class of benzenoid hydrocarbons where both theories are substantially equivalent is rigorously defined using graph theory. A number of topological regularities for these hydrocarbons are proved. Thus, the Dewar‐Longuet‐Higgins equation, the proof of the Ruedenberg's and Pauling's bond orders, the relation between the VB and MO spin and charge density, and Heilbronner's formula are obtained. The limits of validity for all these results are strictly determined.",2700-2706,1974.0,https://www.robinson.biz/search/categories/wp-contentpost.htm,Chemistry
2242,8850c6b96200bc8b92935d16a7685e621214d51c,Facilities Planning with Graph Theory,Basic concepts of Graph Theory are discussed which are relevant to solving problems of locating economic activities within a service or manufacturing facility. The location problem is formulated in terms of Graph Theory knowledge and a solution procedure proposed. An example is provided and finally boundary conditions are elaborated.,242-253,1970.0,http://www.mason-peterson.biz/mainsearch.htm,Mathematics
2243,9be30e1d9f4301afbe46a975c67e8a1481e3d56f,Problems in combinatorics and graph theory,"Three hundred and sixty-nine problems with fully worked solutions for courses in computer science, combinatorics, and graph theory, designed to provide graded practice to students with as little as a high school algebra background. Originally used to prepare Rumanian candidates for participation in the International Mathematical Olympiads, this book includes both simple problems and complex ones, arranged according to subject. It provides various levels of problems, some of which had been previously available only in research journals. All details of the proofs are given in the solutions.",60-139,1985.0,https://www.walker-love.com/explore/mainterms.htm,Computer Science
2244,cd570ac39865e4f297491c8321df914eeaa2ac7d,On some solved and unsolved problems of chemical graph theory,"The development of several novel graph theoretical concepts and their applications in different branches of chemistry are reviewed. After a few introductory remarks we follow with an outline of selected important graph theoretical invariants, introducing some new results and indicating some open problems. We continue with discussing the problem of graph characterization and construction of graphs of chemical interest, with a particular emphasis on large systems. Finally we consider various problems and difficulties associated with special subgraphs, including subgraphs representing Kekule valence structures. The paper ends with a brief review of structure-property and structure-activity correlations, the topic which is one of prime motivations for application of graph theory to chemistry.",699-742,1986.0,https://www.wilson-chen.com/explorefaq.html,Computer Science
2245,bbbff2de136561c6cb4c7890d9f93b357a99e244,Graph Theory and Probability,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) — 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(n) are (3) It is not even known that g(n)1/n tends to a limit. The lower bound in (1) has been obtained by combinatorial and probabilistic arguments without an explicit construction.",34 - 38,1959.0,https://moore.org/list/posts/wp-contentfaq.html,Mathematics
2246,25c6565750d7d46c63d9bc073e6826b4d5898498,Graph Theory and Q-Analysis,"Structures of graph theory are compared with those of Q-analysis and there are many similarities. The graph and simplicial complex defined by a relation are equivalent in terms of the information they represent, so that the choice between graph theory and Q-analysis depends on which gives the most natural and complete description of a system. The higher dimensional graphs are shown to be simplicial families or complexes. Although network theory is very successful in those physical science applications for which it was developed, it is argued that Q-analysis gives a better description of human network systems as patterns of traffic on a backcloth of simplicial complexes. The q-nearness graph represents the q-nearness of pairs of simplices for a given q-value. It is concluded that known results from graph theory could be applied to the q-nearness graph to assist in the investigation of q-connectivity, to introduce the notion of connection defined by graph cuts, and to assist in computation. The application of the q-nearness graph to q-transmission and shomotopy is investigated.",367 - 391,1981.0,https://www.hayes.com/categoriesmain.php,Mathematics
2247,fd3388199757c081f45ec4c02b25cdab84076935,Graph Theory for Rule-Based Modeling of Biochemical Networks,,89-106,2006.0,https://mcdonald-strong.com/tag/tagmain.php,Computer Science
2248,d007e0675a45264aedeacbbe12c731ea78bba070,Problems and Results in Graph Theory and Combinatorial Analysis,"I published several papers with similar titles. One of my latest ones [13] (also see [16] and the yearly meetings at Boca Raton or Baton Rouge) contains, in the introduction, many references to my previous papers . I discuss here as much as possible new problems, and present proofs in only one case. I use the same notation as used in my previous papers . G(' )(n ;1) denotes an r-graph (uniform hypergraph all of whose edges have size r) of n vertices and I edges . If r = 2 and there is no danger of confusion . I omit the upper index r = 2 . K ( r ) (n) denotes the complete hypergraph G ( ' ) (n ; (;)) . K(a, b) denotes the complete bipartite graph (r = 2) of a white and b black vertices . K (r )(t) denotes the hypergraph of It vertices x (i ' ) , I < i < t, 1 < j < 1, and whose (I)tr edges are {x,1t ) , . . . , x~ )} where all the i's and all the j's are distinct . e(G(in)) is the number of edges of G(m) (graph of m vertices), the girth is the length of a smallest circuit of the graph .",50-129,1977.0,https://www.burton.com/categories/categoryfaq.htm,Technology
2249,8691bd56aad7be73d5a6342ee434bea2d4774e00,"ON SOME PROBLEMS IN GRAPH THEORY , COMBINATORIAL ANALYSIS AND COMBINATORIAL NUMBER THEORY","1. G(n) is a graph of n vertices and G(n ; e) is a graph of n vertices and e edges. Is it true that if every induced subgraph of a G(10n) of 5n vertices has more than 2n 2 edges then our G(10n) contains a triangle? It is easy to show that if true this result is best possible . To see this let A i , i =1, 2, . . . , 5, be sets of 2n vertices, put A, = A 6 and join every vertex of A, to every vertex of A; + , . This G(10n ; 20n 2) has of course no triangle and every induced subgraph of 5n vertices contains at least 2n2 edges . Equality is of course possible : choose A,, A, and half the vertices of A, Simonovits pointed out to me that a graph of completely different structure also shows that the conjecture, if true, is best possible . Consider the Petersen graph, which is a G(10 ; 15) . Replace each vertex by a set of n vertices and replace every edge of the Petersen graph by the n 2 edges of a K(n, n) . This gives a G(10n ; 15n 2) and it is easy to see that every induced subgraph of 5n vertices has at least 2n2 edges . The fact that two graphs of different structure are extremal perhaps indicates that the conjecture is either false or difficult to prove . I certainly hope that the latter is the case . It is perhaps tempting to conjecture that my graph has the following extremal property . If a G(10n) has no triangle and every induced subgraph of 5n vertices has at least 2n2 edges, then our graph can have at most 20n2 edges. Perhaps the graph of Simonovits has the smallest number of edges among all extremal graphs; perhaps in fact these two graphs are the only extremal graphs . Many generalizations are possible ; the triangle could be replaced by other graphs . Is it true that every G((4h+2)n), every induced subgraph",31-109,2004.0,https://rose.com/main/blogmain.asp,Technology
2250,0f6d06e5e321698682c31ee07e25d76d03be6e52,A material selection model using graph theory and matrix approach,,248-255,2006.0,http://www.nguyen-webb.com/explore/list/explorehomepage.htm,Materials Science
2251,199f55f80973ff42c0df77495cc639ff6e211fac,Topics in Intersection Graph Theory,"Preface 1. Intersection Graphs. Basic Concepts Intersection Classes Parsimonious Set Representations Clique Graphs Line Graphs Hypergraphs 2. Chordal Graphs. Chordal Graphs as Intersection Graphs Other Characterizations Tree Hypergraphs Some Applications of Chordal Graphs Split Graphs 3. Interval Graphs. Definitions and Characterizations Interval Hypergraphs Proper Interval Graphs Some Applications of Interval Graphs 4. Competition Graphs. Neighborhood Graphs Competition Graphs Interval Competition Graphs Upper Bound Graphs 5. Threshold Graphs. Definitions and Characterizations Threshold Graphs as Intersection Graphs Difference Graphs and Ferrers Digraphs Some Applications of Threshold Graphs 6. Other Kinds of Intersection. p-Intersection Graphs Intersection Multigraphs and Pseudographs Tolerance Intersection Graphs 7. Guide to Related Topics. Assorted Geometric Intersection Graphs Bipartite Intersection Graphs, Intersection Digraphs, and Catch (Di)Graphs Chordal Bipartite and Weakly Chordal Graphs Circle Graphs and Permutation Graphs Clique Graphs of Chordal Graphs and Clique-Helly Graphs Containment, Comparability, Cocomparability, and Asteroidal Triple-Free Graphs Infinite Intersection Graphs Miscellaneous Topics P4-Free Chordal Graphs and Cographs Powers of Intersection Graphs Sphere-of-Influence Graphs Strongly Chordal Graphs Bibliography Index.",76-111,1987.0,http://ortega.com/tags/tagsterms.htm,Mathematics
2252,bccb739463ffd0189a5c6b2f5e6415a9f50fb313,Selected Topics in Graph Theory 2,,150-151,1985.0,http://mitchell.com/categoriesabout.php,Mathematics
2253,0e83b4f43f32727d7aa5cefbd12db8a7ed3f1346,Some Topics in Graph Theory,1. Basic terminology 2. Edge-colourings of graphs 3. Symmetries in graphs 4. Packing of graphs 5. Computational complexity of graph properties.,32-112,1986.0,https://www.spencer.com/categorieshome.html,Mathematics
2254,0a431a6f6816931287bd3b14b48884aa63913b41,Lectures on Spectral Graph Theory,Contents Chapter 1. Eigenvalues and the Laplacian of a graph 1 1.1. Introduction 1 1.2. The Laplacian and eigenvalues 2 1.3. Basic facts about the spectrum of a graph 6,99-150,2001.0,http://www.watson.org/categories/categoryauthor.php,Technology
2255,bc1b7919b0a78f792edcf2cb3abbd082518b45c2,Use of Graph Theory to Support Map Generalization,"In the generalization of a concept, we seek to preserve the essential characteristics and behavior of objects. In map generalization, the appropriate selection and application of procedures (such as merging, exaggeration, and selection) require information at the geometric, attribute, and topological levels. This article highlights the potential of graph theoretic representations in providing the topological information necessary for the efficient and effective application of specific generalization procedures. Besides ease of algebraic manipulation, the principal benefit of a graph theoretic approach is the ability to detect and thus preserve topological characteristics of map objects such as isolation, adjacency, and connectivity. While it is true that topologically based systems have been developed for consistency checking and error detection during editing, this article emphasizes the benefits from a map-generalization perspective. Examples are given with respect to specific generalization procedures ...",210-221,1993.0,http://www.poole.com/searchlogin.php,Mathematics
2256,ed98016990bc4bd91c910070563d7c5ad5f5a38c,A Friendly Introduction to Graph Theory,1. Introductory Concepts. 2. Introduction to Graphs and their Uses. 3. Trees and Bipartite Graphs. 4. Distance and Connectivity. 5. Eularian and Hamiltonian Graphs. 6. Graph Coloring. 7. Matrices. 8. Graph Algorithms. 9. Planar Graphs. 10. Digraphs and Networks. 11. Special Topics. Answers/Solutions to Selected Exercises. Index.,42-129,2002.0,https://www.robinson-burnett.com/list/tagcategory.jsp,Mathematics
2257,21fac6c5fcfdfb51fa60a7f361179d25661c0a46,Graph theory for image analysis: an approach based on the shortest spanning tree,"The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms. An important feature in all of the proposed methods is that regions may be described in a hierarchical way.",65-146,1986.0,https://burns-cobb.com/posts/postssearch.jsp,Mathematics
2258,7ab055436895b106e5a82e910693539b9877d884,Compactness results in extremal graph theory,,275-288,1982.0,http://www.watts.com/searchmain.htm,Mathematics
2259,18261bcc9557f697e795d3aa3a9bb74da54fe58c,"Introduction to Chordal Graphs and Clique Trees, in Graph Theory and Sparse Matrix Computation","Kjjrull U., Triangulation of graph-algorithms giving small total state space. 19 in the number of minimal separators. This possible amendment will resolve a theoretical problem raised by KBMK93, KBMK94] and further addressed by PS95] but will hardly aaect the running time of our algorithm. 18 The entry Frag in Table 3 measures the number of fragments produced for the dynamic programming phase. The entries R and R k measure, respectively, the number of minimal separators and the number of minimal separators of size less than k where k = 7. Note again that R k is smaller than R; Many minimal separators are generated in the rst phase of QuickTree but are not needed for the dynamic programming phase. A second reason for the high values of T 1 is that the algorithm runs over almost all pairs of vertices and for each pair fa; bg produces all minimal a; b-separators. However, after a few pairs, the algorithm usually nds most of the minimal sepa-rators and the remaining run time is just used to verify that indeed all minimal separators have been generated. Table 4 shows the number of Good-Pairs|pairs that generated at least one new minimal separator. All-Pairs denote the number of pairs we used which guarantee that all minimal separators have been generated. For Table 4, 30% of the edges were dropped. Table 4 suggests that if there is no needed guarantee of optimal triangulation, then Phase 1 can be run on a fraction of the possible pairs of vertices and then the dynamic programming phase can be applied. For example, on 3 graphs with 75 nodes and treewidth 7, we selected the top 20% of pairs that had a maximal mutual distance and got close to optimal triangulations (in two instances we got the optimal treewidth 7 and once we got 9 instead of 7). The average running time was reduced from 675 to 373 seconds. 6 Discussion In many applications the treewidth of a triangulation is just an approximation to the real optimization problem. For example, for the updating problem in Bayesian networks, one needs to nd a triangulation that minimizes the sum P i 2 w(Ci) where w is a positive additive weight function on the vertices of G and C i are the cliques of the triangulation Kj90, BG96]. The triangulation algorithm presented herein can be modiied to accommodate such variants. Currently, the algorithm …",98-132,1997.0,https://www.jenkins-evans.com/posts/explore/appprivacy.html,Technology
2260,a372f7ee74933e324b63dc34747ec28201048cb3,Graph theory with applications to algorithms and computer science,"Partial table of contents: Finite Figures Consisting Of Regular Polygons (J. Akiyama, et al.). Eigenvalues, Geometric Expanders and Sorting in Rounds (N. Alon). Long Path Enumeration Algorithms for Timing Verification on Large Digital Systems (T. Asano and S. Sato). On Upsets in Bipartite Tournaments (K. Bagga). Some Results on Binary Matrices Obtained via Bipartite Tournaments (K. Bagga and L. Beineke). Partitioning the Nodes of a Graph (E. Barnes). A Graph Theoretical Characterization of Minimal Deadlocks in Petri Nets (J. Bermond and G. Memmi). On Graceful Directed Graphs that Are Computational Models of Some Algebraic Systems (G. Bloom and D. Hsu). The Cut Frequency Vector (F. Boesch). Diameter Vulnerability in Networks (J. Bond and C. Peyrat). Generalized Colorings of Outerplanar and Planar Graphs (I. Broere and C. Mynhardt). The Ramsey Number for the Pair Complete Bipartite Graph-Graph of Limited Degree (S. Burr, et al.). Embedding Graphs in Books: A Layout Problem with Applications to VLSI Design (F. Chung). Hamilton Cycles and Quotients of Bipartite Graphs (I. Dejter). Problems and Results on Chromatic Numbers in Finite and Infinite Graphs (P. Erdos). Supraconvergence and Functions that Sum to Zero on Cycles (V. Faber and A. White, Jr.). Edge-Disjoint Hamiltonian Cycles (R. Faudree, et al.). Studies Related to the Ramsey Number r(K d5 u - e) (R. Faudree, et al). The Structural Complexity of Flowgraphs (N. Fenton). n-Domination in Graphs (J. Fink and M. Jacobson).",52-124,1985.0,https://choi.org/tagpost.asp,Mathematics
2261,175a476feb8cf13d7f2c739c347de3262f817063,A ring in graph theory,"We call a point set in a complex K a 0-cell if it contains just one point of K, and a 1-cell if it is an open arc. A set L of 0-cells and 1-cells of K is called a linear graph on K if (i) no two members of L intersect, (ii) the union of all the members of L is K, (iii) each end-point of a 1-cell of L is a 0-cell of L and (iv) the number of 0-cells and 1-cells of L is finite and not 0.",26 - 40,1947.0,http://jones-crawford.com/search/tagfaq.html,Mathematics
2262,2c4f5a30c195ef810415b3272a2bfc4af845c96c,On some extremal problems in graph theory,"In this paper we are concerned with various graph invariants (girth, diameter, expansion constants, eigenvalues of the Laplacian, tree number) and their analogs for weighted graphs -- weighing the graph changes a combinatorial problem to one in analysis. We study both weighted and unweighted graphs which are extremal for these invariants. In the unweighted case we concentrate on finding extrema among all (usually) regular graphs with the same number of vertices; we also study the relationships between such graphs.",41-133,1999.0,https://ward.com/wp-contentabout.php,Mathematics
2263,e4f2a719a622ba3b0ed49a82cde9bf07d6ce67f6,Graph Theory and Its Applications,"INTRODUCTION TO GRAPH MODELS Graphs and Digraphs Common Families of Graphs Graph Modeling Applications Walks and Distance Paths, Cycles, and Trees Vertex and Edge Attributes: More Applications STRUCTURE AND REPRESENTATION Graph Isomorphism Revised! Automorphisms and Symmetry Moved and revised! Subgraphs Some Graph Operations Tests for Non-Isomorphism Matrix Representation More Graph Operations TREES Reorganized and revised! Characterizations and Properties of Trees Rooted Trees, Ordered Trees, and Binary Trees Binary-Tree Traversals Binary-Search Trees Huffman Trees and Optimal Prefix Codes Priority Trees Counting Labeled Trees: Prufer Encoding Counting Binary Trees: Catalan Recursion SPANNING TREES Reorganized and revised! Tree-Growing Depth-First and Breadth-First Search Minimum Spanning Trees and Shortest Paths Applications of Depth-First Search Cycles, Edge Cuts, and Spanning Trees Graphs and Vector Spaces Matroids and the Greedy Algorithm CONNECTIVITY Revised! Vertex- and Edge-Connectivity Constructing Reliable Networks Max-Min Duality and Menger's Theorems Block Decompositions OPTIMAL GRAPH TRAVERSALS Eulerian Trails and Tours DeBruijn Sequences and Postman Problems Hamiltonian Paths and Cycles Gray Codes and Traveling Salesman Problems PLANARITY AND KURATOWSKI'S THEOREM Reorganized and revised! Planar Drawings and Some Basic Surfaces Subdivision and Homeomorphism Extending Planar Drawings Kuratowski's Theorem Algebraic Tests for Planarity Planarity Algorithm Crossing Numbers and Thickness DRAWING GRAPHS AND MAPS Reorganized and revised! The Topology of Low Dimensions Higher-Order Surfaces Mathematical Model for Drawing Graphs Regular Maps on a Sphere Imbeddings on Higher-Order Surfaces Geometric Drawings of Graphs New! GRAPH COLORINGS Vertex-Colorings Map-Colorings Edge-Colorings Factorization New! MEASUREMENT AND MAPPINGS New Chapter! Distance in Graphs New! Domination in Graphs New! Bandwidth New! Intersection Graphs New! Linear Graph Mappings Moved and revised! Modeling Network Emulation Moved and revised! ANALYTIC GRAPH THEORY New Chapter! Ramsey Graph Theory New! Extremal Graph Theory New! Random Graphs New! SPECIAL DIGRAPH MODELS Reorganized and revised! Directed Paths and Mutual Reachability Digraphs as Models for Relations Tournaments Project Scheduling and Critical Paths Finding the Strong Components of a Digraph NETWORK FLOWS AND APPLICATIONS Flows and Cuts in Networks Solving the Maximum-Flow Problem Flows and Connectivity Matchings, Transversals, and Vertex Covers GRAPHICAL ENUMERATION Reorganized and revised! Automorphisms of Simple Graphs Graph Colorings and Symmetry Burnside's Lemma Cycle-Index Polynomial of a Permutation Group More Counting, Including Simple Graphs Polya-Burnside Enumeration ALGEBRAIC SPECIFICATION OF GRAPHS Cyclic Voltages Cayley Graphs and Regular Voltages Permutation Voltages Symmetric Graphs and Parallel Architectures Interconnection-Network Performance NON-PLANAR LAYOUTS Reorganized and revised! Representing Imbeddings by Rotations Genus Distribution of a Graph Voltage-Graph Specification of Graph Layouts Non KVL Imbedded Voltage Graphs Heawood Map-Coloring Problem APPENDIX Logic Fundamentals Relations and Functions Some Basic Combinatorics Algebraic Structures Algorithmic Complexity Supplementary Reading BIBLIOGRAPHY General Reading References SOLUTIONS AND HINTS New! INDEXES Index of Applications Index of Algorithms Index of Notations General Index",59-144,1998.0,https://www.velasquez.com/blog/explore/categoriescategory.htm,Mathematics
2264,2808f916c72757f891a101c39a9942fb27f22b78,Extremal problems in graph theory,The aim of this note is to give an account of some recent results and state a number of conjectures concerning extremal properties of graphs.,117-123,1977.0,https://mcclure.com/list/blogterms.htm,Mathematics
2265,d22b5e502389a840a47fd5c6a4fae829e1d8bb8c,Computational Discrete Mathematics: Algorithmic Graph Theory,,44-114,2003.0,http://www.myers.net/search/blogcategory.html,Mathematics
2266,b2cc957d2c913899deea22ec37df9963c7af2b46,"Graph Theory: Flows, Matrices","STRUCTURE OF THE GRAPH MODEL The abstract graph Geometrical realization of graphs Components Leaves Blocks The strongly connected components of directed graphs Problems OPTIMAL FLOWS Two basic problems Maximal set of independent paths The optimal assignment problem The Hungarian method Max flow-min cut Dynamic flow The mobilization problem The synthesis of flow problems Optical planning The role of the critical path Minimal cost transportation Minimal cost flows Problems GRAPHS AND MATRICES The adjacency matrix The incidence matrix The circuit matrix Interrelations between the matrices of graphs The spectrum of graphs, the complexity Linear electrical networks Further matrices associated with graphs Problems and solutions",82-105,1991.0,http://www.king.com/categoriesmain.htm,Mathematics
2267,9aa27be1bbeb898e6daf8c106ccafae6e09856f9,Graph Theory and Probability. II,"Define f(k, l) as the least integer so t h a t every graph having f(k, l) vertices contains either a complete graph of order k or a set of l independent vertices (a complete graph of order k is a graph of k vertices every two of which are connected by an edge, a set of I vertices is called independent if no two are connected by an edge). Throughout this paper c1, c2, … will denote positive absolute constants. It is known (1, 2) that (1) and in a previous paper (3) I stated that I can prove that for every ∈ > 0 and l > l(∈), f (3, l) > l2-∈ . In the present paper I am going to prove that (2)",346 - 352,1961.0,https://www.flores-murray.com/wp-content/posts/wp-contentterms.htm,Technology
2268,7a05bef155bc7a29581cac1c9b2ee9e8ca3dca42,Fundamentals of Graph Theory,,23-41,1986.0,http://moses.net/search/categoriespost.jsp,Computer Science
2269,f3f26c7fd8bca7262b23210bf5318037a27f939d,Parallel computations in graph theory,"In parallel computation two approaches are common; namely unbounded parallelism and bounded parallelism. In this paper both approaches will be considered. The problem of unbounded parallelism is studied in section II and some lower and upper bounds on different connectivity problems for directed and undirected graphs are presented. In section III we mention bounded parallelism and three different k-parallel graph search techniques, namely k-depth search, breadth depth search, and breadth-first search. Each algorithm is analyzed with respect to the optimal serial algorithm. It is shown that for sufficiently dense graphs the parallel breadth first search technique is very close to the optimal bound. Techniques for searching sparse graphs are also discussed.",13-18,1975.0,https://brady.com/tag/appfaq.html,Computer Science
2270,df8f64361e1bd6e976b533f81a50c52ba0e2ed02,An extremal problem in graph theory,"G(n;l) will denote a graph of n vertices and l edges. Let f0(n, k) be the smallest integer such that there is a G (n;f0(n, k)) in which for every set of k vertices there is a vertex joined to each of these. Thus for example fo = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that fo = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, andthen f0(n, k) = f(n, k).",42 - 47,1968.0,https://long.com/categories/app/postshome.php,Mathematics
2271,a38faf02b193d8d121fc9946d5e7c53b13bd9d57,Graph-set analysis of hydrogen-bond patterns in organic crystals.,"A method is presented based on graph theory for categorizing hydrogen-bond motifs in such a way that complex hydrogen-bond patterns can be disentangled, or decoded, systematically and consistently. This method is based on viewing hydrogen-bond patterns topologically as if they were intertwined nets with molecules as the nodes and hydrogen bonds as the lines. Surprisingly, very few parameters are needed to define the hydrogen-bond motifs comprising these networks. The methods for making these assignments, and examples of their chemical utility are given.","
          256-62
        ",1990.0,http://www.sampson.com/posts/posts/searchpost.html,Chemistry
2272,d15c7f4c9eff3f5f1583a4ef3ac1c4fd88caf972,A Beginner's Guide to Graph Theory,"Graphs.- Walks, Paths and Cycles.- Connectivity.- Trees.- Linear Spaces Associated with Graphs.- Factorizations.- Graph Colorings.- Planarity.- Labeling.- Ramsey Theory.- Digraphs.- Critical Paths.- Flows in Networks.- Computational Considerations.- Communications Networks and Small-Worlds.",28-119,2000.0,https://www.patterson-pierce.net/app/taghomepage.html,Mathematics
2273,f8048aa570e607428f8cfbfbcdc2bfa15218df75,Matrices in Combinatorics and Graph Theory,,69-141,2000.0,https://www.ellis.com/blog/tagindex.htm,Mathematics
2274,5549a317454b8314f585a6c13db39664c76686d2,Graph theory and its engineering applications,Basic theory foundations of electrical network theory directed-graph solutions of linear algebraic equations topological analysis of linear systems trees and their generation the realizability of directed graphs with prescribed degrees state equations of networks.,78-111,1997.0,http://www.mitchell.com/posts/posts/explorefaq.html,Mathematics
2275,84b5314ed88bb9fad764b85833fe8038f7f0f57f,Graph Theory and the Evolution of Autocatalytic Networks,"We give a self-contained introduction to the theory of directed graphs, leading up to the relationship between the Perron-Frobenius eigenvectors of a graph and its autocatalytic sets. Then we discuss a particular dynamical system on a fixed but arbitrary graph, that describes the population dynamics of species whose interactions are determined by the graph. The attractors of this dynamical system are described as a function of graph topology. Finally we consider a dynamical system in which the graph of interactions of the species coevolves with the populations of the species. We show that this system exhibits complex dynamics including self-organization of the network by autocatalytic sets, growth of complexity and structure, and collapse of the network followed by recoveries. We argue that a graph theoretic classification of perturbations of the network is helpful in predicting the future impact of a perturbation over short and medium time scales.",355-395,2002.0,http://butler.biz/app/blogregister.html,Physics
2276,349cb135f4f4216c58d000f12bfef8db6390c602,Molecular Orbital Calculations Using Chemical Graph Theory,,70-127,1993.0,https://bridges.com/explore/posts/mainregister.htm,Mathematics
2277,8d2e0a40e065555c75891132def53556d7c958b9,On finite 0-simple semigroups and graph theory,,325-339,1968.0,http://www.baker-sims.com/blog/category/blogsearch.jsp,Computer Science
2278,5c0e88953a818d257805ae5f1fd8ba5a16661ada,Chemical applications of graph theory,,42-133,1976.0,https://www.galvan.net/tagssearch.htm,Technology
2279,ad950d0d65329910e6251168315db07e1cf7a35f,On-Line Coloring and Recursive Graph Theory,"An on-line vertex coloring algorithm receives the vertices ofa graph in some externally determined order, and, whenever a new vertex is presented, the algorithm also learns to which of the previously presented vertices the new vertex is adjacent. As each vertex is received, the algorithm must make an irrevocable choice of a color to assign the new vertex, and it makes this choice without knowledge of future vertices. A class of graphs $r$ is said to be on-line $\chi$-bounded if there exists an on-line algorithm $A$ and a function $f$ such that $A$ uses at most $f(\omega(G))$ colors to properly color any graph $G$ in \Gamma. If $H$ is a graph, let Forb($H$) denote the class of graphs that do not induce $H$. The goal of this paper is to establish that Forb($T$) is on-line $\chi$-bounded for every radius-2 tree $T$. As a corollary, the authors answer a question of Schmerl's; the authors show that every recursive cocomparability graph can be recursively colored with a number of colors that depends only on its clique number.",72-89,1994.0,https://martin.biz/app/list/tagsfaq.html,Mathematics
2280,90fc44ddfc0a4e43e317699e16c5123429b58cf6,Algorithmic graph theory and perfect graphs,,16-148,1980.0,http://www.spears-williams.com/category/wp-contenthome.html,Mathematics
2281,f736de70b906f5aff9fc89a098adf5045cfa8360,Graph Theory,"These notes have not been checked by Prof. A.G. Thomason and should not be regarded as ocial notes for the course. In particular, the responsibility for any errors is mine please email Sebastian Pancratz (sfp25) with any comments or corrections.",78-102,1984.0,http://www.hoffman.com/wp-content/main/categoriesprivacy.jsp,Mathematics
2282,a0c1483e0e34fb9bc1baf929ce892f111e329fa6,From time series to complex networks: The visibility graph,"In this work we present a simple and fast computational method, the visibility algorithm, that converts a time series into a graph. The constructed graph inherits several properties of the series in its structure. Thereby, periodic series convert into regular graphs, and random series do so into random graphs. Moreover, fractal series convert into scale-free networks, enhancing the fact that power law degree distributions are related to fractality, something highly discussed recently. Some remarkable examples and analytical tools are outlined to test the method's reliability. Many different measures, recently developed in the complex network theory, could by means of this new approach characterize time series from a new point of view.",4972 - 4975,2008.0,https://evans-campbell.com/posts/categorylogin.php,Physics
2283,33d54858878c14923035ffcbc018b7128170a1fb,A First Look at Graph Theory,"This book is intended to be an introductory text for mathematics and computer science students at the second and third year levels in universities. It gives an introduction to the subject with sufficient theory for students at those levels, with emphasis on algorithms and applications.",37-147,1991.0,http://www.park.com/tags/searchcategory.jsp,Computer Science
2284,2a7c8231105a4cb615b2b9b57a59658eadba3751,Applied Graph Theory,,144-144,1973.0,http://www.jones.com/tagregister.php,Computer Science
2285,0b9a20fd991a2dc33e6e27b66941b206cb8c638b,On a Hopf Algebra in Graph Theory,"We introduce and start the study of a bialgebra of graphs, which we call the 4-bialgebra, and of the dual bialgebra of 4-invariants. The 4-bialgebra is similar to the ring of graphs introduced by W. T. Tutte in 1946, but its structure is more complicated. The roots of the definition are in low dimensional topology, namely, in the recent theory of Vassiliev knot invariants. In particular, 4-invariants of graphs determine Vassiliev invariants of knots. The relation between the two notions is discussed.",104-121,2000.0,http://martinez-collins.com/blog/main/categoriespost.htm,Computer Science
2286,3767ec228395d3f50aced3cc823d7f5f67c52973,Geometric Graph Theory,"Note: Professor Pach's number: [172]; 2nd edition Reference DCG-CHAPTER-2008-027 Record created on 2008-11-18, modified on 2017-05-12",219-238,2004.0,http://diaz.biz/list/wp-content/mainregister.asp,Computer Science
2287,9883b126a94221c7a1e607438e629b5c465604d9,Extremal Graph Theory,,37-130,1978.0,http://www.miller.net/category/listmain.jsp,Mathematics
2288,851eb4b78f6deb8aba9d39529e462c5319940f51,"Spectral Graph Theory, Regional Conference Series in Math.",,99-123,1997.0,http://www.clayton.org/mainhome.htm,Mathematics
2289,4c584c886296f6c37f0ae71df57344bd184e5a5b,Basic graph theory: paths and circuits,,3-110,1996.0,http://www.lopez.com/tag/posts/searchindex.html,Computer Science
2290,f73bd6b6fa6267b08fad47ccde5700ff1def71f0,The Regularity Lemma and Its Applications in Graph Theory,,84-112,2000.0,http://reyes.com/explore/taglogin.htm,Mathematics
2291,4f270d2b736a4ef269bdfe0e7a1478027e370085,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,,153-158,1967.0,https://jones.biz/explore/search/apphomepage.htm,Computer Science
2292,0f68bd44a967fe294b369b080a12150b20e9e26c,Graph Theory and Its Applications to Problems of Society,,38-145,1987.0,https://www.murray.org/app/wp-contentauthor.html,Mathematics
2293,dce135859cb1bc830946f745067104b8e34d135c,Graph theory and applications,,98-135,1981.0,http://english-stone.com/categories/tag/categoriesprivacy.jsp,Computer Science
2294,845ae535fc0c4417364ec5ef41681b8839ce31fa,Graph theory,,"I-VIII, 1-332",1988.0,http://www.delacruz-lee.com/posts/tagsearch.jsp,Computer Science
2295,90152dddf7c83f0d8d9ba95c48635cfab1016cbe,Fractional Graph Theory,,62-125,1978.0,http://www.ortega.biz/wp-content/taghomepage.html,Mathematics
2296,ab695b94256553dc06276fc44455acb559611282,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",435 - 471,1998.0,http://www.phillips-ward.com/wp-content/wp-contentregister.html,Psychology
2297,034f6a2897bacfe8b86f123f4dfe5aa29af8ac1b,Exploring the repertoire of RNA secondary motifs using graph theory; implications for RNA design.,"Understanding the structural repertoire of RNA is crucial for RNA genomics research. Yet current methods for finding novel RNAs are limited to small or known RNA families. To expand known RNA structural motifs, we develop a two-dimensional graphical representation approach for describing and estimating the size of RNA's secondary structural repertoire, including naturally occurring and other possible RNA motifs. We employ tree graphs to describe RNA tree motifs and more general (dual) graphs to describe both RNA tree and pseudoknot motifs. Our estimates of RNA's structural space are vastly smaller than the nucleotide sequence space, suggesting a new avenue for finding novel RNAs. Specifically our survey shows that known RNA trees and pseudoknots represent only a small subset of all possible motifs, implying that some of the 'missing' motifs may represent novel RNAs. To help pinpoint RNA-like motifs, we show that the motifs of existing functional RNAs are clustered in a narrow range of topological characteristics. We also illustrate the applications of our approach to the design of novel RNAs and automated comparison of RNA structures; we report several occurrences of RNA motifs within larger RNAs. Thus, our graph theory approach to RNA structures has implications for RNA genomics, structure analysis and design.","
          2926-43
        ",2003.0,https://www.walker.org/tags/tag/mainprivacy.html,Biology
2298,a0cfdfb8cfbe58c3820fb1729c9030cd9a05bc2d,Graph theory and theoretical physics,,84-84,1969.0,http://little.com/app/category/appmain.html,Mathematics
2299,188b3b2f4afda1303b6fe4dc61daa30c33ee497d,Graph Theory in Practice: Part II,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",20-143,2000.0,http://www.myers-schultz.org/blog/wp-contentprivacy.html,Computer Science
2300,0f68bd44a967fe294b369b080a12150b20e9e26c,Graph Theory and Its Applications to Problems of Society,,39-135,1987.0,http://patterson.com/categoriespost.php,Mathematics
2301,96f795889e761f6eb0b4601b5803ed48294d6930,Discrete Mathematics With Graph Theory,"From the Publisher: 
Adopting a user-friendly, conversationaland at times humorousstyle, these authors make the principles and practices of discrete mathematics as stimulating as possible while presenting comprehensive, rigorous coverage. Examples and exercises integrated throughout each chapter serve to pique reader interest and bring clarity to even the most complex concepts. Above all, the book is designed to engage today's readers in the interesting, applicable facets of modern mathematics. More than 200 worked examples and problems, as well as over 2500 exercises are included. Full solutions are provided in the back of the book. More than 150 Pausesshort questions inserted at strategic pointsare included. Full solutions to Pauses are included at the end of each section. For educators in area of discrete mathematics.",18-131,1997.0,http://www.perry.biz/tags/wp-content/appsearch.asp,Computer Science
2302,71aa2175f1abc1745e4a8deb7356d0307a7ca64f,Graph Theory Applications,,99-123,1991.0,https://gardner-davis.com/mainauthor.html,Mathematics
2303,ab695b94256553dc06276fc44455acb559611282,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",435 - 471,1998.0,https://www.cooper.com/tagsterms.asp,Psychology
2304,dc9515a0f39739fd0e009a90d0dc48afb6e3ac07,Introductory Graph Theory,,23-115,1984.0,https://www.thompson.biz/app/wp-contenthomepage.html,Mathematics
2305,a0cfdfb8cfbe58c3820fb1729c9030cd9a05bc2d,Graph theory and theoretical physics,,84-84,1969.0,https://guerra.com/taglogin.html,Mathematics
2306,92a868e1159f7694da3f06faa8a345f3d3a50250,Problems and results in combinatorial analysis and graph theory,,81-92,1988.0,http://moody.info/postsindex.html,Computer Science
2307,80aae60da893d9fa40ac4bda63f2ae1aa3a29b55,Graph theory in network analysis,,235-244,1983.0,https://www.odom-webb.com/wp-content/app/listprivacy.php,Computer Science
2308,f73bd6b6fa6267b08fad47ccde5700ff1def71f0,The Regularity Lemma and Its Applications in Graph Theory,,84-112,2000.0,https://www.shelton.biz/tag/exploreabout.php,Mathematics
2309,28c1795001dfd52abf46d5b88c064811f974add2,Graph theory and combinatorics,"WORKING PAPERS q C. Borgs, J. Chayes, N. Immorlica, A. Kalai, V. S. Mirrokni and C. Papadimitriou, The Myth of the Folk Theorem, submitted to STOC. q U. Feige, N. Immorlica, V.S.Mirrokni and H. Nazerzadeh, Functional Approximations: A new approach for analyzing Heuristics, submitted to STOC. q B. Awerbuch, Y. Azar, and A. Epstein, V. S. Mirrokni, A. Skopalik, Fast Convergence to Nearly Optimal Solutions in Potential Games, submitted to STOC. q J. Hartline, V. S. Mirrokni, and M. Sundararajan, Marketing Strategies over Social Networks, submitted to WWW. q U. Feige, N. Immorlica, V.S. Mirrokni and H. Nazerzadeh, Combinatorial Allocation Mechanisms with Penalties for Banner Advertisement, submitted to WWW. q R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, A. Kalai, V. S. Mirrokni, M. Tennenholtz, Trust-based Recommendation Systems: An axiomatic Approach, submitted to WWW. q V. S. Mirrokni, M. Schapira, J. Vondrak, Tight Information-Theoretic Lower Bounds for Maximizing Social-Welfare in Combinatorial Auctions, submitted to IPCO. q M. Goemans, N. Harvey, R. Kleinberg, V. S. Mirrokni, On Learning submodular functions, to be submitted to ICALP. q H. Ackermann, P. Goldberg, V. S. Mirrokni, H. Roeglin, and B. Voecking, Uncoordinated Twosided Markets, to be submitted to ACM EC. q M. Ghodsi, M. Mahini, V. S. Mirrokni, and M. ZadiMoghaddam Singleton Betting for Permutation Betting Markets. q R. Andersen, C. Borgs, J. Chayes, K. Jain, J. Hopcroft, V. S. Mirrokni and S. Teng, Locally Computable Link Spam Features. q V.S. Mirrokni, A. Skopalik, On the Complexity of Nash Dynamics and Sink Equilibria. q R. Andersen, V. S. Mirrokni, Overlapping Clustering for Distributed Computation.",67-121,1979.0,https://adams-thompson.org/posts/listpost.html,Computer Science
2310,dbda5f7876960771410e3b2ec96e16ca1fc85529,Algebraic Graph Theory: Regular graphs and line graphs,,84-131,1974.0,http://salas.com/listregister.php,Mathematics
2311,e8821c26f6cdd1a7d1f1ebb59dff63585de530f7,Graph theory,,48-132,1979.0,http://www.frost-roberts.biz/explore/blog/mainlogin.jsp,Computer Science
2312,4f270d2b736a4ef269bdfe0e7a1478027e370085,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,,153-158,1967.0,https://west.info/search/categoriesauthor.htm,Computer Science
2313,d6af6a19d83e2d4164a91e2362c4603b54470390,Combinatorics and Graph Theory,,24-148,1981.0,https://green.biz/blog/bloghome.html,Mathematics
2314,034f6a2897bacfe8b86f123f4dfe5aa29af8ac1b,Exploring the repertoire of RNA secondary motifs using graph theory; implications for RNA design.,"Understanding the structural repertoire of RNA is crucial for RNA genomics research. Yet current methods for finding novel RNAs are limited to small or known RNA families. To expand known RNA structural motifs, we develop a two-dimensional graphical representation approach for describing and estimating the size of RNA's secondary structural repertoire, including naturally occurring and other possible RNA motifs. We employ tree graphs to describe RNA tree motifs and more general (dual) graphs to describe both RNA tree and pseudoknot motifs. Our estimates of RNA's structural space are vastly smaller than the nucleotide sequence space, suggesting a new avenue for finding novel RNAs. Specifically our survey shows that known RNA trees and pseudoknots represent only a small subset of all possible motifs, implying that some of the 'missing' motifs may represent novel RNAs. To help pinpoint RNA-like motifs, we show that the motifs of existing functional RNAs are clustered in a narrow range of topological characteristics. We also illustrate the applications of our approach to the design of novel RNAs and automated comparison of RNA structures; we report several occurrences of RNA motifs within larger RNAs. Thus, our graph theory approach to RNA structures has implications for RNA genomics, structure analysis and design.","
          2926-43
        ",2003.0,http://www.harris.biz/main/bloglogin.jsp,Biology
2315,90152dddf7c83f0d8d9ba95c48635cfab1016cbe,Fractional Graph Theory,,91-125,1978.0,https://www.foster-pena.com/app/categoryfaq.php,Mathematics
2316,9883b126a94221c7a1e607438e629b5c465604d9,Extremal Graph Theory,,97-126,1978.0,http://www.nelson.biz/posts/blog/categoryauthor.asp,Mathematics
2317,5e6a8fd4713137fe76ab15f27c6e6e5da686cc74,Chemical Applications of Graph Theory,,1-1,1992.0,http://www.shaw.com/taglogin.jsp,Chemistry
2318,a4df015d82386bb4783e7a8bdf430d8118b5155a,Contentment in graph theory: Covering graphs with cliques,,406-424,1977.0,https://davis.net/category/blogpost.htm,Mathematics
2319,8d15baabbc36f2bcaae9d023b8b3cc974c64428a,Applications of Graph Theory to Group Structure.,,1145,1966.0,http://www.perez.com/mainsearch.php,Computer Science
2320,bf69eddc4efacdaf33c135009d468df52ef77f1c,Graph Decompositions: A Study in Infinite Graph Theory,Note to the reader Introduction Fundamental facts and concepts Separating simplices and the existence of prime decompositions Simplicial minors and the existence of prime decompositions The uniqueness of prime decompositions Decompositions into small factors Applications of simplicial decompositions Appendix: Some notes on set theory References Subject index Index of symbols.,79-119,1990.0,http://kelley.com/app/taglogin.php,Mathematics
2321,07ae6518908df65f028106213877c1705ad6692d,GRAPH THEORY IN PRACTICE : PART I,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",46-125,1999.0,https://www.pearson.net/blog/blogprivacy.php,Technology
2322,7b815d6c9c0b072278979ba97eda839d141d5d41,"Review of ""Graph Theory"" by Wataru Mayeda",,300,1973.0,https://www.hale.com/posts/blogfaq.html,Computer Science
2323,aa31f664fb8c6fa41291832376abe94804410d14,An Extremal Problem for Sets with Applications to Graph Theory,,82-89,1985.0,https://www.sanchez-lewis.com/listfaq.php,Computer Science
2324,f7f505e4984a8a60f4d7c3aafd4ad1875913fe2d,Maximizing the total number of spanning trees in a graph: Two related problems in graph theory and optimum design theory,,240-248,1981.0,https://nelson-bush.info/category/blogregister.html,Computer Science
2325,ec5e9df34a4671c07d54d73e0e6b67be47cb71af,On a Problem in Graph Theory,Suppose there are n towns every pair of which are connected by a single one-way road (roads meet only at towns). Is it possible to choose the direction of the traffic on all the roads so that if any two towns are named there is always a third from which the two named can be reached directly by road?,220 - 223,1963.0,https://www.donovan-barrett.com/categories/categories/categoriesprivacy.asp,Mathematics
2326,025c03c7fd1c410ca3a38d94f5696ddee5c28811,Some problems in graph theory,,187-190,1974.0,https://www.frye-smith.com/search/category/tagsearch.php,Computer Science
2327,78d86a61dcba9d0ccaabf5de1f7ae19a033e0121,Graph Theory As A Mathematical Model In Social Science,,96-120,1953.0,https://www.fuller.biz/blog/posts/searchterms.asp,Mathematics
2328,fe01d24bd55ca63ceca04539c88ea477c3a50b07,Algebraic Graph Theory: The multiplicative expansion,,36-130,1974.0,http://foster-clark.biz/list/categoryterms.html,Mathematics
2329,d5151713a569f203e8cc93d3aee5f3a2168ebe8a,The Many Facets of Graph Theory,,15-126,1969.0,https://www.jones.biz/bloghomepage.jsp,Mathematics
2330,21569e44f1daa0a10f635391e1fdfed43decc394,Graph theory and Gaussian elimination.,,3-22,1975.0,http://www.wilson.com/tags/explore/searchpost.php,Mathematics
2331,8df919ca5405ca5d1d8b192c14584b4354fa3dde,SOME ODD GRAPH THEORY,,55-103,1979.0,https://www.manning.com/listcategory.asp,Mathematics
2332,2f6d6254a33015aec2bcc0a79561262eb9f473ed,Extremal graph theory with emphasis on probabilistic methods,Subdivisions Contractions Small graphs of large girth Large graphs of small diameter Cycles in dense graphs The evolution of random graphs The size Ramsey number of a path Weakly saturated graphs List colourings.,"I-VII, 1-64",1986.0,http://reyes.com/blogindex.php,Mathematics
2333,dacf87fd2f31762facc1dc7d3190c3455aa78d80,Applied graph theory: Graphs and electrical networks,,1299-1300,1976.0,http://ward.com/category/listsearch.jsp,Computer Science
2334,8e91e3c18bff916938fb9d5a742611b6b423bc49,Applications of graph theory algorithms,,19-103,1979.0,https://haynes.biz/explore/exploreabout.html,Computer Science
2335,4e618c4e02ec9cfe1e2e07dfeb9155071dabe9fa,Extremal problems in graph theory,,42-112,1997.0,http://bailey.com/explore/categories/categoriesindex.htm,Mathematics
2336,7b44986e28bddf8bc163ac34b439b67169c29b55,Graph theory and computing,,90-119,1972.0,http://www.shelton.com/search/listfaq.html,Computer Science
2337,b134ce6220b0c39bc3d258c7388afb589016a6c4,Graph theory and related topics,,66-150,1979.0,https://shelton.biz/list/main/postssearch.htm,Mathematics
2338,a658730fe7dd269186dca9b09cde32fed232c169,Computational chemical graph theory,,86-136,1990.0,http://sloan-wilkins.com/category/blogauthor.php,Computer Science
2339,56c979574a36c9f5bc7915964adc3d22972415c5,Graph Theory Coding Theory and Block Designs: Strongly regular graphs,"Introduction 1. A brief introduction to design theory 2. Strongly regular graphs 3, Quasi-symmetric designs 4. Strongly regular graphs with no triangles 5. Polarities of designs 6. Extension of graphs 7. Codes 8. Cyclic codes 9. Threshold decoding 10. Reed-Muller codes 11. Self-orthogonal codes and designs 12. Quadratic residue codes 13. Symmetry codes over GF(3) 14. Nearly perfect binary codes and uniformly packed codes 15. Association schemes References Index.",78-102,1975.0,http://www.arnold.org/app/tags/mainpost.php,Mathematics
2340,024d1cffa9e112d9e81c76cc00fb44e3bd761011,Progress in Graph Theory,,27-101,1984.0,http://www.stephens.com/wp-content/posts/searchlogin.php,Computer Science
2341,bcb089785e52cfaebdf9de029ec3a6a7586dbe8c,Proof Techniques in Graph Theory,,997,1970.0,https://white.net/app/app/blogpost.php,Mathematics
2342,cf1a8255192b77f003ea43f3896beaacfdde741b,Bibliography of bond graph theory and application,,1067-1109,1991.0,http://thomas.com/mainlogin.htm,Computer Science
2343,3e3d6d84f66c6f26db23711931c87db01baf9d81,Graph theory and molecular orbitals,,67-78,1972.0,https://www.manning.biz/tagpost.asp,Chemistry
2344,2b694677da99739df582a93bb7e6ca9a92aaf0fb,Applications of graph theory,,68-126,1979.0,http://silva.com/category/blogsearch.php,Computer Science
2345,6a52ba688f0c813dcd1eab78ee159ef9c2f5a31f,Algebraic methods in graph theory,,97-122,1981.0,http://www.garcia-anderson.org/search/list/blogsearch.asp,Mathematics
2346,07dc56d86321ff89a96fd32521fbaefe24ea7376,Topics in graph theory,,28-134,1979.0,https://www.reyes.com/tagsindex.html,Computer Science
2347,6fc1057ccc4d430051b69d456224ce12bca17dcc,Graph theory in modern engineering,,97-148,1973.0,https://www.stewart.com/postsabout.html,Mathematics
2348,4213b7478d12581b144e67d6355313297c80f926,Graph Theory and Algorithms,,78-119,1981.0,http://www.elliott-castro.org/wp-content/tagshome.html,Computer Science
2349,4dd69578aaa585b5ad3f755fd38fff1402a22dec,LANDSCAPE CONNECTIVITY: A GRAPH‐THEORETIC PERSPECTIVE,"Ecologists are familiar with two data structures commonly used to represent landscapes. Vector-based maps delineate land cover types as polygons, while raster lattices represent the landscape as a grid. Here we adopt a third lattice data structure, the graph. A graph represents a landscape as a set of nodes (e.g., habitat patches) connected to some degree by edges that join pairs of nodes functionally (e.g., via dispersal). Graph theory is well developed in other fields, including geography (transportation networks, routing ap- plications, siting problems) and computer science (circuitry and network optimization). We present an overview of basic elements of graph theory as it might be applied to issues of connectivity in heterogeneous landscapes, focusing especially on applications of metapo- pulation theory in conservation biology. We develop a general set of analyses using a hypothetical landscape mosaic of habitat patches in a nonhabitat matrix. Our results suggest that a simple graph construct, the minimum spanning tree, can serve as a powerful guide to decisions about the relative importance of individual patches to overall landscape con- nectivity. We then apply this approach to an actual conservation scenario involving the",1205-1218,2001.0,https://carter.com/search/listprivacy.html,Technology
2350,137a07b135e1bf80cd4aaa1207f8081dba5764bd,Foundations of Chemical Reaction Network Theory,,88-148,2019.0,http://www.robles-peters.org/list/listcategory.html,Mathematics
2351,a3fb71f20f70f139b69e4fcd6155e9275f5b405b,Graph theory,,74-104,1972.0,https://www.gonzales-williams.com/explore/blog/exploresearch.html,Computer Science
2352,31a8063d5032278580969db08e7ef5589fc58cc6,Sampling Signals on Graphs: From Theory to Applications,"The study of sampling signals on graphs, with the goal of building an analog of sampling for standard signals in the time and spatial domains, has attracted considerable attention recently. Beyond adding to the growing theory on graph signal processing (GSP), sampling on graphs has various promising applications. In this article, we review the current progress on sampling over graphs, focusing on theory and potential applications.",14-30,2020.0,http://www.buchanan-meyer.info/tags/categorymain.html,Computer Science
2353,eae7b0ee87fd63ed13783804da27c931f4e451d8,An Introduction to the Theory of Graph Spectra: Frontmatter,Preface 1. Introduction 2. Graph operations and modifications 3. Spectrum and structure 4. Characterizations by spectra 5. Structure and one eigenvalue 6. Spectral techniques 7. Laplacians 8. Additional topics 9. Applications Appendix Bibliography Index of symbols Index.,75-132,2009.0,https://www.dunn.org/categoryauthor.php,Computer Science
2354,c7e6d7d5c6a9438e3ea15c2c4bd440e63847ffbf,"The core decomposition of networks: theory, algorithms and applications",,61 - 92,2019.0,https://www.robinson-roth.com/app/categories/blogpost.php,Computer Science
2355,804b3bbc7e5b9a14d446ff9f92236652cf1b1c72,Spectra of Graphs: Theory and Applications,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,71-146,1997.0,https://johnson.org/categoryhome.php,Mathematics
2356,39cd2f4696bcc1019e218fb7724d1fa263fee9f7,Supereulerian graphs and the Petersen graph,"A graph G is supereulerian if G has a spanning eulerian subgraph. Boesch et al. [J. Graph Theory, 1, 79–84 (1977)] proposed the problem of characterizing supereulerian graphs. In this paper, we prove that any 3-edge-connected graph with at most 11 edge-cuts of size 3 is supereulerian if and only if it cannot be contractible to the Petersen graph. This extends a former result of Catlin and Lai [J. Combin. Theory, Ser. B, 66, 123–139 (1996)].",291-304,2014.0,https://www.lewis.com/list/mainabout.htm,Mathematics
2357,92170231069fd144805ba9c356ee002db123381b,Riemann–Roch and Abel–Jacobi theory on a finite graph,,766-788,2006.0,https://www.marsh-alvarez.com/list/mainterms.jsp,Mathematics
2358,448dc7527c4031086bcdf3d117e73bc163b64c5e,The History of Degenerate (Bipartite) Extremal Graph Problems,,169-264,2013.0,http://www.simmons.com/blog/searchfaq.html,Mathematics
2359,49768fb7280aec248809ec47594a4458888018a8,Graph theoretical analysis of complex networks in the brain,,3 - 3,2007.0,http://clay.com/category/category/mainhome.asp,Computer Science
2360,5874afa99a66458efe791bfa5e7196ec870348db,TOPICS IN GEOMETRIC GROUP THEORY,"We present a brief overview of methods and results in geometric group theory, with the goal of introducing the reader to both topological and metric perspectives. Prerequisites are kept to a minimum: we require only basic algebra, graph theory, and metric space topology.",17-148,2015.0,https://lee.com/blog/wp-contentterms.html,Technology
2361,cc0b5f0ea6a81c2f9ee95e8a246733b630c11e63,Guidelines for a graph-theoretic implementation of structural equation modeling,"Structural equation modeling (SEM) is increasingly being chosen by researchers as a framework for gaining scientific insights from the quantitative analyses of data. New ideas and methods emerging from the study of causality, influences from the field of graphical modeling, and advances in statistics are expanding the rigor, capability, and even purpose of SEM. Guidelines for implementing the expanded capabilities of SEM are currently lacking. In this paper we describe new developments in SEM that we believe constitute a third-generation of the methodology. Most characteristic of this new approach is the generalization of the structural equation model as a causal graph. In this generalization, analyses are based on graph theoretic principles rather than analyses of matrices. Also, new devices such as metamodels and causal diagrams, as well as an increased emphasis on queries and probabilistic reasoning, are now included. Estimation under a graph theory framework permits the use of Bayesian or likelihood methods. The guidelines presented start from a declaration of the goals of the analysis. We then discuss how theory frames the modeling process, requirements for causal interpretation, model specification choices, selection of estimation method, model evaluation options, and use of queries, both to summarize retrospective results and for prospective analyses. 
 
The illustrative example presented involves monitoring data from wetlands on Mount Desert Island, home of Acadia National Park. Our presentation walks through the decision process involved in developing and evaluating models, as well as drawing inferences from the resulting prediction equations. In addition to evaluating hypotheses about the connections between human activities and biotic responses, we illustrate how the structural equation (SE) model can be queried to understand how interventions might take advantage of an environmental threshold to limit Typha invasions. 
 
The guidelines presented provide for an updated definition of the SEM process that subsumes the historical matrix approach under a graph-theory implementation. The implementation is also designed to permit complex specifications and to be compatible with various estimation methods. Finally, they are meant to foster the use of probabilistic reasoning in both retrospective and prospective considerations of the quantitative implications of the results.",art73,2012.0,https://howard-grimes.info/tags/blogmain.php,Computer Science
2362,9e0b0decf155c3f802017238d0096255f75a263c,Topological Graph Polynomials in Colored Group Field Theory,,565-584,2009.0,https://www.davis.com/listsearch.html,Mathematics
2363,5b64f9fe601db2cc862023efbaf56d9b74f1eef4,Large Networks and Graph Limits,"Recently, it became apparent that a large number of the most interesting structures and phenomena of the world can be described by networks. To develop a mathematical theory of very large networks is an important challenge. This book describes one recent approach to this theory, the limit theory of graphs, which has emerged over the last decade. The theory has rich connections with other approaches to the study of large networks, such as ""property testing"" in computer science and regularity partition in graph theory. It has several applications in extremal graph theory, including the exact formulations and partial answers to very general questions, such as which problems in extremal graph theory are decidable. It also has less obvious connections with other parts of mathematics (classical and non-classical, like probability theory, measure theory, tensor algebras, and semidefinite optimization). This book explains many of these connections, first at an informal level to emphasize the need to apply more advanced mathematical methods, and then gives an exact development of the theory of the algebraic theory of graph homomorphisms and of the analytic theory of graph limits. This is an amazing book: readable, deep, and lively. It sets out this emerging area, makes connections between old classical graph theory and graph limits, and charts the course of the future. --Persi Diaconis, Stanford University This book is a comprehensive study of the active topic of graph limits and an updated account of its present status. It is a beautiful volume written by an outstanding mathematician who is also a great expositor. --Noga Alon, Tel Aviv University, Israel Modern combinatorics is by no means an isolated subject in mathematics, but has many rich and interesting connections to almost every area of mathematics and computer science. The research presented in Lovasz's book exemplifies this phenomenon. This book presents a wonderful opportunity for a student in combinatorics to explore other fields of mathematics, or conversely for experts in other areas of mathematics to become acquainted with some aspects of graph theory. --Terence Tao, University of California, Los Angeles, CA Laszlo Lovasz has written an admirable treatise on the exciting new theory of graph limits and graph homomorphisms, an area of great importance in the study of large networks. It is an authoritative, masterful text that reflects Lovasz's position as the main architect of this rapidly developing theory. The book is a must for combinatorialists, network theorists, and theoretical computer scientists alike. --Bela Bollobas, Cambridge University, UK","I-XIV, 1-475",2012.0,http://www.west.com/app/categoryhome.html,Mathematics
2364,d224c80ac2034d832a35ba646a0062bd773ee3c4,New Integrable 4D Quantum Field Theories from Strongly Deformed Planar N=4 Supersymmetric Yang-Mills Theory.,"We introduce a family of new integrable quantum field theories in four dimensions by considering the γ-deformed N=4 supersymmetric Yang-Mills (SYM) theory in the double scaling limit of large imaginary twists and small coupling. This limit discards the gauge fields and retains only certain Yukawa and scalar interactions with three arbitrary effective couplings. In the 't Hooft limit, these 4D theories are integrable, and contain a wealth of conformal correlators such that the whole arsenal of AdS/CFT integrability remains applicable. As a special case of these models, we obtain a quantum field theory of two complex scalars with a chiral, quartic interaction. The Berenstein-Maldacena-Nastase vacuum anomalous dimension is dominated in each loop order by a single ""wheel"" graph, whose bulk represents an integrable ""fishnet"" graph. This explicitly demonstrates the all-loop integrability of gamma-deformed planar N=4 SYM theory, at least in our limit. Using this feature and integrability results we provide an explicit conjecture for the periods of double-wheel graphs with an arbitrary number of spokes in terms of multiple zeta values of limited depth.","
          201602
        ",2016.0,http://www.butler-huffman.net/category/tagshomepage.html,Physics
2365,1d0bf1d65b61383b0f89aef7b907716f7e411128,Image Processing and Analysis With Graphs: theory and Practice,"Graph Theory Concepts and Definitions Used in Image Processing and Analysis, O. Lezoray and L. Grady Introduction Basic Graph Theory Graph Representation Paths, Trees, and Connectivity Graph Models in Image Processing and Analysis Graph Cuts-Combinatorial Optimization in Vision, H. Ishikawa Introduction Markov Random Field Basic Graph Cuts: Binary Labels Multi-Label Minimization Examples Higher-Order Models in Computer Vision, P. Kohli and C. Rother Introduction Higher-Order Random Fields Patch and Region-Based Potentials Relating Appearance Models and Region-Based Potentials Global Potentials Maximum a Posteriori Inference A Parametric Maximum Flow Approach for Discrete Total Variation Regularization, A. Chambolle and J. Darbon Introduction Idea of the approach Numerical Computations Applications Targeted Image Segmentation Using Graph Methods, L. Grady The Regularization of Targeted Image Segmentation Target Specification Conclusion A Short Tour of Mathematical Morphology on Edge and Vertex Weighted Graphs, L. Najman and F. Meyer Introduction Graphs and lattices Neighborhood Operations on Graphs Filters Connected Operators and Filtering with the Component Tree Watershed Cuts MSF Cut Hierarchy and Saliency Maps Optimization and the Power Watershed Partial Difference Equations on Graphs for Local and Nonlocal Image Processing, A. Elmoataz, O. Lezoray, V.-T. Ta, and S. Bougleux Introduction Difference Operators on Weighted Graphs Construction of Weighted Graphs p-Laplacian Regularization on Graphs Examples Image Denoising with Nonlocal Spectral Graph Wavelets, D.K. Hammond, L. Jacques, and P. Vandergheynst Introduction Spectral Graph Wavelet Transform Nonlocal Image Graph Hybrid Local/Nonlocal Image Graph Scaled Laplacian Model Applications to Image Denoising Conclusions Acknowledgments Image and Video Matting, J. Wang Introduction Graph Construction for Image Matting Solving Image Matting Graphs Data Set Video Matting Optimal Simultaneous Multisurface and Multiobject Image Segmentation, X. Wu, M.K. Garvin, and M. Sonka Introduction Motivation and Problem Description Methods for Graph-Based Image Segmentation Case Studies Conclusion Acknowledgments Hierarchical Graph Encodings, L. Brun and W. Kropatsch Introduction Regular Pyramids Irregular Pyramids Parallel construction schemes Irregular Pyramids and Image properties Graph-Based Dimensionality Reduction, J.A. Lee and M. Verleysen Summary Introduction Classical methods Nonlinearity through Graphs Graph-Based Distances Graph-Based Similarities Graph embedding Examples and comparisons Graph Edit Distance-Theory, Algorithms, and Applications, M. Ferrer and H. Bunke Introduction Definitions and Graph Matching Theoretical Aspects of GED GED Computation Applications of GED The Role of Graphs in Matching Shapes and in Categorization, B. Kimia Introduction Using Shock Graphs for Shape Matching Using Proximity Graphs for Categorization Conclusion Acknowledgment 3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching, A. Sharma, R. Horaud, and D. Mateus Introduction Graph Matrices Spectral Graph Isomorphism Graph Embedding and Dimensionality Reduction Spectral Shape Matching Experiments and Results Discussion Appendix: Permutation and Doubly- stochastic Matrices Appendix: The Frobenius Norm Appendix: Spectral Properties of the Normalized Laplacian Modeling Images with Undirected Graphical Models, M.F. Tappen Introduction Background Graphical Models for Modeling Image Patches Pixel-Based Graphical Models Inference in Graphical Models Learning in Undirected Graphical Models Tree-Walk Kernels for Computer Vision, Z. Harchaoui and F. Bach Introduction Tree-Walk Kernels as Graph Kernels The Region Adjacency Graph Kernel as a Tree-Walk Kernel The Point Cloud Kernel as a Tree-Walk Kernel Experimental Results Conclusion Acknowledgments",49-150,2017.0,https://www.henderson.com/mainterms.htm,Mathematics
2366,a89cc66d5c615eb350b46eda017ff002aa3123d9,Neural Field Continuum Limits and the Structure–Function Partitioning of Cognitive–Emotional Brain Networks,"Simple Summary Pessoa postulates that bran anatomy associated with the processing and expression of emotion-laden content, such as the amygdala and limbic cortices, is resource capacity-limited. Thus, brains require multichannel or parallel structure-function connectivity to effectively perceive, motivate, integrate, represent, recall, and execute cognitive-emotional relationships. Pessoa employs 2D graph network theory to support his views on distributed brain organization and operation, concluding that brains evolve through dual-process competition and cooperation to form highly embedded computational architectures with little structure–function compartmentalization. Low-dimensional graph theory has become a popular mathematical tool to model, simulate, and visualize evolving complex, sometimes intractable, brain networks. Graph theory offers advantages to study and understand various biological and technological network behaviors and, for Pessoa, it permits a framework that accounts for structure–function features thus far poorly explained by perhaps “traditional” perspectives, which advocate for the mapping of structure–function relationships onto well-localized brain areas. Pessoa nonetheless fails to fully appreciate the significance of weak-to-strong structure-function correlations for brain dynamics and why those correlations, caused by differential control parameters such as Hebbian and antiHebbian neuronal plasticity, are best assessed using neural field theories. Neural fields demonstrate that embedded brain networks optimally evolve between exotic computational phases and continuum limits with the accompaniment of some network partitioning, rather than unconstrained embeddedness, when rendering healthy cognitive-emotional functionality. Abstract In The cognitive-emotional brain, Pessoa overlooks continuum effects on nonlinear brain network connectivity by eschewing neural field theories and physiologically derived constructs representative of neuronal plasticity. The absence of this content, which is so very important for understanding the dynamic structure-function embedding and partitioning of brains, diminishes the rich competitive and cooperative nature of neural networks and trivializes Pessoa’s arguments, and similar arguments by other authors, on the phylogenetic and operational significance of an optimally integrated brain filled with variable-strength neural connections. Riemannian neuromanifolds, containing limit-imposing metaplastic Hebbian- and antiHebbian-type control variables, simulate scalable network behavior that is difficult to capture from the simpler graph-theoretic analysis preferred by Pessoa and other neuroscientists. Field theories suggest the partitioning and performance benefits of embedded cognitive-emotional networks that optimally evolve between exotic classical and quantum computational phases, where matrix singularities and condensations produce degenerate structure-function homogeneities unrealistic of healthy brains. Some network partitioning, as opposed to unconstrained embeddedness, is thus required for effective execution of cognitive-emotional network functions and, in our new era of neuroscience, should be considered a critical aspect of proper brain organization and operation.",46-108,2023.0,https://www.york.net/app/categories/searchprivacy.html,Medicine
2367,2b9e05c6e282934f69c366f1856b637a921fed5f,Graph Structure and Monadic Second-Order Logic - A Language-Theoretic Approach,"The study of graph structure has advanced in recent years with great strides: finite graphs can be described algebraically, enabling them to be constructed out of more basic elements. Separately the properties of graphs can be studied in a logical language called monadic second-order logic. In this book, these two features of graph structure are brought together for the first time in a presentation that unifies and synthesizes research over the last 25 years. The author not only provides a thorough description of the theory, but also details its applications, on the one hand to the construction of graph algorithms, and, on the other to the extension of formal language theory to finite graphs. Consequently the book will be of interest to graduate students and researchers in graph theory, finite model theory, formal language theory, and complexity theory.",179,2012.0,https://gibson.org/listsearch.html,Computer Science
2368,89cc8969c288b3cf6b5c691baaafb7c8e5c82d1b,Open problems in the spectral theory of signed graphs,"Signed graphs are graphs whose edges get a sign $+1$ or $-1$ (the signature). Signed graphs can be studied by means of graph matrices extended to signed graphs in a natural way. Recently, the spectra of signed graphs have attracted much attention from graph spectra specialists. One motivation is that the spectral theory of signed graphs elegantly generalizes the spectral theories of unsigned graphs. On the other hand, unsigned graphs do not disappear completely, since their role can be taken by the special case of balanced signed graphs. 
Therefore, spectral problems defined and studied for unsigned graphs can be considered in terms of signed graphs, and sometimes such generalization shows nice properties which cannot be appreciated in terms of (unsigned) graphs. Here, we survey some general results on the adjacency spectra of signed graphs, and we consider some spectral problems which are inspired from the spectral theory of (unsigned) graphs.",#P2.10,2019.0,http://www.dominguez-roach.biz/mainhomepage.jsp,Computer Science
2369,c99ac1ab758d967c4c07e03ad650ecc73814feb6,Nonstable K-theory for Graph Algebras,,157-178,2004.0,https://www.parker.com/category/tagcategory.php,Mathematics
2370,d62cb827ad80f315e416ac2e74f189328651801a,Theory and Applications of Models of Computation,,49-128,2015.0,https://www.marquez-chase.com/list/posts/listpost.html,Computer Science
2371,050c86809f8b48c5f1254ebe67fdab4e01202cfb,Toward a spectral theory of cellular sheaves,,315 - 358,2018.0,https://www.price-green.com/main/categorypost.php,Mathematics
2372,cd3929c6e994a712739b907f06622eb814e18881,A theory of graph comprehension.,"Venn diagrams, flow charts, tree structures, node networks, to name just a few-or to the great lengths that computer companies go to advertise the graphic capabilities of their products, to see that charts and graphs have enormous appeal to people. All of this is true despite the fact that in virtually every case, the same information can be communicated by nonpictorial means: tables of numbers, lists of propositions cross-referI ! enced by global variables, labeled bracketings, and so on. Perhaps pictorial displays are simply pleasing to the eye, but both introspection and experimental evidence (Carter, 1947; Culbertson & Powers, 1959; Schutz, l%la, I",31-115,1990.0,http://www.williamson.info/categoryfaq.htm,Computer Science
2373,b6c8e5cb261b412656c79b34268f11643ff358a2,Introduction to the Algebraic Theory of Graph Grammars (A Survey),,1-69,1978.0,http://www.hill.org/listterms.htm,Computer Science
2374,1eae65773032fae65e7c003da3f67c47996c7f71,"Algorithmic graph minor theory: Decomposition, approximation, and coloring","At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.",637-646,2005.0,https://www.rowland.com/posts/blog/wp-contenthomepage.php,Mathematics
2375,db48859397cfe3d28e32892d1f8df6709bce2704,Graph Spectra for Complex Networks,"This concise and self-contained introduction builds up the spectral theory of graphs from scratch, with linear algebra and the theory of polynomials developed in the later parts. The book focuses on properties and bounds for the eigenvalues of the adjacency, Laplacian and effective resistance matrices of a graph. The goal of the book is to collect spectral properties that may help to understand the behavior or main characteristics of real-world networks. The chapter on spectra of complex networks illustrates how the theory may be applied to deduce insights into real-world networks. The second edition contains new chapters on topics in linear algebra and on the effective resistance matrix, and treats the pseudoinverse of the Laplacian. The latter two matrices and the Laplacian describe linear processes, such as the flow of current, on a graph. The concepts of spectral sparsification and graph neural networks are included.",72-124,2010.0,https://www.valencia.com/wp-contentauthor.htm,Mathematics
2376,e9ca95e54ec262719fa05607a694d7b85a139f0b,"Theory and Applications of the Analytic Network Process: Decision Making With Benefits, Opportunities, Costs, and Risks",what are the real world applications of analytic number. theory and applications of the analytic network process. the analytic network process springerlink. forthing talks the analytic network. theory and applications of the analytic network process. theory and applications of the analytic network process. theory and applications of the analytic network process. marketing applications of the analytic hierarchy process. thomas saaty google scholar citations. international journal of the analytic hierarchy process. what are some important applications of game theory quora. an introduction to graph theory and network analysis with. learning from networks algorithms theory amp applications. international journal of industrial engineering theory. 9781888603064 theory and applications of the analytic. fundamentals of decision making and priority theory with. theory and applications of the analytic network process. pdf the analytic network process researchgate. theory and applications of the analytic network process. theory and applications of the analytic network process. theory and applications of the analytic network process. analytic network process. pdf network theory social network analysis. conference on harmonic analysis function theory operator. plex analysis applications toward number theory. theory and applications of the analytic network process. network meta analysis applying graph theory. theory and applications of the analytic element method. 1888603062 theory and applications of the analytic. learning from networks algorithms theory and applications. analytic network process an overview of applications. theory and applications of the analytic element method. social network analysis theory and applications blogger. pdf generated using the open source mwlib toolkit see. using the analytic hierarchy process ahp to select and. the analytic network process springerlink. a first course in network theory ernesto estrada philip. thomas l saaty. theory and applications of the analytic network process. evaluating the performance of taiwan homestay using. graph theory in network analysis. theory and applications of the analytic network process. application of the analytic network process to facility. theory and applications of the analytic network process. decision making with the analytic network process. decision making with the analytic network process. social network analysis theory and applications in. buy theory and applications of the analytic network. decision making with the analytic network process,49-106,2005.0,https://johnson-martinez.com/mainhome.html,Computer Science
2377,87c3e5caec704425c53b76c7f4497b9c45479554,Fundamental Theory for Typed Attributed Graph Transformation,,161-177,2004.0,http://www.elliott.com/posts/tags/appauthor.asp,Computer Science
2378,62fa835ba18dfac1e5ee801da1e27634cc5370e7,Graph-theoretic methods in database theory,"As in many areas of computer science and other disciplines, graph theoretic tools play an important role also in databases. Many concepts are best captured in terms of graphs or hypergraphs, and problems can then be formulated and solved using graph theoretic algorithms. There is a great number of such examples from schema design, dependency theory, transaction processing, query optimization, data distribution, and a host of other areas. We will not attempt to touch on the wide range of all these applications. Rather, we will concentrate on a particular, basic type of problems that has attracted a great deal of attention in the database literature over the last few years and has come to play a central role: techniques for searching graphs and computing transitive closure, and some of the applications and related problems in query processing. There is an extensive literature on these types of problems, which we cannot reasonably hope to cover in this space, but we shall give a flavour of the issues that arise in solving these problems in various frameworks.",20-145,1990.0,https://gray.com/mainterms.php,Computer Science
2379,0d399759edd123e62f2e81933d88411b96038a86,Graph minor theory,Lecture notes for the topics course on Graph Minor theory.,75-86,2005.0,http://johnson-manning.com/category/mainhome.jsp,Mathematics
2380,ec2701b6b5c58e1ba851e28452a303f76cb4c7c2,Term graph rewriting: theory and practice,"Partial table of contents: How to Get Confluence for Explicit Substitutions (T. Hardin) Graph Rewriting Systems for Efficient Compilation (Z. Ariola & Arvind) Abstract Reduction: Towards a Theory via Abstract Interpretation (M. van Eekelen, et al.) The Adequacy of Term Graph Rewriting for Simulating Term Rewriting (J. Kennaway, et al.) Hypergraph Rewriting: Critical Pairs and Undecidability of Confluence (D. Plump) MONSTR: Term Graph Rewriting for Parallel Machines (R. Banach) Parallel Execution of Concurrent Clean on ZAPP (R. Goldsmith, et al.) Implementing Logical Variables and Disjunctions in Graph Rewrite Systems (P. McBrien) Index.",77-106,1993.0,http://www.flores.com/blog/tagcategory.php,Mathematics
2381,fa459de6552f5cd0cbe28539c0c7c65bc112a164,"Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods",Abstract The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction...,531-554,1984.0,https://gardner.com/postshomepage.html,Mathematics
2382,0fb7af235cac61c78c4d184f718b043f3a0e37a4,Graph Algorithms in the Language of Linear Algebra,"The thesis presents usefulness of duality between graph and his adjacency matrix. The teoretical part provides the basis of graph theory and matrix algebra mainly focusing on sparse matrices and options of their presentation witch takes into account the number of nonzero elements in the matrix. The thesis includes presentation of possible operations on sparse matrices and algorithms that basically work on graphs, but with help of duality between graph and his adjacency matrix can be presented with sequence of operations on matrices. 
Practical part presents implementation of some algorithms that can work both with graphs or their adjacency matrices in programming language Java and testing algorithms that work with matrices. 
It focuses on comparison in efficiency of algorithm working with matrix written in standard mode and with matrix written in format for sparse matrices. It also studies witch presentation of matrices works beter for witch algorithm.",62-127,2012.0,http://www.moore.com/wp-content/appsearch.htm,Computer Science
2383,c41eb895616e453dcba1a70c9b942c5063cc656c,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,"In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.",3837-3845,2016.0,https://www.collins.info/tagabout.html,Computer Science
2384,60797ccfb655cc1678793cf16ec787f73816cbf4,An algebraic theory of graph reduction,"We show how membership in classes of graphs definable in monadic second order logic and of bounded treewidth can be decided by finite sets of terminating reduction rules. The method is constructive in the sense that we describe an algorithm which will produce, from a formula in monadic second order logic and an integer k such that the class defined by the formula is of treewidth ≤ k, a set of rewrite rules that reduces any member of the class to one of finitely many graphs, in a number of steps bounded by the size of the graph. This reduction system corresponds to an algorithm that runs in time linear in the size of the graph.",1134-1164,1990.0,http://www.shaw.com/tagmain.htm,Computer Science
2385,19cd8122dc531dcdf70f1430331f1df9458ccda2,The Gewirtz Graph: An Exercise in the Theory of Graph Spectra,"We prove that there is a unique graph (on 56 vertices) with spectrum 101235(-4)20 and examine its structure. It turns out that, e.g., the Coxeter graph (on 28 vertices) and the Sylvester graph (on 36 vertices) are induced subgraphs. We give descriptions of this graph.",397-407,1993.0,http://lawrence.com/tagsabout.php,Computer Science
2386,0cac090d379836a82b7aae6abc222ab7822b8763,Graph- Theoretical Approaches to the Theory of Voting*,"In this article, language, concepts, and theorems from the theory of directed graphs are used to characterize and analyze the structure of majority preference. A number of results are then derived concerning ""sincere,"" ""sophisticated,"" and ""cooperative"" voting decisions under two common majority voting procedures. These results supplement the work of Black and Farquharson. Perhaps contrary to ""common-sense"" thinking, general strategic manipulation of voting processes has beneficial consequences. It is widely recognized-and not only by political scientists-that the decisions of a voting body may be affected not only by such obviously relevant matters as the preferences of its members and their participation in or absence from particular votes, but also by such ""technical"" matters as the nature of the voting procedure and the order in which proposals are voted on. It is also recognized that voting may have ""gamelike"" characteristics offering strategic opportunities both to voters as individuals and to voters in coalitions. Finally, most political scientists-though probably few politicians or citizens-are by now aware of the ""paradox of voting"" and may have some sense of its connection with these questions of decision, procedure, and strategy. Over the past decade or so a somewhat technical literature on the theory of voting has developed in the ""public choice"" area. The present article adds to this literature by presenting a number of new propositions concerning majority voting under two common voting procedures. These propositions pertain to the questions alluded to in the first paragraph. These new results, together with some more familiar ones, are obtained by employing language, concepts, and theorems from the mathematical theory of directed graphs. In these respects, the article will be of interest primarily to specialists in the area *This article is in part a combination and revision of two unpublished papers:",769,1977.0,http://www.goodwin.com/categories/categories/tagsmain.asp,Sociology
2387,725aa166223bf01ab21fb6b002b1e7f13b626d82,Spectra of graphs : theory and application,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,78-145,1995.0,http://www.lee.biz/tags/categoryterms.htm,Mathematics
2388,ffb03ba337560ccf1204e57e1d1bb831c55e21bf,Graph-theoretic connectivity control of mobile robot networks,"In this paper, we provide a theoretical framework for controlling graph connectivity in mobile robot networks. We discuss proximity-based communication models composed of disk-based or uniformly-fading-signal-strength communication links. A graph-theoretic definition of connectivity is provided, as well as an equivalent definition based on algebraic graph theory, which employs the adjacency and Laplacian matrices of the graph and their spectral properties. Based on these results, we discuss centralized and distributed algorithms to maintain, increase, and control connectivity in mobile robot networks. The various approaches discussed in this paper range from convex optimization and subgradient-descent algorithms, for the maximization of the algebraic connectivity of the network, to potential fields and hybrid systems that maintain communication links or control the network topology in a least restrictive manner. Common to these approaches is the use of mobility to control the topology of the underlying communication network. We discuss applications of connectivity control to multirobot rendezvous, flocking and formation control, where so far, network connectivity has been considered an assumption.",1525-1540,2011.0,http://nguyen.com/category/wp-contentabout.html,Computer Science
2389,21877990650a820ac946f5fd2a2cf8085114ae73,Exploratory Factor Analysis,"Selection of factors to be extracted: Theory is the first criteria to determine the number of factors to be extracted. From theory, we know that the number of factors extracted does make sense. Most researchers use the Eigenvalue criteria for the number of factors to be extracted. Value of the percentage and variance explained method is also used for exploratory factor analysis. We can use the scree test criteria for the selection of factors. In this method, Eigenvalue is plotted on a graph and factors are selected.",92-132,2022.0,https://www.edwards-sims.com/searchfaq.php,Technology
2390,b8af507417e61be3ab2ba21a9d8c6a8174bb1be6,Subgraph frequencies: mapping the empirical and extremal geography of large graph collections,"A growing set of on-line applications are generating data that can be viewed as very large collections of small, dense social graphs --- these range from sets of social groups, events, or collaboration projects to the vast collection of graph neighborhoods in large social networks. A natural question is how to usefully define a domain-independent 'coordinate system' for such a collection of graphs, so that the set of possible structures can be compactly represented and understood within a common space. In this work, we draw on the theory of graph homomorphisms to formulate and analyze such a representation, based on computing the frequencies of small induced subgraphs within each graph. We find that the space of subgraph frequencies is governed both by its combinatorial properties --- based on extremal results that constrain all graphs --- as well as by its empirical properties --- manifested in the way that real social graphs appear to lie near a simple one-dimensional curve through this space. We develop flexible frameworks for studying each of these aspects. For capturing empirical properties, we characterize a simple stochastic generative model, a single-parameter extension of Erdos-Renyi random graphs, whose stationary distribution over subgraphs closely tracks the one-dimensional concentration of the real social graph families. For the extremal properties, we develop a tractable linear program for bounding the feasible space of subgraph frequencies by harnessing a toolkit of known extremal graph theory. Together, these two complementary frameworks shed light on a fundamental question pertaining to social graphs: what properties of social graphs are 'social' properties and what properties are 'graph' properties? We conclude with a brief demonstration of how the coordinate system we examine can also be used to perform classification tasks, distinguishing between structures arising from different types of social graphs.",84-111,2013.0,https://wagner.com/tagregister.php,Computer Science
2391,31fa50edbabd71611da501f6246d92c2562c4b46,Graph Ramsey theory and the polynomial hierarchy,"Summary form only given, as follows. In the Ramsey theory of graphs F/spl rarr/(G, H) means that for every way of coloring the edges of F red and blue F will contain either a red G or a blue H as a subgraph. The problem ARROWING of deciding whether F/spl rarr/(G, H) lies in /spl Pi//sub 2//sup P/=coNP/sup NP/ and it was shown to be coNP-hard by S.A. Burr (1990). We prove that ARROWING is actually /spl Pi//sub 2//sup P/-complete, simultaneously settling a conjecture of Burr and providing a natural example of a problem complete for a higher level of the polynomial hierarchy. We also consider several specific variants of ARROWING, where G and H are restricted to particular families of graphs. We have a general completeness result for this case under the assumption that certain graphs are constructible in polynomial time. Furthermore we show that STRONG ARROWING, the version of ARROWING for induced subgraphs, is /spl Pi//sub 2//sup P/-complete.",6-,1999.0,https://newton-santiago.org/tag/categorysearch.php,Mathematics
2392,a968620dcf399d7d7f89f98dc1d3da8c114dac8d,"MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",,1367-1372,2008.0,https://roberts-martinez.net/list/categorieshome.asp,Chemistry
2393,764afc61d329400886d3d027c9d50d97c431f7c8,Multibond graph elements in physical systems theory,,1-36,1985.0,https://parrish.com/tags/tagpost.php,Mathematics
2394,dce8146987557735a19771aefa1f027211a2c275,Statistical mechanics of complex networks,"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. 
Traditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. 
The scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. 
The non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology.",27-141,2001.0,http://www.anthony.info/category/main/mainlogin.html,Computer Science
2395,09350843d21273d36caf22675cb588f88c518462,Estimating and understanding exponential random graph models,"We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ``practically'' ill-posed. We give the first rigorous proofs of ``degeneracy'' observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erd\H{o}s-R\'{e}nyi model. We also find classes of models where the limiting graphs differ from Erd\H{o}s-R\'{e}nyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs.",2428-2461,2011.0,https://palmer.com/listhome.html,Mathematics
2396,aa6be519b394b44ab24c6ad964f8a2c6a9b23571,Consensus and Cooperation in Networked Multi-Agent Systems,"This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations",215-233,2007.0,https://kim.com/listhomepage.jsp,Computer Science
2397,0e21202ae9085bff250fd60ce0fcf54438c95ab6,On the algebraic theory of graph colorings,,15-50,1966.0,https://kelly.com/categories/category/categoriesterms.php,Mathematics
2398,7b00a2ae0c927a3d3dd875dc35f9a777d74a8f3e,A homology theory for spanning tress of a graph,,241-251,1977.0,https://johnson.org/search/explore/exploreprivacy.jsp,Mathematics
2399,a55df99552d6093b6150fe2ca79d017644b73935,BrainNet Viewer: A Network Visualization Tool for Human Brain Connectomics,"The human brain is a complex system whose topological organization can be represented using connectomics. Recent studies have shown that human connectomes can be constructed using various neuroimaging technologies and further characterized using sophisticated analytic strategies, such as graph theory. These methods reveal the intriguing topological architectures of human brain networks in healthy populations and explore the changes throughout normal development and aging and under various pathological conditions. However, given the huge complexity of this methodology, toolboxes for graph-based network visualization are still lacking. Here, using MATLAB with a graphical user interface (GUI), we developed a graph-theoretical network visualization toolbox, called BrainNet Viewer, to illustrate human connectomes as ball-and-stick models. Within this toolbox, several combinations of defined files with connectome information can be loaded to display different combinations of brain surface, nodes and edges. In addition, display properties, such as the color and size of network elements or the layout of the figure, can be adjusted within a comprehensive but easy-to-use settings panel. Moreover, BrainNet Viewer draws the brain surface, nodes and edges in sequence and displays brain networks in multiple views, as required by the user. The figure can be manipulated with certain interaction functions to display more detailed information. Furthermore, the figures can be exported as commonly used image file formats or demonstration video for further use. BrainNet Viewer helps researchers to visualize brain networks in an easy, flexible and quick manner, and this software is freely available on the NITRC website (www.nitrc.org/projects/bnv/).",65-150,2013.0,https://www.barker.com/wp-content/tagmain.html,Computer Science
2400,c52f657964a8014e72d925768e2dcab23819abc5,Social Network Analysis,"This paper reports on the development of social network analysis, tracing its origins in classical sociology and its more recent formulation in social scientific and mathematical work. It is argued that the concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be discerned. Social network analysis has been used in studies of kinship structure, social mobility, science citations, contacts among members of deviant groups, corporate power, international trade exploitation, class structure, and many other areas. A review of the formal models proposed in graph theory, multidimensional scaling, and algebraic topology is followed by extended illustrations of social network analysis in the study of community structure and interlocking directorships.",109 - 127,1988.0,https://www.cortez.info/category/wp-content/tagsregister.jsp,Sociology
2401,01fb1a96b1e023b7f1f5e7af700765b5a2b1998c,Variable Neighborhood Search,,759-787,2018.0,https://mcpherson.biz/tags/blog/bloglogin.htm,Computer Science
2402,fc45cd4563ce8ab02a5fe5facb22ea69864d9ef0,Information Theory and Network Coding,"This book contains a thorough discussion of the classical topics in information theory together with the first comprehensive treatment of network coding, a subject first emerged under information theory in the mid 1990's that has now diffused into coding theory, computer networks, wireless communications, complexity theory, cryptography, graph theory, etc. With a large number of examples, illustrations, and original problems, this book is excellent as a textbook or reference book for a senior or graduate level course on the subject, as well as a reference for researchers in related fields.",75-124,2008.0,https://www.franklin.com/posts/explore/categoryfaq.php,Computer Science
2403,4060c1e0491f3cc34f77bd623592799d04f5c78a,Graph models of habitat mosaics.,"Graph theory is a body of mathematics dealing with problems of connectivity, flow, and routing in networks ranging from social groups to computer networks. Recently, network applications have erupted in many fields, and graph models are now being applied in landscape ecology and conservation biology, particularly for applications couched in metapopulation theory. In these applications, graph nodes represent habitat patches or local populations and links indicate functional connections among populations (i.e. via dispersal). Graphs are models of more complicated real systems, and so it is appropriate to review these applications from the perspective of modelling in general. Here we review recent applications of network theory to habitat patches in landscape mosaics. We consider (1) the conceptual model underlying these applications; (2) formalization and implementation of the graph model; (3) model parameterization; (4) model testing, insights, and predictions available through graph analyses; and (5) potential implications for conservation biology and related applications. In general, and for a variety of ecological systems, we find the graph model a remarkably robust framework for applications concerned with habitat connectivity. We close with suggestions for further work on the parameterization and validation of graph models, and point to some promising analytic insights.","
          260-73
        ",2009.0,http://www.mayo.com/exploremain.html,Computer Science
2404,88496bd36dd61ca42dbd5020d23e76ebeaa994a4,Information flow and cooperative control of vehicle formations,"We consider the problem of cooperation among a collection of vehicles performing a shared task using intervehicle communication to coordinate their actions. Tools from algebraic graph theory prove useful in modeling the communication network and relating its topology to formation stability. We prove a Nyquist criterion that uses the eigenvalues of the graph Laplacian matrix to determine the effect of the communication topology on formation stability. We also propose a method for decentralized information exchange between vehicles. This approach realizes a dynamical system that supplies each vehicle with a common reference to be used for cooperative motion. We prove a separation principle that decomposes formation stability into two components: Stability of this is achieved information flow for the given graph and stability of an individual vehicle for the given controller. The information flow can thus be rendered highly robust to changes in the graph, enabling tight formation control despite limitations in intervehicle communication capability.",1465-1476,2004.0,http://campbell-hudson.biz/maincategory.html,Mathematics
2405,cc2e6e4fde1560bd4839a8cb47b00899cf146a31,Large Networks and Graph Limits,"The book Large Networks and Graph Limits, xiv + 475 pp., published in late 2012, comprises five parts, the first an illuminating introduction and the last a tantalizing taste of how the scope of the theory developed in its pages might be extended to other combinatorial structures than graphs. The three central parts treat in depth the topics of graph algebras, limits for sequences of dense graphs (this constitutes the most substantial part, occupying nearly half the book) and limits for sequences of bounded degree graphs. Primarily the book is aimed at graduate students and research mathematicians interested in graph theory and its application to networks (for example, the internet and networks in social science, biology, statistical physics and engineering). There are 23 chapters and an appendix, the latter conveniently giving necessary background from areas of mathematics outside mainstream graph theory. A bibliography collects together the extensive research in this area up to 2012, and a subject, author and notation index facilitate navigation of the book. The author maintains a webpage for corrections and supplementary material. Indeed, via the author’s homepage the reader can freely access the many papers he has written with collaborators on the topic of graph homomorphisms and graph limits. The book synthesizes much of the material in these papers, with some revision in",55-113,,https://www.morgan-bryant.com/tag/wp-contentregister.jsp,Technology
2406,5d2f80db3312b9d203f2ed9877f0e6971f4bf7dc,Discrete signal processing on graphs: Graph fourier transform,"We propose a novel discrete signal processing framework for the representation and analysis of datasets with complex structure. Such datasets arise in many social, economic, biological, and physical networks. Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs, so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges. We discuss the notions of signals and filters on graphs, and define the concepts of the spectrum and Fourier transform for graph signals. We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix and study their properties. As a potential application of the graph Fourier transform, we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain.",6167-6170,2013.0,https://mcconnell.com/search/posts/appterms.php,Mathematics
2407,e1a50831ee71998ca4c577bb996e6353c5eb2d4a,Connectedness Index of uncertain Graph,"In practical applications of graph theory, non-deterministic factors are frequently encountered. This paper employs uncertainty theory to deal with non-deterministic factors in problems of graph connectivity. The concepts of uncertain graph and connectedness index of uncertain graph are proposed in this paper. It presents two algorithms to calculate connectedness index of an uncertain graph.",127-138,2013.0,http://www.johnson.com/mainterms.php,Mathematics
2408,2e64c17ae0799e3995e927ec46b31e03ce26aa76,Evolutionary Dynamics: Exploring the Equations of Life,Preface 1. Introduction 2. What Evolution Is 3. Fitness Landscapes and Sequence Spaces 4. Evolutionary Games 5. Prisoners of the Dilemma 6. Finite Populations 7. Games in Finite Populations 8. Evolutionary Graph Theory 9. Spatial Games 10. HIV Infection 11. The Evolution of Virulence 12. The Evolutionary Dynamics of Cancer 13. Language Evolution 14. Conclusion Further Reading References Index,35-119,2006.0,https://www.howard-beck.com/wp-contenthomepage.html,Mathematics
2409,acfd9ea27a4183cc6ae1d74998e2e1e0c9e98093,Computing topological parameters of biological networks,"UNLABELLED
Rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. In order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile Cytoscape plugin NetworkAnalyzer. It computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. NetworkAnalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. It is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.


AVAILABILITY
NetworkAnalyzer can be downloaded via the Cytoscape web site: http://www.cytoscape.org","
          282-4
        ",2008.0,http://www.robinson-sullivan.com/blogpost.php,Computer Science
2410,98f84b38956ece37082c7bc2a8282f47454f4427,"Functional Analysis, Sobolev Spaces and Partial Differential Equations",,87-142,2010.0,https://berger-sanford.info/search/tag/wp-contentterms.htm,Mathematics
2411,ef32407a7947a1051c7ecdcdeb857ed835bbed99,Rigid graph control architectures for autonomous formations,"This article sets out the rudiments of a theory for analyzing and creating architectures appropriate to the control of formations of autonomous vehicles. The theory rests on ideas of rigid graph theory, some but not all of which are old. The theory, however, has some gaps in it, and their elimination would help in applications. Some of the gaps in the relevant graph theory are as follows. First, there is as yet no analogue for three-dimensional graphs of Laman's theorem, which provides a combinatorial criterion for rigidity in two-dimensional graphs. Second, for three-dimensional graphs there is no analogue of the two-dimensional Henneberg construction for growing or deconstructing minimally rigid graphs although there are conjectures. Third, global rigidity can easily be characterized for two-dimensional graphs, but not for three-dimensional graphs.",84-126,2008.0,https://www.watson-harvey.info/search/categoriesmain.htm,Mathematics
2412,d2b93dfbe50f3c642c64b8ea581cb6e449c71f82,Nonlocal Operators with Applications to Image Processing,"We propose the use of nonlocal operators to define new types of flows and functionals for image processing and elsewhere. A main advantage over classical PDE-based algorithms is the ability to handle better textures and repetitive structures. This topic can be viewed as an extension of spectral graph theory and the diffusion geometry framework to functional analysis and PDE-like evolutions. Some possible applications and numerical examples are given, as is a general framework for approximating Hamilton–Jacobi equations on arbitrary grids in high demensions, e.g., for control theory.",1005-1028,2008.0,https://www.fowler.com/tagauthor.jsp,Mathematics
2413,636b0754486f29f0bbc68cc2b410f564a3dfefe6,Spectra of Graphs,"This book gives an elementary treatment of the basic material about graph spectra, both for ordinary, and Laplace and Seidel spectra. The text progresses systematically, by covering standard topics before presenting some new material on trees, strongly regular graphs, two-graphs, association schemes, p-ranks of configurations and similar topics. Exercises at the end of each chapter provide practice and vary from easy yet interesting applications of the treated theory, to little excursions into related topics. Tables, references at the end of the book, an author and subject index enrich the text. Spectra of Graphs is written for researchers, teachers and graduate students interested in graph spectra. The reader is assumed to be familiar with basic linear algebra and eigenvalues, although some more advanced topics in linear algebra, like the Perron-Frobenius theorem and eigenvalue interlacing are included.",93-108,2011.0,https://www.munoz.com/category/tagabout.htm,Computer Science
2414,9d061f8cc3cb4eb77579adcdbe99169b3e839b27,Graph Degree Linkage: Agglomerative Clustering on a Directed Graph,,428-441,2012.0,http://adams-stewart.info/mainpost.html,Computer Science
2415,86a8e3b54eb7b0dd8076d73494f5c82f853ab860,A Theory of Graphs,,423-460,1993.0,http://taylor.com/categoriesmain.html,Computer Science
2416,b9dd445a4e7ad794012db01339f8fe9967b923a8,Consensus Problems on Networks With Antagonistic Interactions,"In a consensus protocol an agreement among agents is achieved thanks to the collaborative efforts of all agents, expresses by a communication graph with nonnegative weights. The question we ask in this paper is the following: is it possible to achieve a form of agreement also in presence of antagonistic interactions, modeled as negative weights on the communication graph? The answer to this question is affirmative: on signed networks all agents can converge to a consensus value which is the same for all agents except for the sign. Necessary and sufficient conditions are obtained to describe cases in which this is possible. These conditions have strong analogies with the theory of monotone systems. Linear and nonlinear Laplacian feedback designs are proposed.",935-946,2013.0,http://www.lopez.com/tags/blog/tagpost.html,Mathematics
2417,7a498d6cef22d72dc6bf8da90f145ea8f5d9fece,Wiener Index of Trees: Theory and Applications,,211-249,2001.0,http://www.baker.com/wp-content/category/wp-contentpost.htm,Mathematics
2418,221aa3be55a4ead8fc2aa83b12aac370bfba72f5,A Formal Basis for the Heuristic Determination of Minimum Cost Paths,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",100-107,1968.0,https://white.com/categories/wp-contenthomepage.php,Computer Science
2419,e8b8c5f4a81e11576ee2c74ab65c66a42bbad270,Random graphs with arbitrary degree distributions and their applications.,"Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.","
          026118
        ",2000.0,https://www.robinson.com/categories/tags/tagterms.php,Mathematics
2420,9ccf7b6cb32cf89752a35bd910555adac54773e0,The Knowledge Complexity of Interactive Proof Systems,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.In this paper a computational complexity theory of the “knowledge” contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable.",203-225,1989.0,https://mills-jordan.com/category/categoryprivacy.asp,Computer Science
2421,0b4497542279def00135e7a3140b3418606679e9,An Introduction to Hydrogen Bonding,"1. Brief History 2. Nature and Properties 3. Strong Hydrogen Bonds 4. Moderate Hydrogen Bonds 5. Weak Hydrogen Bonds 6. Cooperativity, Patterns, Graph Set Theory, Liquid Crystals 7. Disorder, Proton Transfer, Isotope Effect, Ferroelectrics, Transitions 8. Water, Water Dimers, Ices, Hydrates 9. Inclusion Compounds 10. Hydrogen Bonding in Biological Molecules 11. Methods",15-133,1997.0,https://www.sweeney.com/postshomepage.html,Chemistry
2422,9a84002359c777dc941e1465f148abe43cb3331f,Networks and epidemic models,"Networks and the epidemiology of directly transmitted infectious diseases are fundamentally linked. The foundations of epidemiology and early epidemiological models were based on population wide random-mixing, but in practice each individual has a finite set of contacts to whom they can pass infection; the ensemble of all such contacts forms a ‘mixing network’. Knowledge of the structure of the network allows models to compute the epidemic dynamics at the population scale from the individual-level behaviour of infections. Therefore, characteristics of mixing networks—and how these deviate from the random-mixing norm—have become important applied concerns that may enhance the understanding and prediction of epidemic patterns and intervention measures. Here, we review the basis of epidemiological theory (based on random-mixing models) and network theory (based on work from the social sciences and graph theory). We then describe a variety of methods that allow the mixing network, or an approximation to the network, to be ascertained. It is often the case that time and resources limit our ability to accurately find all connections within a network, and hence a generic understanding of the relationship between network structure and disease dynamics is needed. Therefore, we review some of the variety of idealized network types and approximation techniques that have been utilized to elucidate this link. Finally, we look to the future to suggest how the two fields of network theory and epidemiological modelling can deliver an improved understanding of disease dynamics and better public health through effective disease control.",295 - 307,2005.0,https://norris-martinez.com/postsprivacy.html,Medicine
2423,c3d47d49e7c46d2543e92a26b964f42b25371ba4,Geometric Algorithms and Combinatorial Optimization,,52-146,1988.0,http://king.info/categoryhome.htm,Mathematics
2424,088ab372dff19fb8837d0f48d395d1a987251f3f,Geometry of cuts and metrics,,"I-XII, 1-587",2009.0,http://abbott.info/search/categoryindex.asp,Computer Science
2425,59d86a93c4ef54b5489bc375cd02e64205823f42,Random Walks for Image Segmentation,"A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs",1768-1783,2006.0,https://williams.com/blogregister.php,Computer Science
2426,7fe0ef2ddacd193101dc5ba3df97b0241a5e8fc6,Stability of multiagent systems with time-dependent communication links,"We study a simple but compelling model of network of agents interacting via time-dependent communication links. The model finds application in a variety of fields including synchronization, swarming and distributed decision making. In the model, each agent updates his current state based upon the current information received from neighboring agents. Necessary and/or sufficient conditions for the convergence of the individual agents' states to a common value are presented, thereby extending recent results reported in the literature. The stability analysis is based upon a blend of graph-theoretic and system-theoretic tools with the notion of convexity playing a central role. The analysis is integrated within a formal framework of set-valued Lyapunov theory, which may be of independent interest. Among others, it is observed that more communication does not necessarily lead to faster convergence and may eventually even lead to a loss of convergence, even for the simple models discussed in the present paper.",169-182,2005.0,http://www.weber-hays.com/tagshome.asp,Mathematics
2427,1c8003c27d0022f241b42a1d5ca12b85e44726e6,Constructing free-energy approximations and generalized belief propagation algorithms,"Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a ""valid"" or ""maxent-normal"" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the ""Bethe method"", the ""junction graph method"", the ""cluster variation method"", and the ""region graph method"". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.",2282-2312,2005.0,http://www.frost.com/blog/explore/postsabout.php,Mathematics
2428,c6b745c7ecc3fc89d0df71727e1a0f456be7187a,Brain graphs: graphical models of the human brain connectome.,"Brain graphs provide a relatively simple and increasingly popular way of modeling the human brain connectome, using graph theory to abstractly define a nervous system as a set of nodes (denoting anatomical regions or recording electrodes) and interconnecting edges (denoting structural or functional connections). Topological and geometrical properties of these graphs can be measured and compared to random graphs and to graphs derived from other neuroscience data or other (nonneural) complex systems. Both structural and functional human brain graphs have consistently demonstrated key topological properties such as small-worldness, modularity, and heterogeneous degree distributions. Brain graphs are also physically embedded so as to nearly minimize wiring cost, a key geometric property. Here we offer a conceptual review and methodological guide to graphical analysis of human neuroimaging data, with an emphasis on some of the key assumptions, issues, and trade-offs facing the investigator.","
          113-40
        ",2011.0,https://www.grant.com/main/wp-content/listsearch.htm,Computer Science
2429,a8a18497987e8b4715cba7cd6d2f8e6a1d58b2fa,The small world of the cerebral cortex,,145-162,2007.0,http://www.floyd-singleton.com/posts/app/applogin.html,Mathematics
2430,eb524f7c1e29bdd7d27c33a90921c5ea7f347234,Frequency assignment: Theory and applications,"In this paper we introduce the minimum-order approach to frequency assignment and present a theory which relates this approach to the traditional one. This new approach is potentially more desirable than the traditional one. We model assignment problems as both frequency-distance constrained and frequency constrained optimization problems. The frequency constrained approach should be avoided if distance separation is employed to mitigate interference. A restricted class of graphs, called disk graphs, plays a central role in frequency-distance constrained problems. We introduce two generalizations of chromatic number and show that many frequency assignment problems are equivalent to generalized graph coloring problems. Using these equivalences and recent results concerning the complexity of graph coloring, we classify many frequency assignment problems according to the ""execution time efficiency"" of algorithms that may be devised for their solution. We discuss applications to important real world problems and identify areas for further work.",1497-1514,1980.0,http://duke.com/app/appprivacy.htm,Mathematics
2431,e507a66243223b83c50ec8609c8e2db5a99277a7,A Spectral Graph Uncertainty Principle,"The spectral theory of graphs provides a bridge between classical signal processing and the nascent field of graph signal processing. In this paper, a spectral graph analogy to Heisenberg's celebrated uncertainty principle is developed. Just as the classical result provides a tradeoff between signal localization in time and frequency, this result provides a fundamental tradeoff between a signal's localization on a graph and in its spectral domain. Using the eigenvectors of the graph Laplacian as a surrogate Fourier basis, quantitative definitions of graph and spectral “spreads” are given, and a complete characterization of the feasibility region of these two quantities is developed. In particular, the lower boundary of the region, referred to as the uncertainty curve, is shown to be achieved by eigenvectors associated with the smallest eigenvalues of an affine family of matrices. The convexity of the uncertainty curve allows it to be found to within ε by a fast approximation algorithm requiring O(ε-1/2) typically sparse eigenvalue evaluations. Closed-form expressions for the uncertainty curves for some special classes of graphs are derived, and an accurate analytical approximation for the expected uncertainty curve of Erd-s-Rényi random graphs is developed. These theoretical results are validated by numerical experiments, which also reveal an intriguing connection between diffusion processes on graphs and the uncertainty bounds.",4338-4356,2012.0,https://williams-walls.com/wp-content/taghome.html,Computer Science
2432,ada56e1f7575d7f542215c48625c161ab060bed0,Static scheduling algorithms for allocating directed task graphs to multiprocessors,"Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.",406-471,1999.0,https://johnson-graham.com/category/exploreterms.htm,Computer Science
2433,a79d1c0f6e8bee2ca0cab3522e67b27a533e19e3,On maximizing the second smallest eigenvalue of a state-dependent graph Laplacian,"We consider the set G consisting of graphs of fixed order and weighted edges. The vertex set of graphs in G will correspond to point masses and the weight for an edge between two vertices is a functional of the distance between them. We pose the problem of finding the best vertex positional configuration in the presence of an additional proximity constraint, in the sense that, the second smallest eigenvalue of the corresponding graph Laplacian is maximized. In many recent applications of algebraic graph theory in systems and control, the second smallest eigenvalue of Laplacian has emerged as a critical parameter that influences the stability and robustness properties of dynamic systems that operate over an information network. Our motivation in the present work is to ""assign"" this Laplacian eigenvalue when relative positions of various elements dictate the interconnection of the underlying weighted graph. In this venue, one would then be able to ""synthesize"" information graphs that have desirable system theoretic properties.",116-120,2006.0,http://dickerson-harris.biz/posts/category/categoriesabout.jsp,Computer Science
2434,1db1447bc61a68500ec31e94daf27cf057831f83,Shock Waves on the Highway,A simple theory of traffic flow is developed by replacing individual vehicles with a continuous “fluid” density and applying an empirical relation between speed and density. Characteristic features of the resulting theory are a simple “graph-shearing” process for following the development of traffic waves in time and the frequent appearance of shock waves. The effect of a traffic signal on traffic streams is studied and found to exhibit a threshold effect wherein the disturbances are minor for light traffic but suddenly build to large values when a critical density is exceeded.,42-51,1956.0,http://chaney.com/app/search/wp-contentsearch.html,Mathematics
2435,2e15e356a1368e65b42417676276160110daf272,Second-Order Consensus for Multiagent Systems With Directed Topologies and Nonlinear Dynamics,"This paper considers a second-order consensus problem for multiagent systems with nonlinear dynamics and directed topologies where each agent is governed by both position and velocity consensus terms with a time-varying asymptotic velocity. To describe the system's ability for reaching consensus, a new concept about the generalized algebraic connectivity is defined for strongly connected networks and then extended to the strongly connected components of the directed network containing a spanning tree. Some sufficient conditions are derived for reaching second-order consensus in multiagent systems with nonlinear dynamics based on algebraic graph theory, matrix theory, and Lyapunov control approach. Finally, simulation examples are given to verify the theoretical analysis.",881-891,2010.0,http://www.reese.com/tag/listpost.php,Computer Science
2436,c6e311a9640b47b9264e13de867ab6cc34d195db,Markov Decision Processes,,642-646,2004.0,http://www.massey-ingram.com/main/tagfaq.htm,Computer Science
2437,b79267cfe6881e05275cfa14aa3f7af0e6ae9e10,Graph removal lemmas,"The graph removal lemma states that any graph on n vertices with o(n^{v(H)}) copies of a fixed graph H may be made H-free by removing o(n^2) edges. Despite its innocent appearance, this lemma and its extensions have several important consequences in number theory, discrete geometry, graph theory and computer science. In this survey we discuss these lemmas, focusing in particular on recent improvements to their quantitative aspects.",1-50,2012.0,https://peters.org/appregister.asp,Computer Science
2438,439f26a0938b1c4617655d0f951eda77033aedbe,Introduction to Quantum Graphs,"A ""quantum graph"" is a graph considered as a one-dimensional complex and equipped with a differential operator (""Hamiltonian""). Quantum graphs arise naturally as simplified models in mathematics, physics, chemistry, and engineering when one considers propagation of waves of various nature through a quasi-one-dimensional (e.g., ""meso-"" or ""nano-scale"") system that looks like a thin neighborhood of a graph. Works that currently would be classified as discussing quantum graphs have been appearing since at least the 1930s, and since then, quantum graphs techniques have been applied successfully in various areas of mathematical physics, mathematics in general and its applications. One can mention, for instance, dynamical systems theory, control theory, quantum chaos, Anderson localization, microelectronics, photonic crystals, physical chemistry, nano-sciences, superconductivity theory, etc. Quantum graphs present many non-trivial mathematical challenges, which makes them dear to a mathematician's heart. Work on quantum graphs has brought together tools and intuition coming from graph theory, combinatorics, mathematical physics, PDEs, and spectral theory. This book provides a comprehensive introduction to the topic, collecting the main notions and techniques. It also contains a survey of the current state of the quantum graph research and applications.",87-131,2012.0,https://brown.net/wp-contentabout.htm,Mathematics
2439,e5a0fff54abb5eca5e61f2b8d73a5f2acaad6c3a,Topological quantum chemistry,,298-305,2017.0,http://johnson-reid.com/tags/categories/searchindex.php,Physics
2440,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",80-90,2014.0,http://www.brown.net/list/wp-content/wp-contentindex.htm,Computer Science
2441,3e502fb40768c140ef24ea742a212c263f380f71,Modeling and control of formations of nonholonomic mobile robots,"This paper addresses the control of a team of nonholonomic mobile robots navigating in a terrain with obstacles while maintaining a desired formation and changing formations when required, using graph theory. We model the team as a triple, (g, r, H), consisting of a group element g that describes the gross position of the lead robot, a set of shape variables r that describe the relative positions of robots, and a control graph H that describes the behaviors of the robots in the formation. Our framework enables the representation and enumeration of possible control graphs and the coordination of transitions between any two formations.",905-908,2001.0,http://bradley.biz/app/tagsprivacy.php,Computer Science
2442,56a44211b353f02b4f9dd31369edde8e3804b4d6,"ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.","We describe some of the capabilities of the ergm package and the statistical theory underlying it. This package contains tools for accomplishing three important, and interrelated, tasks involving exponential-family random graph models (ERGMs): estimation, simulation, and goodness of fit. More precisely, ergm has the capability of approximating a maximum likelihood estimator for an ERGM given a network data set; simulating new network data sets from a fitted ERGM using Markov chain Monte Carlo; and assessing how well a fitted ERGM does at capturing characteristics of a particular network data set.","
          nihpa54860
        ",2008.0,http://robinson-hale.net/main/categoriesmain.html,Computer Science
2443,83b205bb0de66586214bdaabac4b1ec67e24fe0f,the Effect of,"This research is a descriptive quantitative study, which aims to determine whether there is an influence of mathematics anxiety on the learning interest of high school students / equivalent during the Pandemic in Medan City. With research subjects, Indonesian Private Vocational High School students built 1 Medan, Indonesian Private Vocational Schools built 2 Medan, SMK Negeri 1 Medan, SMK Negeri 2 Medan, and Methodist SMA 12 Medan. The research instrument used in this study was a non-test instrument. The instrument used in this study was a questionnaire consisting of a mathematics anxiety questionnaire and a learning interest questionnaire. The questionnaire used is a closed direct questionnaire where alternative answers are available in the questionnaire, so that the respondent only needs to choose one answer that suits his condition. Based on the test results, it was found that mathematics anxiety was 61% and students' interest in learning was 67%. So it can be concluded that there is no effect of math anxiety on students' interest in learning.",72-119,2004.0,https://williams-bass.com/app/list/postslogin.asp,Technology
2444,b8523a1ae8f7ebc53b2af47b6541f6348383c0f8,Challenges with graph interpretation: a review of the literature,"With the growing emphasis on the development of scientific inquiry skills, the display and interpretation of data are becoming increasingly important. Graph interpretation competence is, in fact, essential to understanding today’s world and to be scientifically literate. However, graph interpretation is a complex and challenging activity. Graph interpretation competence is affected by many factors, including aspects of graph characteristics, the content of the graph and viewers’ prior knowledge. For instance, the prior theory and expectations that students have may lead to biases and misinterpretation of graphs. One basic controversy that remains unanswered, for example, is what should we teach first in order to make students scientific literate, how to graph or how to interpret a graph? If it is the case that the ability to interpret a graph be developed prior to the ability to create, then it is important to understand what graph interpretation entails. This paper reviews current literature on graph interpretation competence and argues that it should be explicitly taught given its importance and its complexity.",183 - 210,2011.0,https://www.rodriguez-miller.com/explore/list/wp-contentauthor.html,Computer Science
2445,e1b10e80013766521e82bc56babaab63c2265847,Renormalization in Quantum Field Theory and the Riemann–Hilbert Problem I: The Hopf Algebra Structure of Graphs and the Main Theorem,,249-273,1999.0,http://holland-parker.com/main/posts/explorelogin.htm,Physics
2446,1e890895a38fe79be13636e563ea669ea63133e1,Applications of Hyperstructure Theory,,61-122,2010.0,https://www.miller.biz/wp-contentauthor.html,Mathematics
2447,afc34303fcd5a4175f33d5161eb056826f64b880,Kron Reduction of Graphs With Applications to Electrical Networks,"Consider a weighted undirected graph and its corresponding Laplacian matrix, possibly augmented with additional diagonal elements corresponding to self-loops. The Kron reduction of this graph is again a graph whose Laplacian matrix is obtained by the Schur complement of the original Laplacian matrix with respect to a specified subset of nodes. The Kron reduction process is ubiquitous in classic circuit theory and in related disciplines such as electrical impedance tomography, smart grid monitoring, transient stability assessment, and analysis of power electronics. Kron reduction is also relevant in other physical domains, in computational applications, and in the reduction of Markov chains. Related concepts have also been studied as purely theoretic problems in the literature on linear algebra. In this paper we analyze the Kron reduction process from the viewpoint of algebraic graph theory. Specifically, we provide a comprehensive and detailed graph-theoretic analysis of Kron reduction encompassing topological, algebraic, spectral, resistive, and sensitivity analyses. Throughout our theoretic elaborations we especially emphasize the practical applicability of our results to various problem setups arising in engineering, computation, and linear algebra. Our analysis of Kron reduction leads to novel insights both on the mathematical and the physical side.",150-163,2011.0,http://hale-hart.com/blog/searchauthor.html,Computer Science
2448,78bb1ded151a2674d634d04d717d458339b7cb2c,Spectral redemption in clustering sparse networks,"Significance Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a “nonbacktracking” matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices. Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.",20935 - 20940,2013.0,https://adkins.com/app/app/apphomepage.html,Computer Science
2449,ca47be74efccb005d88f8455aff73e0622949e96,Mathematical Concepts in Organic Chemistry,,24-124,1986.0,https://hess-bolton.com/explorepost.html,Mathematics
2450,2c03d0e5113cc34ff607c652c68a5e542e607735,Property testing and its connection to learning and approximation,"The authors study the question of determining whether an unknown function has a particular property or is /spl epsiv/-far from any function with that property. A property testing algorithm is given a sample of the value of the function on instances drawn according to some distribution, and possibly may query the function on instances of its choice. First, they establish some connections between property testing and problems in learning theory. Next, they focus on testing graph properties, and devise algorithms to test whether a graph has properties such as being k-colorable or having a /spl rho/-clique (clique of density /spl rho/ w.r.t. the vertex set). The graph property testing algorithms are probabilistic and make assertions which are correct with high probability utilizing only poly(1//spl epsiv/) edge-queries into the graph, where /spl epsiv/ is the distance parameter. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph which correspond to the property being tested, if it holds for the input graph.",339-348,1996.0,http://www.ferguson.com/list/postsmain.php,Computer Science
2451,b623b893faa4c6ef54ba87af04a970b8250c5274,"Sharp thresholds of graph properties, and the -sat problem","Consider G(n, p) to be the probability space of random graphs on n vertices with edge probability p. We will be considering subsets of this space defined by monotone graph properties. A monotone graph property P is a property of graphs such that a) P is invariant under graph automorphisims. b) If graph H has property P , then so does any graph G having H as a subgraph. A monotone symmetric family of graphs is a family defined by such a property. One of the first observations made about random graphs by Erdos and Renyi in their seminal work on random graph theory [12] was the existence of threshold phenomena, the fact that for many interesting properties P , the probability of P appearing in G(n, p) exhibits a sharp increase at a certain critical value of the parameter p. Bollobas and Thomason proved the existence of threshold functions for all monotone set properties ([6]), and in [14] it is shown that this behavior is quite general, and that all monotone graph properties exhibit threshold behavior, i.e. the probability of their appearance increases from values very close to 0 to values close to 1 in a very small interval. More precise analysis of the size of the threshold interval is done in [7]. This threshold behavior which occurs in various settings which arise in combinatorics and computer science is an instance of the phenomenon of phase transitions which is the subject of much interest in statistical physics. One of the main questions that arises in studying phase transitions is: how “sharp” is the transition? For example, one of the motivations for this paper arose from the question of the sharpness of the phase transition for the property of satisfiability of a random kCNF Boolean formula. Nati Linial, who introduced me to this problem, suggested that although much concrete analysis was being performed on this problem the best approach would be to find general conditions for sharpness of the phase transition, answering the question posed in [14] as to the relation between the length of the threshold interval and the value of the critical probability. In this paper we indeed introduce a simple condition and prove it is sufficient. Stated roughly, in the setting of random graphs, the main theorem states that if a property has a coarse threshold, then it can be approximated by the property of having certain given graphs as a subgraph. This condition can be applied in a more",1017-1054,1999.0,http://www.camacho.net/list/categorypost.html,Mathematics
2452,5ce9e830146f9e4b095511f693a939e749686430,A Theory of Network Localization,"In this paper, we provide a theoretical foundation for the problem of network localization in which some nodes know their locations and other nodes determine their locations by measuring the distances to their neighbors. We construct grounded graphs to model network localization and apply graph rigidity theory to test the conditions for unique localizability and to construct uniquely localizable networks. We further study the computational complexity of network localization and investigate a subclass of grounded graphs where localization can be computed efficiently. We conclude with a discussion of localization in sensor networks where the sensors are placed randomly",1663-1678,2006.0,https://www.chapman.com/categories/categoriesregister.html,Computer Science
2453,796cd1df17ac1eedbc504dd9eaf2f1ca30b8a6be,KEGGgraph: a graph approach to KEGG PATHWAY in R and bioconductor,"Motivation: KEGG PATHWAY is a service of Kyoto Encyclopedia of Genes and Genomes (KEGG), constructing manually curated pathway maps that represent current knowledge on biological networks in graph models. While valuable graph tools have been implemented in R/Bioconductor, to our knowledge there is currently no software package to parse and analyze KEGG pathways with graph theory. Results: We introduce the software package KEGGgraph in R and Bioconductor, an interface between KEGG pathways and graph models as well as a collection of tools for these graphs. Superior to existing approaches, KEGGgraph captures the pathway topology and allows further analysis or dissection of pathway graphs. We demonstrate the use of the package by the case study of analyzing human pancreatic cancer pathway. Availability:KEGGgraph is freely available at the Bioconductor web site (http://www.bioconductor.org). KGML files can be downloaded from KEGG FTP site (ftp://ftp.genome.jp/pub/kegg/xml). Contact: j.zhang@dkfz-heidelberg.de Supplementary information: Supplementary data are available at Bioinformatics online.",1470 - 1471,2009.0,https://chang.net/app/wp-content/appindex.asp,Computer Science
2454,6350d7697aa098fadc46296218223325076826a3,Topological properties of hypercubes,"The n-dimensional hypercube is a highly concurrent loosely coupled multiprocessor based on the binary n-cube topology. Machines based on the hypercube topology have been advocated as ideal parallel architectures for their powerful interconnection features. The authors examine the hypercube from the graph-theory point of view and consider those features that make its connectivity so appealing. Among other things, they propose a theoretical characterization of the n-cube as a graph and and show how to map various other topologies into a hypercube. >",867-872,1988.0,http://www.edwards.com/tag/category/categoryindex.htm,Computer Science
2455,015f6fcfe7fb621dd84fd24ea2beb35321896005,Simplifying multiloop integrands and ultraviolet divergences of gauge theory and gravity amplitudes,"We use the duality between color and kinematics to simplify the construction of the complete four-loop four-point amplitude of N = 4 super-Yang-Mills theory, including the nonplanar contributions. The duality completely determines the amplitude's integrand in terms of just two planar graphs. The existence of a manifestly dual gauge-theory amplitude trivializes the construction of the corresponding N = 8 supergravity integrand, whose graph numerators are double copies (squares) of the N = 4 super-Yang-Mills numerators. The success of this procedure provides further nontrivial evidence that the duality and double-copy properties hold at loop level. The new form of the four-loop four-point supergravity amplitude makes manifest the same ultraviolet power counting as the corresponding N = 4 super-Yang-Mills amplitude. We determine the amplitude's ultraviolet pole in the critical dimension of D = 11/2, the same dimension as for N = 4 super-Yang-Mills theory. Strikingly, exactly the same combination of vacuum integrals (after simplification) describes the ultraviolet divergence of N = 8 supergravity as the subleading-in-1/N{sub c}{sup 2} single-trace divergence in N = 4 super-Yang-Mills theory.",105014,2012.0,https://www.jimenez-alvarez.net/wp-contentlogin.html,Physics
2456,2d03b2d4cbd2c483a46c0c39b5fecdf407319eec,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,"Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.",692-700,2013.0,http://www.washington-gutierrez.com/listhomepage.php,Mathematics
2457,4dc403313b0fa80139fe6ac21802f0e1d16b772e,Spectral sparsification of graphs: theory and algorithms,"Graph sparsification is the approximation of an arbitrary graph by a sparse graph.
 We explain what it means for one graph to be a spectral approximation of another and review the development of algorithms for spectral sparsification. In addition to being an interesting concept, spectral sparsification has been an important tool in the design of nearly linear-time algorithms for solving systems of linear equations in symmetric, diagonally dominant matrices. The fast solution of these linear systems has already led to breakthrough results in combinatorial optimization, including a faster algorithm for finding approximate maximum flows and minimum cuts in an undirected network.",87-94,2013.0,http://www.smith.com/posts/categoriessearch.htm,Mathematics
2458,3d0e01922b2bf5ef53b8e7fc0885ab1f80a1d945,Topological Indices and Related Descriptors in QSAR and QSPR,"There are several different broad approaches to making correlations between chemical structure and some desired property or bioactivity (either of which is here spoken of simply as the “activity”). The relevance of such approaches to quantitative-structure -activity-relationships (QSAR) has now achieved widespread use, perhaps most especially in evaluating bioactivities (e.g., for drug development). One general approach to QSAR seeks to correlate a desired activity with another reference property (e.g., the octanol -water partition coefficient) which is more easily measured; a second general approach seeks to correlate a desired activity to quantum-chemically computed descriptors; and a third general approach seeks to correlate a desired activity with various “topological indices” (which in the mathematical graph-theory literature are usually referred to as “graph invariants”). Especially the first two approaches seem often to have been imagined to associate to a surmised mechanism giving rise to the desired activity, though for bioactivities mechanistic details are in practice often wanting. The present book is dedicated to the third sometimes somewhat controversial but now increasingly successful topological-index approach sindeed the book is focused not only on QSAR but also on quantitative structureproperty relations (QSPR) for other properties. The book consists of 17 chapters by several different leading practitioners in the field. There is a rational plan, with evident coordination between chapters, sometimes engendered by the participation of one of the editors. These chapters might be somewhat approximately divided into four categories: the first consisting of two or three introductory chapters; the second category consisting of six or so chapters presenting a diversity of topological indices; the third consisting of five or so chapters using particular chemical philosophies for the selection of the topological indices to be used; and the fourth consisting of three chapters attending to a few further computational problems and related strategies. The first introductory category includes chapters by Devillers, by Balaban and O. Ivanciuc, and by O. Ivanciuc and Balaban (though the last of these chapters could be included in the next category). Devillers gives some general history of QSAR relating to the other broad approaches not detailed in the present book and makes some related philosophical remarks. Balaban and Ivanciuc present a history focusing on the introductions of various topological indices. The third of these chapters presents a general review of chemical graph-theoretic rudiments, with nice illustrative examples. Multilinear regression analysis is a presumed prerequisite throughout the present book (much as it is also similarly utilized for other broad QSAR approaches outside the scope of the present book). The second set of chapters presenting a great variety of graph invariants includes chapters by O. Ivanciuc, T. Ivanciuc, and Balaban, by O. and T. Ivanciuc, by S. Nikolic ́, N. Trinajstić, and Z. Mihalić, by L. H. Hall and L. B. Kier, and by D. Bonchev (though the last chapter could be placed in the next category instead). Here the graph invariants may often be viewed as obtained in some manner from various graphtheoretic matrices, such as the adjacency, Laplacian, shortest-path distance, Szeged, path, or detour matrices as well as matrices derived therefrom by inverting the elements or “complementing” them or raising them to a power. Particularly in the first chapter of the present set of chapters, attention is directed to the incorporation of heteroatom and bond-weighting aspects of molecular graphs into the design of the topological indices, though such considerations appear in passing in other chapters of this set too, and in the subsequent categories of chapters such considerations are generally explicitly developed or already presumed. The third set of more tightly application-focused chapters are by E. Estrada, by Kier and Hall, by Hall and Kier, by S. C. Basak, by Devillers, and by J. E. Dubois, J. P. Doucet, A. Panaye, and B. T. Fan (though here the first two of these chapters could also be placed in the previous category). Often the topological indices in a chapter here become much restricted in comparison with the possibilities enunciated in the previous set of chapters, but the oft-major point then is that the topological indices are rationally selected within the chemical philosophical approach of the authors. For instance, Kier and Hall focus on their set of “kappa” indices for encoding global shape and flexibility information and their “electrotopological” indices for encoding mean local structural information (including electronegativity characteristics for the various atoms). And Dubois et al . focus on their ordering of substructural features to describe activities which are viewable to be determined by a local region as perturbed by the surrounding environment within the molecule. Basak also describes a method of average neighborhood analysis, using information-theory-related graph invariants. The fourth and final category of chapters concerned with miscellaneous additional problems and related computational strategies are by Basak, B. D. Gute, and G. D. Grunwald, by O. Ivanciuc, and by O. Ivanciuc and Devillers. The chapter by Basak incorporates a variety of extra-graph-theoretic (quantum-chemical or property) information in fitting for an activity. (There are a couple of other earlier chapters which also mention in passing the idea that various pieces of geometric information can be incorporated in indices otherwise resembling standard purely graph-theoretic topological indices.) The chapter by Ivanciuc concerns the use of neural networks to aid in identifying (especially nonlinear) correlations between activity and topological indices. The final chapter is an overview mentioning several available software packages. Overall this∼800-page book provides a reasonably comprehensive and fair presentation of the current field of use of topological indices for QSAR and QSPR. Throughout the book (in virtually every chapter) a variety of illustrative examples are developed, and resultant fits are noted. The book is certainly of value for anyone interested in QSAR and QSPR, regardless of whether the researcher is a practitioner of the topological-index approach or of one of the other oft-used approaches to predict activities. D. J. Klein Texas A&M Uni Versity/Gal Veston",93-116,2003.0,https://www.jenkins.biz/tag/main/wp-contentcategory.htm,Technology
2459,86ee21d690eec2a73806c2086949e944f8b46f7c,Automatic segmentation of seven retinal layers in SDOCT images congruent with expert manual segmentation,"Segmentation of anatomical and pathological structures in ophthalmic images is crucial for the diagnosis and study of ocular diseases. However, manual segmentation is often a time-consuming and subjective process. This paper presents an automatic approach for segmenting retinal layers in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Results show that this method accurately segments eight retinal layer boundaries in normal adult eyes more closely to an expert grader as compared to a second expert grader.",19413 - 19428,2010.0,http://white.biz/categories/blogregister.html,Medicine
2460,eac8fb1c9883da002a8a9fb1d514bde116219dc1,Resistance distance,,81-95,1993.0,https://kim-davis.info/blog/explorehome.html,Technology
2461,4fcfc3a3263d3d8487e887165ae4200cdac269c5,Graph limits and exchangeable random graphs,"We develop a clear connection between deFinetti's theorem for exchangeable arrays (work of Aldous{Hoover{Kallenberg) and the emerging area of graph limits (work of Lov asz and many coauthors). Along the way, we translate the graph theory into more classical prob- ability.",33-61,2007.0,http://shepherd.com/blog/app/tagsmain.php,Mathematics
2462,cc127ca14e04bb68cefe4848c4c0b8c5219309fd,The theory of the monetary circuit,"The present paper reviews the pre-history, the process of formation and the possible directions of future development of the theory of monetary circuit. The author reveals the main theoretical constructions of Graziany and the other leading representatives of the circuitist school. The principles of derivation of transaction and balance sheet matrices reflecting the main ideas of the theory are discussed. The dynamic variants of the theory as well as the connection between the circuitist approach and the input-output model are subject to examination. The paper studies the possibility the circuitist approach to be further broadened on the basis of the mathematical graphs theory. The author emphasizes that the theory of monetary circuit denies the neoclassical dichotomy and rejects the postulate of the neutrality of money. The opportunity is also offered to upgrade the monetary circuit theory by using the mathematical graph theory. The paper includes also a critical evaluation of the presented theory.",47-127,2013.0,http://www.harris-white.com/main/search/mainprivacy.php,Economics
2463,be4bebe5282233f1fa94a5d8a6fc6e452310a27b,Topological index based on the ratios of geometrical and arithmetical means of end-vertex degrees of edges,,1369-1376,2009.0,https://www.alexander.biz/explore/bloghome.html,Mathematics
2464,eff3e2a802a63b15ce57498611165eccca0ddbe3,The average distances in random graphs with given expected degrees,"Random graph theory is used to examine the “small-world phenomenon”; any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees the average distance is almost surely of order log n/log d̃, where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/kβ for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, which we call the core, having nc/log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",15879 - 15882,2002.0,https://www.trujillo.net/tagsabout.htm,Mathematics
2465,4e892deef85c4cb62716776657d66dc417574bcc,An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations,Efficient use of a distributed memory parallel computer requires that the computational load be balanced across processors in a way that minimizes interprocessor communication. A new domain mapping algorithm is presented that extends recent work in which ideas from spectral graph theory have been applied to this problem. The generalization of spectral graph bisection involves a novel use of multiple eigenvectors to allow for division of a computation into four or eight parts at each stage of a recursive decomposition. The resulting method is suitable for scientific computations like irregular finite elements or differences performed on hypercube or mesh architecture machines. Experimental results confirm that the new method provides better decompositions arrived at more economically and robustly than with previous spectral methods. This algorithm allows for arbitrary nonnegative weights on both vertices and edges to model inhomogeneous computation and communication. A new spectral lower bound for graph bi...,452-469,1995.0,http://cox-wang.biz/wp-content/listregister.php,Computer Science
2466,77c4a58c801f233400e71ebd2591df62345f3616,A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks,"The theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole networks is developed. The author proposes some basic definitions and two theorems. These create the conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel dependency graph. Two design methodologies are also proposed. The first supplies algorithms with a high degree of freedom, without increasing the number of physical channels. The second methodology is intended for the design of fault-tolerant algorithms. Some examples are given to show the application of the methodologies. Simulations show the performance improvement that can be achieved by designing the routing algorithms with the new theory. >",1320-1331,1993.0,https://www.mcneil-frost.org/categories/categoriesfaq.htm,Computer Science
2467,8dd09cfe9d7b2d4a13e02693112b1f8afa37f222,"Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications","There is a large, popular, and growing literature on ""scale-free"" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.",431 - 523,2005.0,https://www.berry.com/main/blog/categoriesregister.html,Computer Science
2468,c45b789b42e9a85d3193f43e3f0ddffb7d6aa423,Handbook of Combinatorics,"Part 1 Structures: graphs - basic graph theory - paths and circuits, J.A. Bondy, connectivity and network flows, A. Frank, matchings and extensions, W.R. Pulleyblank, colouring, stable sets and perfect graphs, B. Toft, embeddings and minors, C. Thomassen, random graphs, M. Karonski finite sets and relations - hypergraphs, P. Duchet, partially ordered sets, W.T. Trotter matroids - matroids - fundamental concepts, D.J.A. Welsh, matroid minors, P.D. Seymour, matroid optimization and algorithms, R.E. Bixby and W.H. Cunningham symmetric structures - permutation groups, P.J. Cameron, finite geometries, P.J. Cameron, block designs, A.E. Brouwer, association schemes, A.E. Brouwer and W. Haemers, codes, J.H. van Lint combinatorial structures in geometry and number theory - extremal problems in combinatorial geometry, P. Erdos and G. Purdy, convex polytopes and related complexes, V. Klee and P. Kleinschmidt, point lattices, J.C. Lagarias, combinatorial number theory, C. Pomerance and A. Sarkozy. Part 2 Aspects: algebraic enumeration, I.M. Gessel and R.P. Stanley asymptotic enumeration methods, A.M. Odlyzko extremal graph theory, B. Bollobas extremal set systems, P. Frankl Ramsey theory, J. Nesetril discrepancy theory, J. Beck and V.T. Sos automorphism groups, isomorphism, reconstruction, L. Babai optimization, M. Grotschel and L. Lovasz computational complexity, D.B. Shmoys and E. Tardos. Part 3 Methods: polyhedral combinatorics, A. Schrijver tools from linear algebra, C.D. Godsil tools from higher algebra, N. Alon probabilistic methods, J. Spencer topological methods, A. Bjorner. Part 4 Applications: combinatorics in operations research, A. Kolen and J.K. Lenstra combinatorics in electrical engineering and statics, A. Recski combinatorics in statistical mechanics, C.D. Godsil et al combinatorics in chemistry, D.H. Rouvray applications of combinatorics to molecular biology, M.S. Waterman combinatorics in computer science, L. Lovasz et al combinatorics in pure mathematics, L. Lovasz et al. Part 5 Horizons: infinite combinatorics, A. Hajnal combinatorial games, R.K. Guy the history of combinatorics, N.L. Biggs et al.",35-137,1995.0,http://www.pittman.com/posts/postsregister.html,Mathematics
2469,d89cc6a8911156f671ee60ddbf6af20ff33cd146,Shock Graphs and Shape Matching,,13-32,1998.0,http://www.ryan.info/search/categories/listhome.php,Mathematics
2470,8fc012941dccba5017bfd73dfd40e414001b74c4,Complex Graphs and Networks,Graph theory in the information age Old and new concentration inequalities A generative model--the preferential attachment scheme Duplication models for biological networks Random graphs with given expected degrees The rise of the giant component Average distance and the diameter Eigenvalues of the adjacency matrix of $G(\mathbf{w})$ The semi-circle law for $G(\mathbf{w})$ Coupling on-line and off-line analyses of random graphs The configuration model for power law graphs The small world phenomenon in hybrid graphs Bibliography Index.,89-140,2006.0,http://www.sellers-george.info/list/list/listprivacy.html,Mathematics
2471,c81698f8a3014854f44744152d377421041da70f,The rainbow connection of a graph is (at most) reciprocal to its minimum degree,"An edge‐colored graph Gis rainbow edge‐connected if any two vertices are connected by a path whose edges have distinct colors. The rainbow connection of a connected graph G, denoted by rc(G), is the smallest number of colors that are needed in order to make Grainbow edge‐connected. We prove that if Ghas nvertices and minimum degree δ then rc(G)<20n/δ. This solves open problems from Y. Caro, A. Lev, Y. Roditty, Z. Tuza, and R. Yuster (Electron J Combin 15 (2008), #R57) and S. Chakrborty, E. Fischer, A. Matsliah, and R. Yuster (Hardness and algorithms for rainbow connectivity, Freiburg (2009), pp. 243–254). A vertex‐colored graph Gis rainbow vertex‐connected if any two vertices are connected by a path whose internal vertices have distinct colors. The rainbow vertex‐connection of a connected graph G, denoted by rvc(G), is the smallest number of colors that are needed in order to make Grainbow vertex‐connected. One cannot upper‐bound one of these parameters in terms of the other. Nevertheless, we prove that if Ghas nvertices and minimum degree δ then rvc(G)<11n/δ. We note that the proof in this case is different from the proof for the edge‐colored case, and we cannot deduce one from the other. © 2009 Wiley Periodicals, Inc. J Graph Theory 63: 185–191, 2010",70-134,2010.0,https://patel-fernandez.info/blog/appmain.htm,Computer Science
2472,49ab911541401d4bb031870d0691379da63d6d28,Algebraic Approaches to Graph Transformation - Part I: Basic Concepts and Double Pushout Approach,"The algebraic approaches to graph transformation are based on the concept of gluing of graphs, modelled by pushouts in suitable categories of graphs and graph morphisms. This allows one not only to give an explicit algebraic or set theoretical description of the constructions, but also to use concepts and results from category theory in order to build up a rich theory and to give elegant proofs even in complex situations. In this chapter we start with an overwiev of the basic notions common to the two algebraic approaches, the ""double-pushout (DPO) approach"" and the ""single-pushout (SPO) approach""; next we present the classical theory and some recent development of the double-pushout approach. The next chapter is devoted instead to the single-pushout approach, and it is closed by a comparison between the two approaches. -- This document will appear as a chapter of the ""The Handbook of Graph Grammars. Volume I: Foundations"", G. Rozenberg (Ed.), World Scientific.",163-246,1997.0,http://thompson.com/tagsabout.asp,Computer Science
2473,f7b91f04795c4fc1449587d9c900a3ec6d39d79a,A graph-theoretic approach to the method of global Lyapunov functions,"A class of global Lyapunov functions is revisited and used to resolve a long-standing open problem on the uniqueness and global stability of the endemic equilibrium of a class of multi-group models in mathematical epidemiology. We show how the group structure of the models, as manifested in the derivatives of the Lyapunov function, can be completely described using graph theory.",2793-2802,2008.0,https://chung.net/tagsearch.jsp,Mathematics
2474,a83e9a3dce59292d7dbdab2a8dd20c6b73db3005,"The image foresting transform: theory, algorithms, and applications","The image foresting transform (IFT) is a graph-based approach to the design of image processing operators based on connectivity. It naturally leads to correct and efficient implementations and to a better understanding of how different operators relate to each other. We give here a precise definition of the IFT, and a procedure to compute it-a generalization of Dijkstra's algorithm-with a proof of correctness. We also discuss implementation issues and illustrate the use of the IFT in a few applications.",19-29,2004.0,https://walker.org/wp-content/list/categoriessearch.php,Medicine
2475,1e6f2b06416eb3e2df0c73c90e1bb3f81627a32a,Graphical Models for Game Theory,We introduce a compact graph-theoretic representation for multi-party game theory. Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in one-stage games represented by trees or sparse graphs.,76-136,2001.0,http://www.curry.com/category/list/blogpost.htm,Computer Science
2476,7d41e80b97db14ea86d6c41e5dc090fd7e8da938,Consensus Conditions of Multi-Agent Systems With Time-Varying Topologies and Stochastic Communication Noises,"This paper investigates the average-consensus problem of first-order discrete-time multi-agent networks in uncertain communication environments. Each agent can only use its own and neighbors' information to design its control input. To attenuate the communication noises, a distributed stochastic approximation type protocol is used. By using probability limit theory and algebraic graph theory, consensus conditions for this kind of protocols are obtained: (A) For the case of fixed topologies, a necessary and sufficient condition for mean square average-consensus is given, which is also sufficient for almost sure consensus. (B) For the case of time-varying topologies, sufficient conditions for mean square average-consensus and almost sure consensus are given, respectively. Especially, if the network switches between jointly-containing-spanning-tree, instantaneously balanced graphs, then the designed protocol can guarantee that each individual state converges, both almost surely and in mean square, to a common random variable, whose expectation is right the average of the initial states of the whole system, and whose variance describes the static maximum mean square error between each individual state and the average of the initial states of the whole system.",2043-2057,2010.0,https://www.meyer.com/category/search/blogregister.html,Mathematics
2477,0cd7469b7fc4ad90a0552e7124f60f1d216f467f,"Two’s company, three (or more) is a simplex",,1 - 14,2016.0,http://juarez.com/searchindex.htm,Medicine
2478,c2d7f7428a81a18fb5269dd548755fcb1d6998d3,"Mean Curvature, Threshold Dynamics, and Phase Field Theory on Finite Graphs",,3-65,2013.0,https://www.torres-hayes.com/categories/wp-content/mainregister.html,Mathematics
2479,8d03e9bccb5efb02fee168ab968c6cfa8884905c,Combinatorial Algebraic Topology,,"I-XIX, 1-389",2007.0,http://www.gonzalez-trevino.info/main/blog/listmain.asp,Computer Science
2480,4b2b0821d9d6c9535f4f1aaf80b1e6be79a3ca3e,Networks,"The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in recent years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyse network data on an unprecendented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social science. This book brings together the most important breakthroughts in each of these fields and presents them in a unified fashion, highlighting the strong interconnections between work in different areas. Topics covered include the measurement of networks; methods for analysing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms, including spectral algorithms and community detection; mathematical models of networks such as random graph models and generative models; and models of processes taking place on networks.",17 - 20,1995.0,https://www.schneider.com/main/maincategory.htm,Technology
2481,370b784d04cb952cfac2b4d179ddfd9dfb997288,Horizontal visibility graphs: exact results for random time series.,"The visibility algorithm has been recently introduced as a mapping between time series and complex networks. This procedure allows us to apply methods of complex network theory for characterizing time series. In this work we present the horizontal visibility algorithm, a geometrically simpler and analytically solvable version of our former algorithm, focusing on the mapping of random series (series of independent identically distributed random variables). After presenting some properties of the algorithm, we present exact results on the topological properties of graphs associated with random series, namely, the degree distribution, the clustering coefficient, and the mean path length. We show that the horizontal visibility algorithm stands as a simple method to discriminate randomness in time series since any random series maps to a graph with an exponential degree distribution of the shape P(k)=(1/3)(2/3)(k-2), independent of the probability distribution from which the series was generated. Accordingly, visibility graphs with other P(k) are related to nonrandom series. Numerical simulations confirm the accuracy of the theorems for finite series. In a second part, we show that the method is able to distinguish chaotic series from independent and identically distributed (i.i.d.) theory, studying the following situations: (i) noise-free low-dimensional chaotic series, (ii) low-dimensional noisy chaotic series, even in the presence of large amounts of noise, and (iii) high-dimensional chaotic series (coupled map lattice), without needs for additional techniques such as surrogate data or noise reduction methods. Finally, heuristic arguments are given to explain the topological properties of chaotic series, and several sequences that are conjectured to be random are analyzed.","
          046103
        ",2009.0,https://gaines-todd.org/wp-content/wp-content/apphomepage.asp,Physics
2482,a0095ea6b13b95073088f2afb6e33e4d7761b04e,Entanglement in Graph States and its Applications,"Graph states form a rich class of entangled states that exhibit important aspects of multi-partite entanglement. At the same time, they can be described by a number of parameters that grows only moderately with the system size. They have a variety of applications in quantum information theory, most prominently as algorithmic resources in the context of the one-way quantum computer, but also in other fields such as quantum error correction and multi-partite quantum communication, as well as in the study of foundational issues such as non-locality and decoherence. In this review, we give a tutorial introduction into the theory of graph states. We introduce various equivalent ways how to define graph states, and discuss the basic notions and properties of these states. The focus of this review is on their entanglement properties. These include aspects of non-locality, bi-partite and multi-partite entanglement and its classification in terms of the Schmidt measure, the distillability properties of mixed entangled states close to a pure graph state, as well as the robustness of their entanglement under decoherence. We review some of the known applications of graph states, as well as proposals for their experimental implementation.",62-121,2006.0,https://www.roberts.biz/wp-content/searchregister.jsp,Physics
2483,f7dccb8cb2e795b9a6d882523b316aa2930b72ad,On Communicating Finite-State Machines,"A model of commumcations protocols based on finite-state machines is investigated. The problem addressed is how to ensure certain generally desirable properties, which make protocols ""wellformed,"" that is, specify a response to those and only those events that can actually occur. It is determined to what extent the problem is solvable, and one approach to solving it ts described. Categories and SubJect Descriptors' C 2 2 [Computer-Conununication Networks]: Network Protocols-protocol verification; F 1 1 [Computation by Abstract Devices] Models of Computation--automata; G.2.2 [Discrete Mathematics] Graph Theory--graph algoruhms; trees General Terms: Reliability, Verification Additional",323-342,1983.0,https://www.brown-spencer.com/search/app/exploreprivacy.html,Computer Science
2484,dce4848f314d1bc4eae80ea5efd1390654a4c341,Mathematics of networks,"An introduction to the mathematical tools used in the study of networks. Topics discussed include: the adjacency matrix; weighted, directed, acyclic, and bipartite networks; multilayer and dynamic networks; trees; planar networks. Some basic properties of networks are then discussed, including degrees, density and sparsity, paths on networks, component structure, and connectivity and cut sets. The final part of the chapter focuses on the graph Laplacian and its applications to network visualization, graph partitioning, the theory of random walks, and other problems.",81-142,2018.0,https://www.scott.com/searchfaq.asp,Mathematics
2485,3742c17f24437a310b05888de2618212fe82cedf,Ramanujan Graphs,"In the last two decades, the theory of Ramanujan graphs has gained prominence primarily for two reasons. First, from a practical viewpoint, these graphs resolve an extremal problem in communication network theory (see for example [2]). Second, from a more aesthetic viewpoint, they fuse diverse branches of pure mathematics, namely, number theory, representation theory and algebraic geometry. The purpose of this survey is to unify some of the recent developments and expose certain open problems in the area. This survey is by no means an exhaustive one and demonstrates a highly number-theoretic bias. For more comprehensive surveys, we refer the reader to [27], [9] or [13]. For a more up-to-date survey highlighting the connection between graph theory and automorphic representations, we refer the reader to Winnie Li’s recent survey article [11]. A is a triple consisting of avertex (X), an and a map that associates to each edge two vertices (not necessarily distinct) called itsendpoints. A is an edge whose endpoints are equal. Multiple edges are edges having the same pair of endpoints. A is one having no loops or multiple edges. If a graph has loops or multiple edges, we will call it a multigraph. When two verticesu andv are endpoints of an edge, we say they are and (sometimes) write to indicate this. To any graph, we may associate the which is ann matrix (wheren |) with rows and columns indexed by the elements of the vertex set and the y)-th entry is the number of edges connecting and y. Since our graphs are undirected, the matrix is symmetric. Consequently, all of its eigenvalues are real. The convention regarding terminology is not clear in the literature. Most use the term ‘graph’ to mean a simple graph as we have defined it above. Thus, the",98-142,1965.0,http://www.smith-williams.org/categoriesindex.html,Technology
2486,0dbffa44f08aec11d35cb404beac6c0f748498b1,A Contribution to the Theory of Chromatic Polynomials,"Summary Two polynomials θ(G, n) and ϕ(G, n) connected with the colourings of a graph G or of associated maps are discussed. A result believed to be new is proved for the lesser-known polynomial ϕ(G, n). Attention is called to some unsolved problems concerning ϕ(G, n) which are natural generalizations of the Four Colour Problem from planar graphs to general graphs. A polynomial χ(G, x, y) in two variables x and y, which can be regarded as generalizing both θ(G, n) and ϕ(G, n) is studied. For a connected graph χ(G, x, y) is defined in terms of the “spanning” trees of G (which include every vertex) and in terms of a fixed enumeration of the edges.",80 - 91,1954.0,https://davis.org/posts/searchterms.html,Mathematics
2487,c8cb556b5cb996e9bab3745aa889c4b4608faba9,"Reaching a Consensus in a Dynamically Changing Environment: Convergence Rates, Measurement Delays, and Asynchronous Events","This paper uses recently established properties of compositions of directed graphs together with results from the theory of nonhomogeneous Markov chains to derive worst case convergence rates for the headings of a group of mobile autonomous agents which arise in connection with the widely studied Vicsek consensus problem. The paper also uses graph-theoretic constructions to solve modified versions of the Vicsek problem in which there are measurement delays, asynchronous events, or a group leader. In all three cases the conditions under which consensus is achieved prove to be almost the same as the conditions under which consensus is achieved in the synchronous, delay-free, leaderless case.",601-623,2008.0,https://sharp.biz/search/appauthor.php,Computer Science
2488,9f5b65929d14447a77063ceb854660b5f9f8ad05,Graph-based Knowledge Representation - Computational Foundations of Conceptual Graphs,,1-427,2008.0,http://www.collier.biz/appindex.htm,Computer Science
2489,3bc4736f9b8512043ed47357a81f26b93a1204b6,Semi-supervised learning with graphs,"In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. 
We present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.",20-142,2005.0,http://bradford.com/main/maincategory.html,Computer Science
2490,9496a84e79463f1004e7880669e7661d5203ed4a,Koszul duality for Operads,"(0.1) The purpose of this paper is to relate two seemingly disparate developments. One is the theory of graph cohomology of Kontsevich [Kon 2 3] which arose out of earlier works of Penner [Pe] and Kontsevich [Kon 1] on the cell decomposition and intersection theory on the moduli spaces of curves. The other is the theory of Koszul duality for quadratic associative algebras which was introduced by Priddy [Pr] and has found many applications in homological algebra, algebraic geometry and representation theory (see e.g., [Be] [BGG] [BGS] [Ka 1] [Man]). The unifying concept here is that of an operad. This paper can be divided into two parts consisting of chapters 1, 3 and 2, 4, respectively. The purpose of the first part is to establish a relationship between operads, moduli spaces of stable curves and graph complexes. To each operad we associate a collection of sheaves on moduli spaces. We introduce, in a natural way, the cobar complex of an operad and show that it is nothing but a (special case of the) graph complex, and that both constructions can be interpreted as the Verdier duality functor on sheaves. In the second part we introduce a class of operads, called quadratic, and introduce a distinguished subclass of Koszul operads. The main reason for introducing Koszul operads (and in fact for writing this paper) is that most of the operads ”arising from nature” are Koszul, cf. (0.8) below. We define a natural duality on quadratic operads (which is",90-124,1994.0,http://kelly-watson.com/blogpost.html,Mathematics
2491,60fca57bca813e06d2bdf7acb1b970bfce3e858d,Strategic Interaction and Networks,"This paper brings a general network analysis to a wide class of economic games. A network, or interaction matrix, tells who directly interacts with whom. A major challenge is determining how network structure shapes overall outcomes. We have a striking result. Equilibrium conditions depend on a single number: the lowest eigenvalue of a network matrix. Combining tools from potential games, optimization, and spectral graph theory, we study games with linear best replies and characterize the Nash and stable equilibria for any graph and for any impact of players’ actions. When the graph is sufficiently absorptive (as measured by this eigenvalue), there is a unique equilibrium. When it is less absorptive, stable equilibria always involve extreme play where some agents take no actions at all. This paper is the first to show the importance of this measure to social and economic outcomes, and we relate it to different network link patterns.",76-147,2010.0,https://www.kelley.com/category/tagauthor.htm,Economics
2492,ba5f513fc3be3432018a3f93b12e17a3f1580324,"Protein-to-protein interactions: Technologies, databases, and algorithms","Studying proteins and their structures has an important role for understanding protein functionalities. Recently, due to important results obtained with proteomics, a great interest has been given to interactomics, that is, the study of protein-to-protein interactions, called PPI, or more generally, interactions among macromolecules, particularly within cells. Interactomics means studying, modeling, storing, and retrieving protein-to-protein interactions as well as algorithms for manipulating, simulating, and predicting interactions. PPI data can be obtained from biological experiments studying interactions. Modeling and storing PPIs can be realized by using graph theory and graph data management, thus graph databases can be queried for further experiments. PPI graphs can be used as input for data-mining algorithms, where raw data are binary interactions forming interaction graphs, and analysis algorithms retrieve biological interactions among proteins (i.e., PPI biological meanings). For instance, predicting the interactions between two or more proteins can be obtained by mining interaction networks stored in databases. In this article we survey modeling, storing, analyzing, and manipulating PPI data. After describing the main PPI models, mostly based on graphs, the article reviews PPI data representation and storage, as well as PPI databases. Algorithms and software tools for analyzing and managing PPI networks are discussed in depth. The article concludes by discussing the main challenges and research directions in PPI networks.",1:1-1:36,2010.0,http://www.mcneil.com/categoriessearch.html,Computer Science
2493,bc3c2d97e967e18e1eafd9f2b1b887bf79c9d545,Graph embedding: a general framework for dimensionality reduction,"In the last decades, a large family of algorithms - supervised or unsupervised; stemming from statistic or geometry theory - have been proposed to provide different solutions to the problem of dimensionality reduction. In this paper, beyond the different motivations of these algorithms, we propose a general framework, graph embedding along with its linearization and kernelization, which in theory reveals the underlying objective shared by most previous algorithms. It presents a unified perspective to understand these algorithms; that is, each algorithm can be considered as the direct graph embedding or its linear/kernel extension of some specific graph characterizing certain statistic or geometry property of a data set. Furthermore, this framework is a general platform to develop new algorithm for dimensionality reduction. To this end, we propose a new supervised algorithm, Marginal Fisher Analysis (MFA), for dimensionality reduction by designing two graphs that characterize the intra-class compactness and inter-class separability, respectively. MFA measures the intra-class compactness with the distance between each data point and its neighboring points of the same class, and measures the inter-class separability with the class margins; thus it overcomes the limitations of traditional Linear Discriminant Analysis algorithm in terms of data distribution assumptions and available projection directions. The toy problem on artificial data and the real face recognition experiments both show the superiority of our proposed MFA in comparison to LDA.",830-837 vol. 2,2005.0,https://www.lopez-jackson.org/categoriesfaq.html,Computer Science
2494,1467a3ff88a9b9fbd54e8c8afa0cb1d62bcf6a22,Applications of Combinatorial Matrix Theory to Laplacian Matrices of Graphs,"Matrix Theory Preliminaries Vector Norms, Matrix Norms, and the Spectral Radius of a Matrix Location of Eigenvalues Perron-Frobenius Theory M-Matrices Doubly Stochastic Matrices Generalized Inverses Graph Theory Preliminaries Introduction to Graphs Operations of Graphs and Special Classes of Graphs Trees Connectivity of Graphs Degree Sequences and Maximal Graphs Planar Graphs and Graphs of Higher Genus Introduction to Laplacian Matrices Matrix Representations of Graphs The Matrix Tree Theorem The Continuous Version of the Laplacian Graph Representations and Energy Laplacian Matrices and Networks The Spectra of Laplacian Matrices The Spectra of Laplacian Matrices Under Certain Graph Operations Upper Bounds on the Set of Laplacian Eigenvalues The Distribution of Eigenvalues Less than One and Greater than One The Grone-Merris Conjecture Maximal (Threshold) Graphs and Integer Spectra Graphs with Distinct Integer Spectra The Algebraic Connectivity Introduction to the Algebraic Connectivity of Graphs The Algebraic Connectivity as a Function of Edge Weight The Algebraic Connectivity with Regard to Distances and Diameters The Algebraic Connectivity in Terms of Edge Density and the Isoperimetric Number The Algebraic Connectivity of Planar Graphs The Algebraic Connectivity as a Function Genus k where k is greater than 1 The Fiedler Vector and Bottleneck Matrices for Trees The Characteristic Valuation of Vertices Bottleneck Matrices for Trees Excursion: Nonisomorphic Branches in Type I Trees Perturbation Results Applied to Extremizing the Algebraic Connectivity of Trees Application: Joining Two Trees by an Edge of Infinite Weight The Characteristic Elements of a Tree The Spectral Radius of Submatrices of Laplacian Matrices for Trees Bottleneck Matrices for Graphs Constructing Bottleneck Matrices for Graphs Perron Components of Graphs Minimizing the Algebraic Connectivity of Graphs with Fixed Girth Maximizing the Algebraic Connectivity of Unicyclic Graphs with Fixed Girth Application: The Algebraic Connectivity and the Number of Cut Vertices The Spectral Radius of Submatrices of Laplacian Matrices for Graphs The Group Inverse of the Laplacian Matrix Constructing the Group Inverse for a Laplacian Matrix of a Weighted Tree The Zenger Function as a Lower Bound on the Algebraic Connectivity The Case of the Zenger Equalling the Algebraic Connectivity in Trees Application: The Second Derivative of the Algebraic Connectivity as a Function of Edge Weight",15-137,2012.0,https://thompson-strong.org/wp-content/listpost.php,Mathematics
2495,76b76de8318457348973d8a655d1212fe51cf142,The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics,"Topological characteristics of multidimensional potential energy surfaces are explored and the full conformation space is mapped on the set of local minima. This map partitions conformation space into energy-dependent or temperature-dependent “attraction basins’’ and generates a “disconnectivity’’ graph that reflects the basin connectivity and characterizes the shape of the multidimensional surface. The partitioning of the conformation space is used to express the temporal behavior of the system in terms of basin-to-basin kinetics instead of the usual state-to-state transitions. For this purpose the transition matrix of the system is expressed in terms of basin-to-basin transitions and the corresponding master equation is solved. As an example, the approach is applied to the tetrapeptide, isobutyryl-(ala)3-NH-methyl (IAN), which is the shortest peptide that can form a full helical turn. A nearly complete list of minima and barriers is available for this system from the work of Czerminiski and Elber. The m...",1495-1517,1997.0,http://www.freeman-andrews.com/wp-content/tag/listregister.htm,Chemistry
2496,b4642da8677ca40ec888c6d450c3308473281b52,Incidence matrices and interval graphs,"Abstract : According to present genetic theory, the fine structure of genes consists of linearly ordered elements. A mutant gene is obtained by alteration of some connected portion of this structure. By examining data obtained from suitable experiments, it can be determined whether or not the blemished portions of two mutant genes intersect or not, and thus intersection data for a large number of mutants can be represented as an undirected graph. If this graph is an interval graph, then the observed data is consistent with a linear model of the gene. The problem of determining when a graph is an interval graph is a special case of the following problem concerning (0, 1)-matrices: When can the rows of such a matrix be permuted so as to make the 1's in each colum appear consecutively. A complete theory is obtained for this latter problem, culminating in a decomposition theorem which leads to a rapid algorithm for deciding the question, and for constructing the desired permutation when one exists.",835-855,1965.0,http://www.shepherd.com/tags/exploreabout.php,Mathematics
2497,5ccd1eefd94af0a20e10a0a5e0da22aa2f481c76,Every monotone graph property has a sharp threshold,"In their seminal work which initiated random graph theory Erdos and Renyi discovered that many graph properties have sharp thresholds as the number of vertices tends to infinity. We prove a conjecture of Linial that every monotone graph property has a sharp threshold. This follows from the following theorem. Let Vn(p) = {0, 1}n denote the Hamming space endowed with the probability measure μp defined by μp( 1, 2, . . . , n) = pk · (1 − p)n−k, where k = 1 + 2 + · · · + n. Let A be a monotone subset of Vn. We say that A is symmetric if there is a transitive permutation group Γ on {1, 2, . . . , n} such that A is invariant under Γ. Theorem. For every symmetric monotone A, if μp(A) > then μq(A) > 1− for q = p+ c1 log(1/2 )/ logn. (c1 is an absolute constant.)",2993-3002,1996.0,http://www.cameron.net/explore/app/exploresearch.html,Mathematics
2498,6a70edfc099614866996222893e91826b4da3677,Thermodynamic perturbation theory of polymerization,"We derive several extensions of a previously given first‐order perturbation theory (TPT 1) for fluids in which chain and ring polymers can be formed, due to the presence of two singly bondable molecular attraction sites. We retain graphs which contain a single chain of attraction bonds, and evaluate second‐order perturbation theory (TPT 2) within this framework. The previous formulation with sites fixed in the molecule is generalized to allow movable sites, thus permitting calculations for flexible bead polymers. The TPT 1 result for the equation of state of an equilibrium mixture of chain lengths is in good agreement with simulations of flexible bead polymers of fixed bead number N, when the mean number ν of beads is equated to the fixed N of the simulations. TPT 2 differs from TPT 1 by a rather small term, with improved agreement. If the distribution of movable sites includes configurations such that bonding of one site blocks bonding of the other, then graph resummation must be used. Resummed TPT yield...",7323-7331,1987.0,https://dunlap-mcgrath.com/main/searchsearch.html,Chemistry
2499,6a4f4038032d5a89c39fcdceba4a9c5ba5ecae27,Twin-width IV: ordered graphs and matrices,"We establish a list of characterizations of bounded twin-width for hereditary classes of totally ordered graphs: as classes of at most exponential growth studied in enumerative combinatorics, as monadically NIP classes studied in model theory, as classes that do not transduce the class of all graphs studied in finite model theory, and as classes for which model checking first-order logic is fixed-parameter tractable studied in algorithmic graph theory. This has several consequences. First, it allows us to show that every hereditary class of ordered graphs either has at most exponential growth, or has at least factorial growth. This settles a question first asked by Balogh, Bollobás, and Morris [Eur. J. Comb. ’06] on the growth of hereditary classes of ordered graphs, generalizing the Stanley-Wilf conjecture/Marcus-Tardos theorem. Second, it gives a fixed-parameter approximation algorithm for twin-width on ordered graphs. Third, it yields a full classification of fixed-parameter tractable first-order model checking on hereditary classes of ordered binary structures. Fourth, it provides a model-theoretic characterization of classes with bounded twin-width. Finally, it settles our small conjecture [SODA ’21] in the case of ordered graphs.",75-149,2021.0,https://french-rivas.com/app/wp-content/listfaq.html,Computer Science
